<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Colin Okasaki</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Colin Okasaki</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>/img/icon.png</url>
      <title>Colin Okasaki</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Notation</title>
      <link>/courses/qsci483/notation/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/notation/</guid>
      <description>&lt;h2 id=&#34;matrixvector-notation-&#34;&gt;Matrix/Vector Notation&lt;/h2&gt;
&lt;p&gt;In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector&amp;rsquo;s components are not specified I will bold it such as $\mathbf{v}$. I will write $X&#39;$ to mean the transpose of a matrix $X$ or $v&#39;$ to mean the transpose of a matrix $v$, as is common in regression analysis.&lt;/p&gt;
&lt;h3 id=&#34;advanced-notation-matrix-calculus-&#34;&gt;Advanced Notation: Matrix Calculus&lt;/h3&gt;
&lt;p&gt;For an $n\times 1$ vector $v$ and a scalar $f$ we will write $\dfrac{\partial v}{\partial f}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial v}{\partial f}\right)_i = \dfrac{\partial v_i}{\partial f}$. We will write $\dfrac{\partial f}{\partial v}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial f}{\partial v}\right)_i = \dfrac{\partial f}{\partial v_i}$. A useful resource for matrix calculus can be found on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
 or in the more extensive 
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notation and Terminology</title>
      <link>/courses/gaussian-processes/notation/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/notation/</guid>
      <description>&lt;h2 id=&#34;matrixvector-notation-&#34;&gt;Matrix/Vector Notation&lt;/h2&gt;
&lt;p&gt;In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector&amp;rsquo;s components are not specified I will bold it such as $\mathbf{v}$. I will write $X&#39;$ to mean the transpose of a matrix $X$ or $v&#39;$ to mean the transpose of a matrix $v$, as is common in regression analysis.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus-&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;p&gt;For an $n\times 1$ vector $v$ and a scalar $f$ we will write $\dfrac{\partial v}{\partial f}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial v}{\partial f}\right)_i = \dfrac{\partial v_i}{\partial f}$. We will write $\dfrac{\partial f}{\partial v}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial f}{\partial v}\right)_i = \dfrac{\partial f}{\partial v_i}$. A useful resource for matrix calculus can be found on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
 or in the more extensive 
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-process-notation-&#34;&gt;Gaussian Process Notation&lt;/h2&gt;
&lt;p&gt;I will write $f \sim GP(\mu(x),k(x,x&amp;rsquo;))$ to denote a Gaussian process with mean function $\mu$ and covariance function (kernel) $k(x,x&amp;rsquo;)$. I will in general &lt;em&gt;not&lt;/em&gt; assume that $k$ is stationary (see 
&lt;a href=&#34;#terminology&#34;&gt;terminology&lt;/a&gt;
). Under this definition $f$ is a stochastic process with the defining property that for any set of points ${x_i}$ (in whatever space $\Omega$ we choose), the vector $f(x)_i = f(x_i)$ is distributed as a multivariate normal (MVN) random vector with mean $\mu_i = \mu(x_i)$ and covariance matrix $\Sigma_{ij} = k(x_i,x_j)$. Many of the properties I will discuss on this page are actually properties of the MVN distribution.&lt;/p&gt;
&lt;h2 id=&#34;terminology-&#34;&gt;Terminology&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process is called &lt;em&gt;stationary&lt;/em&gt; (or &lt;em&gt;homogeneous&lt;/em&gt;) if $k(x,x&amp;rsquo;) = k(x-x&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;A Gaussian Process is called &lt;em&gt;isotropic&lt;/em&gt; if $k(x,x&amp;rsquo;) = k(|x-x&#39;|)$&lt;/li&gt;
&lt;li&gt;A matrix $M$ is said to be &lt;em&gt;positive semidefinite&lt;/em&gt; if it has the property that $v&amp;rsquo;Mv \geq 0$ for any vector $v$. Covariance matrices are positive semidefinite.&lt;/li&gt;
&lt;li&gt;A kernel is said to be positive semidefinite (psd) if $$\int_\Omega k(x,x&amp;rsquo;)f(x)f(x&amp;rsquo;)dxdx&amp;rsquo; \geq 0$$ for all $L_2$ functions $f$. Gram matrices (i.e. covariance matrices) from psd kernels are psd matrices.&lt;/li&gt;
&lt;li&gt;The inverse of the covariance matrix in a MVN distribution is $Q = \Sigma^{-1}$ and is called the &lt;em&gt;precision matrix&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Simple Linear Regression</title>
      <link>/courses/qsci483/linear-regression/simple-linear-regression/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/simple-linear-regression/</guid>
      <description>&lt;p&gt;In this document I will outline the math used in the analysis of a simple linear regression model. Make sure to check out my 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 document to clarify any confusing notation.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Simple Linear Regression is based upon the equation
$$
y_i \sim N(\beta_0 + \beta_1 x_i,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim \beta_0 + \beta_1 x_i + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. It is important to remember that the expectations, or means, of these variables are: $E[y_i] = \beta_0 + \beta_1 x_i$ and $E[\epsilon_i] = 0$. We will assume that there are $n$ data points and only 1 predictor. This is what makes it _simple_ linear regression. In more general linear regression models you have more than 1 predictor. In multivariate linear regression models you have more than one dependent variable as well. In addition to the predictor variable, we also have an intercept, or mean term, which can be thought of as a second predictor.&lt;/p&gt;
&lt;h3 id=&#34;fitting-&#34;&gt;Fitting&lt;/h3&gt;
&lt;h4 id=&#34;objective-function-&#34;&gt;Objective Function&lt;/h4&gt;
&lt;p&gt;To fit a simple linear regression we first must choose a measure of fit. Here we will choose least squares, since this corresponds to 
&lt;a href=&#34;/courses/qsci483/linear-regression/properties&#34;&gt;maximum likelihood esitmation&lt;/a&gt;
 in this model. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\hat{y}_i$, the value predicted by our fitted model. We can calculate $\hat{y}_i$ using our predictor $x_i$ for data point $i$ along with estimated coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$:
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x.
$$
Alternatively we can find the whole vector $\hat{y} = \hat{\beta_0}\mathbf{1} + x\hat{\beta}$ (where $\mathbf{1}$ is a vector of all ones). To find a single squared residuals we calculate $r_i^2 = (y_i - \hat{\beta}_0 - \hat{\beta}_1 x)^2$. We will define the function $f(\hat{\beta}_0,\hat{\beta}_1)$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as:
\begin{align*}
f(\hat{\beta})
&amp;amp; = \sum_{i=1}^n (\mbox{residual})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1)^2 \\&lt;br&gt;
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;optimization-&#34;&gt;Optimization&lt;/h4&gt;
&lt;p&gt;From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to both $\beta_0$ and $\beta_1$, to find the minimum sum of squared residuals (the &lt;em&gt;least squares&lt;/em&gt;). So let us take the partial derivative with respect to $\hat{\beta}_0$:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_0} }
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_0} \sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_0}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)\dfrac{\partial }{ \partial \hat{\beta}_0}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)
\end{align*}
To set this derivative equal to zero we need to set:
\begin{align*}
\sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) &amp;amp; = 0.
\end{align*}
This can be accomplished by splitting up the sum and getting
\begin{align*}
\sum_{i=1}^n y_i &amp;amp; = \sum_{i=1}^n \hat{\beta}_0 + \sum_{i=1}^n \hat{\beta}_1 x_i \\&lt;br&gt;
n\hat{\beta}_0 &amp;amp; = \sum_{i=1}^n y_i - \hat{\beta}_1 \sum_{i=1}^n x_i \\&lt;br&gt;
\hat{\beta}_0 &amp;amp; = \overline{y} - \hat{\beta}_1\overline{x}
\end{align*}
Now that we have calculated $\hat{\beta}_0$ in terms of $y,x,$ and $\hat{\beta_1}$ we can take the derivative with respect to $\beta_1$ to find the least squares estimator:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_1} }
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_1} \sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_1}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)\dfrac{\partial }{ \partial \hat{\beta}_1}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)(x_i)
\end{align*}
To set this derivative equal to zero we need to set:
\begin{align*}
\sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)x_i &amp;amp; = 0.
\end{align*}
But here we can plug in our estimator $\hat{\beta}_0$ to get
\begin{align*}
\sum_{i=1}^n \left(y_i - (\overline{y}-\hat{\beta}_1\overline{x}) - \hat{\beta}_1x_i\right)x_i &amp;amp; = 0.
\end{align*}
Then we can expand the sum on the left to get
\begin{align*}
\sum_{i=1}^n (y_i - \overline{y})x_i - \sum_{i=1}^n \hat{\beta}_1(x_i - \overline{x})x_i &amp;amp; = 0 \\&lt;br&gt;
\hat{\beta}_1 \sum_{i=1}^n (x_i - \overline{x})x_i &amp;amp; = \sum_{i=1}^n (y_i-\overline{y})x_i \\&lt;br&gt;
\hat{\beta}_1 &amp;amp; = \frac{\sum_{i=1}^n (y_i-\overline{y})x_i}{\sum_{i=1}^n (x_i - \overline{x})x_i}
\end{align*}
So we have found an estimator for $\hat{\beta}_1$ in terms of only the predictors and the responses. We also have an estimator for $\hat{\beta}_0$ in terms of the predictors, the responses, and $\hat{\beta}_1$. Now, traditionally, $\hat{\beta}_1$ is written in a slightly different form, as
\begin{align*}
\hat{\beta}_1 &amp;amp; = \frac{S_{xy}}{S_{xx}} \\&lt;br&gt;
S_{xx} &amp;amp; = \frac{1}{n-1}\sum (x_i-\overline{x})^2 \\&lt;br&gt;
S_{xy} &amp;amp; = \frac{1}{n-1}\sum (x_i-\overline{x})(y_i-\overline{y}).
\end{align*}
The reason for this is that $S_{xy}$ is the sample covariance of $x$ and $y$, and $S_{xx}$ is the sample variance of $x$, so it is nice to express $\hat{\beta}_1$ in terms of other statistics that we already know about. We can see that the two formulas for $\hat{\beta}_1$ are equivalent by doing a little more math, taking $S_{xx}$ and $S_{xy}$ and changing them to a slightly different format:
\begin{align*}
(N-1)S_{xy} &amp;amp; = \sum (x_i-\overline{x})(y_i-\overline{y}) \\&lt;br&gt;
&amp;amp; = \sum (y_i-\overline{y})x_i - \sum (y_i-\overline{y})\overline{x} \\&lt;br&gt;
&amp;amp; = \sum (y_i-\overline{y})x_i - \overline{x}\sum (y_i - \overline{y}) \\&lt;br&gt;
&amp;amp; = \sum (y_i -\overline{y})x_i - \overline{x}(\sum \left(y_i - \frac{1}{n}\sum y_i\right) \\&lt;br&gt;
&amp;amp; = \sum (y_i -\overline{y})x_i - \overline{x}(\sum y_i - \sum y_i) \\&lt;br&gt;
&amp;amp; = \sum (y_i - \overline{y})x_i
\end{align*}
I encourage you to try doing the same calculation with $S_{xx}$: you will find that it follows exactly the same format. So we can see that the formula we have derived for $\hat{\beta}_1$ is exactly the same as the traditional format in terms of the sample (co)variances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>General Linear Algebraic Properties</title>
      <link>/courses/gaussian-processes/linear-algebra/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/linear-algebra/</guid>
      <description>&lt;p&gt;The Woodbury matrix identity is $$(A+UCV)^{-1}=A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.$$ Simpler versions of this identity are
\begin{align*}
(I+UV)^{-1} &amp;amp; = I-U(I+VU)^{-1}V, \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-(I+P)^{-1}P \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-P(I+P)^{-1}
\end{align*}
In the special case that $u$ and $v$ are vectors and $C = I$ we get the Sherman-Morrison formula: $$(A+uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.$$ A similar formula is the matrix determinant lemma $$\mbox{det}(A+uv^T) = (1+v^TA^{-1}u)\mbox{det}(A).$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/courses/qsci483/linear-regression/linear-regression/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/linear-regression/</guid>
      <description>&lt;p&gt;In this document I will outline the math used in the analysis of a more general linear regression model, with more than one predictor variable. Make sure to check out my 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 document to clarify any confusing notation, and check out my 
&lt;a href=&#34;/courses/qsci483/linear-regression/simple-linear-regression&#34;&gt;simple linear regression&lt;/a&gt;
 document if you would like a slightly simpler analysis to start off with. This document will charge right into matrix notation from the start.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Linear regression is based upon the equation
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. In the context of regression, it is important to remember that: $E[y_i] = X\beta$ and $E[\epsilon_i] = 0$. We will assume that there are $n$ data points and $k$ predictors. This $\beta$ is a $k\times 1$ vector, $X$ is a $n\times k$ matrix and $y$ and $\epsilon$ are $n\times 1$ vectors.&lt;/p&gt;
&lt;h3 id=&#34;fitting-&#34;&gt;Fitting&lt;/h3&gt;
&lt;h4 id=&#34;objective-function-&#34;&gt;Objective Function&lt;/h4&gt;
&lt;p&gt;To fit a simple linear regression we choose a measure of fit: least squares. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\hat{y}_i$, the value predicted by our fitted model. We can calculate $\hat{y}_i$ using the $1\times k$ vector $x_i$ of predictors for data point $i$:
$$
\hat{y}_i = x_i\hat{\beta}.
$$
Alternatively we can find the whole vector $\hat{y} = X\beta$. To find a single squared residuals we calculate $r_i^2 = (y_i - x_i\hat{\beta})^2$. We will define the function $f(\hat{\beta})$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as:
\begin{align*}
f(\hat{\beta})
&amp;amp; = \sum_{i=1}^n (\mbox{residual})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n (y_i - x_i\hat{\beta})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2
\end{align*}
In matrix notation we can rewrite this, however. The residual _vector_ can be written as the $n\times 1$ vector
$$
r = y - X\hat{\beta}.
$$
The sum of squares can then be written as $r&amp;rsquo;r$. So then
\begin{align*}
f(\hat{\beta})
&amp;amp; = r&amp;rsquo;r \\&lt;br&gt;
&amp;amp; = (y-X\hat{\beta})&#39;(y-X\hat{\beta}).
\end{align*}
We can expand this quadratic equation as
\begin{align*}
f(\hat{\beta})
&amp;amp; = y&amp;rsquo;y - y&amp;rsquo;X\hat{\beta} - \hat{\beta}&amp;lsquo;X&amp;rsquo;y + \hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}.
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;optimization-&#34;&gt;Optimization&lt;/h4&gt;
&lt;p&gt;From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to each $\beta_i$, to find the minimum sum of squared residuals (the &lt;em&gt;least squares&lt;/em&gt;). So let us take the partial derivative with respect to a particular coefficient:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_j}
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_j} \sum_{i=1}^n \left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_j}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\dfrac{\partial }{ \partial \hat{\beta}_j}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right) \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\left(-x_{ij}\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2x_{ij}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\end{align*}
Verify that this can be written in matrix notation as
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_j}
&amp;amp; = -2\left(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta}\right)_j,
\end{align*}
or in matrix calculus notation
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}} &amp;amp; = -2\left(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta}\right).
\end{align*}
Remember that according to calculus, every single entry $\dfrac{\partial f}{ \partial \beta_j}$ must be equal to zero at any extremum (and in particular at the minimum). Thus we can set this whole matrix equation equal to zero, and we get
$$
X&amp;rsquo;y = X&amp;rsquo;X\hat{\beta}.
$$
Since we are trying to derive an equation for $\hat{\beta}$ we move the matrix over to the other side and we get
$$
\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y.
$$
We can derive this same equation in fewer steps using the more complex matrix calculus notation, which for example allows us to take the derivative of matrix products and use the matrix chain rule:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}}
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}}\left(y&amp;rsquo;y - y&amp;rsquo;X\hat{\beta} - \hat{\beta}&amp;lsquo;X&amp;rsquo;y + \hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}\right) \\&lt;br&gt;
&amp;amp; = - \dfrac{\partial }{ \partial \hat{\beta}}\left(y&amp;rsquo;X\hat{\beta}\right) - \dfrac{\partial }{ \partial \hat{\beta}}\left(\hat{\beta}&amp;lsquo;X&amp;rsquo;y\right) + \dfrac{\partial }{ \partial \hat{\beta}}\left(\hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}\right) \\&lt;br&gt;
&amp;amp; = -y&amp;rsquo;X-(X&amp;rsquo;y)&amp;lsquo;+\hat{\beta}&amp;lsquo;X&amp;rsquo;X + (X&amp;rsquo;X\hat{\beta})&amp;rsquo; \\&lt;br&gt;
&amp;amp; = -2(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta})&amp;rsquo; \\&lt;br&gt;
&amp;amp; = 0.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;important-quantities&#34;&gt;Important Quantities&lt;/h3&gt;
&lt;p&gt;Now we have shown that the ordinary least squares estimator for $\beta$ is $(X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. Using this we may calculate all sorts of other things.&lt;/p&gt;
&lt;h4 id=&#34;predicted-values-&#34;&gt;Predicted Values&lt;/h4&gt;
&lt;p&gt;The predicted values are
\begin{align*}
\hat{y}
&amp;amp; = X\hat{\beta} \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo;y \\&lt;br&gt;
&amp;amp; = Hy.
\end{align*}
Here we have defined the &amp;ldquo;hat matrix,&amp;rdquo; $H = X(X&amp;rsquo;X)^{-1}X&#39;$. This is a useful matrix in linear regression, which maps the data to its predicted values. This is sometimes also called the projection matrix $P$, since it projects the data onto a lower-dimensional linear space. It is sometimes also called the influence matrix. It has two nice properties which we will use in a moment: it is symmetric and idempotent. This means $H&#39;=H$ and $H^2 = H&amp;rsquo;H = H$. We can see this by calculating:
\begin{align*}
H&amp;rsquo; &amp;amp; = (X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)&amp;rsquo; \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = H \\&lt;br&gt;
H^2 &amp;amp; = (X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)(X(X&amp;rsquo;X)^{-1}X&amp;rsquo;) \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}(X&amp;rsquo;X)(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = H
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;residuals-&#34;&gt;Residuals&lt;/h4&gt;
&lt;p&gt;The residuals are
\begin{align*}
\hat{\epsilon}
&amp;amp; = y-\hat{y} \\&lt;br&gt;
&amp;amp; = y-Hy \\&lt;br&gt;
&amp;amp; = (I-H)y \\&lt;br&gt;
&amp;amp; = (I-X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)y \\&lt;br&gt;
&amp;amp; = My
\end{align*}
Here we have defined the &amp;ldquo;residual maker&amp;rdquo; matrix, which can also be called the residual operator. This matrix takes the data and gives you the residuals of the model. It also inherits symmetry and idempotency from the hat matrix, since:
\begin{align*}
M&amp;rsquo; &amp;amp; = (I-H)&amp;rsquo; \\&lt;br&gt;
&amp;amp; = I-H \\&lt;br&gt;
M^2 &amp;amp; = (I-H)(I-H) \\&lt;br&gt;
&amp;amp; = I - 2H + H^2 \\&lt;br&gt;
&amp;amp; = I - 2H + H \\&lt;br&gt;
&amp;amp; = I-H
\end{align*}
With all of this put together we now have the tools to analyze the 
&lt;a href=&#34;../standard-error&#34;&gt;residual standard error&lt;/a&gt;
. First, however, we will analyze the properties of $\hat{\beta}$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Block Matrices</title>
      <link>/courses/gaussian-processes/block-matrices/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/block-matrices/</guid>
      <description>&lt;p&gt;Block matrices have some nice linear algebraic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\&lt;br&gt;
-(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; (D-CA^{-1}B)^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
(A-BD^{-1}C)^{-1} &amp;amp; -(A-BD^{-1}C)^{-1}BD^{-1} \\&lt;br&gt;
-D^{-1}C(A-BD^{-1}C)^{-1} &amp;amp; D^{-1} + D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\mbox{det}
\begin{pmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{pmatrix}
=
\mbox{det}(A)\times \mbox{det}(D-CA^{-1}B) = \mbox{det}(D)\times\mbox{det}(A-BD^{-1}C)$$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Residual Standard Error</title>
      <link>/courses/qsci483/linear-regression/standard-error/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/standard-error/</guid>
      <description>&lt;p&gt;A standard error is the estimated standard deviation $\hat{\sigma}$ for some variable. The residual standard error for linear regression is our estimate of the standard deviation of the noise $\epsilon$. Recall that we assume the noise is independent and heteroskedastic. This is important since it means we only need to estimate one single parameter for all of the noise variables.&lt;/p&gt;
&lt;p&gt;We estimate the residual variance using the equation $$\hat{\sigma}^2 = \frac{1}{n-k}\sum \hat{\epsilon}_i^2.$$ You can think of the $n-k$ term as accounting for the fact that we have estimated $k$ parameters before making this estimate. Recall that we fit our model by minimizing the sum of squared residuals. So we&amp;rsquo;ve actually optimized this model to minimize exactly $\sum \hat{\epsilon}_i^2$. The more parameters we have to work with the better that optimization will be. So we would probably get an answer that was too small if we just calculated the mean $$\frac{1}{n}\sum \hat{\epsilon}_i^2.$$ Dividing by a smaller number $n-k$ accounts for this, making $\hat{\sigma}^2$ an unbiased estimator of the residual variance.&lt;/p&gt;
&lt;h3 id=&#34;unbiasedness-&#34;&gt;Unbiasedness&lt;/h3&gt;
&lt;p&gt;We can see why this is by calculating the expectation of $\hat{\sigma}^2$. We won&amp;rsquo;t even both trying to avoid matrices in this derivation:
\begin{align*}
E[\hat{\sigma}^2]
&amp;amp; = E\left[\frac{1}{n-k}\hat{\epsilon}&#39;\hat{\epsilon}\right] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[y&amp;rsquo;M&amp;rsquo;My] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[y&amp;rsquo;My] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[(X\beta + \epsilon)&amp;lsquo;M(X\beta+\epsilon)] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} \left( E[\beta&amp;rsquo;X&amp;rsquo;MX\beta] + E[\epsilon&amp;rsquo;M\epsilon] \right)
\end{align*}
where we have eliminated the cross terms since $E[\epsilon]=0$. Now we can calculate
\begin{align*}
X&amp;rsquo;MX
&amp;amp; = X&amp;rsquo;(I-H)X \\&lt;br&gt;
&amp;amp; = X&amp;rsquo;X - X&amp;rsquo;(X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)X \\&lt;br&gt;
&amp;amp; = X&amp;rsquo;X - X&amp;rsquo;X \\&lt;br&gt;
&amp;amp; = 0
\end{align*}
so that the first term drops out. At this point we could either do a lot of algebra or we could make use of a convenient statistical property (we will do the latter). The expectation of a _quadratic form_ (i.e. $v&amp;rsquo;Mv$ for some random vector $v$ and constant matrix $M$) can be written as $$E[v&amp;rsquo;Mv] = \mbox{tr}[M\Sigma] + \mu^TM\mu$$ where $\mu$ is the mean of $v$ and $\Sigma$ the covariance of $v$. Using this property we find that
\begin{align*}
E[\hat{\sigma}^2]
&amp;amp; = \frac{1}{n-k}E[\epsilon&amp;rsquo;M\epsilon] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k}\mbox{tr}[M\Sigma(\epsilon)] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k}\mbox{tr}[\sigma^2M] \\&lt;br&gt;
&amp;amp; = \frac{\sigma^2}{n-k}\mbox{tr}[I-H].
\end{align*}
Now, so long as $X$ is full rank (i.e. there are no redundant predictors) the hat matrix has trace $k$. This can be shown using complicated eigenvalue proofs that you can find on Google. Thus $\mbox{tr}[I-H] = \mbox{tr}[I]-\mbox{tr}[H] = n-k$. Thus indeed the $n-k$ term drops out and we find that $\hat{\sigma}^2$ is an unbiased estimator of the true residual variance.&lt;/p&gt;
&lt;h3 id=&#34;distribution-&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;In fact, we have done more than show it is unbiased. Much of the algebra that we did actually did not depend on the outer expectation. We actually showed more generally that $$\hat{\sigma}^2 = \frac{1}{n-k}\epsilon&amp;rsquo;M\epsilon.$$ This is useful because it is a &lt;em&gt;quadratic form&lt;/em&gt;, as we described above. Quadratic forms over multivariate normal random vectors have nice properties which we will now derive. Specifically we will show that: $$\hat{\sigma}^2 \sim \frac{\sigma^2}{n-k}\chi^2_{n-k}.$$&lt;/p&gt;
&lt;p&gt;This proof will involve some additional use of linear algebraic terms and assumptions. Specifically, we will use the fact that a symmetric matrix $A$ can be decomposed into $A = PDP^T$ for an &lt;em&gt;orthogonal&lt;/em&gt; (i.e. $P^2 = I$) matrix $P$ and a diagonal matrix $D$. Orthogonal matrices are nice because $\tilde{z} = Pz$, the product of an orthogonal matrix with a standard normal vector, is still a standard normal vector.&lt;/p&gt;
&lt;p&gt;We will also use the fact that if $A$ is symmetric &lt;em&gt;and&lt;/em&gt; idempotent then all the diagonal entries are either 0 or 1. We will further use the fact that the number of entries that are 1 is equal to the trace of $A$. Since $M$ is symmetric and idempotent we will use all these properties to show:
\begin{align*}
\epsilon&amp;rsquo;M\epsilon
&amp;amp; = \epsilon&amp;rsquo;PDP^T\epsilon \\&lt;br&gt;
&amp;amp; = \sigma^2 z&amp;rsquo;PDP^Tz \\&lt;br&gt;
&amp;amp; = \sigma^2 \tilde{z}&amp;lsquo;D\tilde{z} \\&lt;br&gt;
&amp;amp; = \sigma^2 \sum_{i=1}^n D_{ii}\tilde{z}_i^2 \\&lt;br&gt;
&amp;amp; = \sigma^2 \sum_{i=1}^{n-k} \tilde{z}_i^2 \\&lt;br&gt;
&amp;amp; \sim \sigma^2 \chi^2_{n-k}.
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dirichlet BCs</title>
      <link>/courses/gaussian-processes/dirichlet/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/dirichlet/</guid>
      <description>&lt;p&gt;The simplest version of the finite element method is said to have &amp;ldquo;natural&amp;rdquo; Neumann boundary conditions, meaning that Neumann boundary conditions are naturally satisfied without imposing any additional structure. Dirichlet boundary conditions then are said to be &amp;ldquo;essential,&amp;rdquo; meaning they must be explicitly imposed after the fact. Certain FEM formulations change up this Neumann=natural, Dirichlet=essential pradigm but for our purposes we will treat these terms as interchangeable.&lt;/p&gt;
&lt;p&gt;Now, suppose that we are confronted with the FEM equation $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ K_{21} &amp;amp; K_{22} \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11} &amp;amp; L_{12} \\ L_{21} &amp;amp; L_{22} \end{bmatrix} \begin{bmatrix} f_1 \\ f_2 \end{bmatrix},$$
with the Dirichlet BC $u_2 = u^*$. We can modify our FEM equation to enforce this, to $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ 0 &amp;amp; I \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}.$$ We can simplify this to an equation for $u$ by inverting the matrix on the left: $$\begin{bmatrix} u_1 \\ u_2 \end{bmatrix} = \begin{bmatrix} K_{11}^{-1} &amp;amp; -K_{11}^{-1}K_{12} \\ 0 &amp;amp; I \end{bmatrix} \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}$$
Now, assuming that $f\sim N(\mu_f,Q_f)$ and $u^*$ is given (often $u^* = 0$) we obtain the following distribution for $u_1$:
\begin{align*}
u_1 &amp;amp; = K_{11}^{-1}L_1f - K_{11}^{-1}K_{12}u^* \\&lt;br&gt;
u_1 &amp;amp; \sim N\left(K_{11}^{-1}(L_1\mu_f - K_{12}u^*), K_{11}^{-1}L_1\Sigma_fL_1^TK_{11}^{-T}\right).
\end{align*}&lt;/p&gt;
&lt;p&gt;Now often we are interested in the posterior distribution for $u$ and/or $f$. Since we have assumed that $u^*$ is given the two are deterministically related. However, $u$ is of lower dimension than $f$. So, we will try to calculate the posterior distribution of $f$ since this can be used to calculate the posterior of $u$ but not vice versa. We will assume that we have made some observations $y = y_1 + y_2 = A_1u_1 + A_2u_2 + \epsilon$ of $u$ from which we wish to make inference. Since $u^*$ is known we essentially have observations:
\begin{align*}
y &amp;amp; = A_1K_{11}^{-1}(L_1f - K_{12}u^*) + A_2u^* + \epsilon \\&lt;br&gt;
&amp;amp; = A_1K_{11}^{-1}L_1f + (A_2 - A_1K_{11}^{-1}K_{12})u^* + \epsilon \\&lt;br&gt;
&amp;amp; = y_f + y^* + \epsilon
\end{align*}
Using the 
&lt;a href=&#34;../posteriors/#linear-observations&#34;&gt;results&lt;/a&gt;
 for a posterior from a linearly-observed MVN distribution we see that:
\begin{align*}
[f|y=a] &amp;amp; = N\left(\mu_f + Q_{f|y}^{-1}B^TQ_{\epsilon}(y - y^* - B^T\mu_f), Q_{f|y}^{-1}\right) \\&lt;br&gt;
Q_{f|y} &amp;amp; = Q_f + B^TQ_{\epsilon}B \\&lt;br&gt;
B &amp;amp; = A_1K_{11}^{-1}L_1
\end{align*}&lt;/p&gt;
&lt;p&gt;So the challenges that we face in implementing these two different ideas are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u_1$ is lower dimensional than $f$ and so $f$ is not uniquely defined by $u_1$&lt;/li&gt;
&lt;li&gt;$L_1\Sigma_fL_1^T$ is not easily invertible since $L_1$ is not square&lt;/li&gt;
&lt;li&gt;OR: $K_{11}^{-1}$ is dense&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My solution to this is to take the approach of modeling at the $u$ level. We can deal with $u_1$ being lower dimensional by calculating the degenerate MVN distribution for $f|u_1$. We can deal with $L_1$ being rectangular by considering our old approximation $\tilde{L}$. Using this approximation, which we have always needed to do to keep $L^{-1}$ sparse, we have that $(\tilde{L}_1\Sigma_f\tilde{L}_1^T)_{ij} = \tilde{L}_{ii}\Sigma_{ij(f)}\tilde{L}_{jj}$ for $1\leq i,j\leq n_1$. This is exactly the same as if we truncated the additional rows and columns of $\tilde{L}$ and $\Sigma_f$, since $\tilde{L}$ is zero in all of those columns anyways. Thus we obtain
\begin{align*}
(L_1\Sigma_fL_1^T)^{-1}
&amp;amp; \approx (\tilde{L}_1\Sigma_f\tilde{L}_1^T)^{-1} \\&lt;br&gt;
&amp;amp; = (\tilde{L}_{11}\Sigma_{11(f)}\tilde{L}_{11})^{-1} \\&lt;br&gt;
&amp;amp; = \tilde{L}^{-1}_{11}\Sigma_{11(f)}^{-1}\tilde{L}_{11}^{-1} \\&lt;br&gt;
&amp;amp; = \tilde{L}^{-1}_{11}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\tilde{L}^{-1}_{11}.
\end{align*}
Since $u_2$ is low dimensional, we can easily invert the dense $Q_{22}$ and we still end up with a sparse matrix $Q_{11(u)}$. So we have solved problem (1).&lt;/p&gt;
&lt;p&gt;Now we hit up against the following problem: we may conduct efficient inference on $u_1$, but even though $u_1$ and $f$ are deterministically linked, $f$ is not uniquely identified by $u_1$. So we need to find the degenerate MVN distribution defining $f|u_1$. Alternatively we can assume that there is some small noise that enters between $f$ and $u_1$ so that the two are &lt;em&gt;not&lt;/em&gt; deterministically linked and so that we have a non-degenerate distribution for $f$. This becomes more important if we have measurements for $f$ as well as $u$. For now we will assume this is not the case.&lt;/p&gt;
&lt;p&gt;The primary obstacle we need to overcome here is how to obtain inference for a degenerate distribution while maintaining our sparsity paradigm, which depends upon the inverse of the covariance matrix. To obtain the distribution for $f|u_1$ we will consider the following algebra:
\begin{align*}
[f|u_1]
&amp;amp; = [f_1,f_2|u] \\&lt;br&gt;
&amp;amp; = [f_1|u,f_2][f_2|u].
\end{align*}
So what we are able to do here is specify each of these distributions separately. Since all we really need to be able to do is calculate the posterior mean of $f$ and simulate draws of $f|u$, if we can do this we are finished. So what are these distributions? Once $f_2$ is specified, $f_1$ is uniquely determined by $u_1$ so we find that: $$[f_1|u_1,f_2] = L_{11}^{-1}(K_{11}u_1 + K_{12}u^* - L_{12}f_2).$$ So then all we really need to sample is $[f_2|u]$. This can be calculated using Bayes formula:
\begin{align*}
[f_2|u]
&amp;amp; = [u|f_2][f_2] \\&lt;br&gt;
&amp;amp; = [u_1|f_2,u^*][f_2]
\end{align*}
Then we can use the formula $$K_{11}u_1 + K_{12}u^* = L_{11}f_1 + L_{12}f_{2}$$
to obtain $$u_1 = K_{11}^{-1}(L_{11}f_1 + L_{12}f_2 - K_{12}u^*)$$
and the (non-degenerate MVN) distribution
\begin{align*}
u_1
&amp;amp; = N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}L_{11}\Sigma_{11(f)}L_{11}K_{11}^{-T}\right) \\&lt;br&gt;
&amp;amp; \approx N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\bar{L}_{11}^{-1}\Sigma_{11(f)}^{-1}\bar{L}_{11}^{-1}K_{11}\right) \\&lt;br&gt;
&amp;amp; = N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\bar{L}_{11}^{-1}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\bar{L}_{11}^{-1}K_{11}\right)
\end{align*}
Now this distribution bears a remarkable resemblance to what we have previously written for $[u_1]$ with no conditioning at all, since our approximation $\tilde{L}$ removes the direct dependence of $u_1$ on $f_2$. However, it is important to note that in this formulation we _actually_ have the matrix $L_{11}$, rather than a truncation of $\tilde{L}$. Thus we need to approximate $L_{11}$ directly rather than approximation $\tilde{L}\approx L$. Thus we get what we have denote $\bar{L}_{11}\approx L_{11}$ which we obtain by grouping all the terms in $L_{11}$ onto the diagonal (the same operation which gives us $\tilde{L}$ from $L$). Since a number of non-zero terms from $L$ are removed by this operation, and in particular these non-zero terms are along the edges of $u_1$ where it abuts $u_2$, we see that in fact this is a substantively different distribution. Our approximation for $[u_1|f_2]$ has less uncertainty along the boundary, just as we would expect. Now that we have this distribution all we need do is find the unconditional distribution $[f_2]$, which may easily be found by blockwise inversion of $Q_{f}$: $$[f_2] = N(\mu_{2(f)},(Q_{22} - Q_{21}Q_{11}^{-1}Q_{12})^{-1}).$$ This involves a dense $n_2\times n_2$ matrix but since the boundary is lower-dimensional this is okay.&lt;/p&gt;
&lt;p&gt;Now actually calculating this full posterior involves some &lt;em&gt;heavy&lt;/em&gt; algebra and I&amp;rsquo;m sure there&amp;rsquo;s a better way to do this but here goes:
\begin{align*}
[f_2|u_1]
&amp;amp; \propto [u_1|f_2][f_2] \\&lt;br&gt;
&amp;amp; = N\left(u_1|K_{11}^{-1}(L_{11}\mu_1 + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}\bar{L}_{11}\Sigma_{11}\bar{L}_{11}K_{11}^{-T}\right)N\left(f_2|\mu_2,\Sigma_{22}\right) \\&lt;br&gt;
&amp;amp; \propto \exp\left(-\frac{1}{2}(u_1 - \mu_{u|f})^TK_{11}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}K_{11}(u_1 - \mu_{u|f})\right)\exp\left(-\frac{1}{2}(f_2-\mu_2)^T\Sigma_{22}^{-1}(f_2-\mu_2)\right) \\&lt;br&gt;
&amp;amp; \propto \exp\left(-\frac{1}{2}f^T(\Sigma_{22}^{-1}+L_{12}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}L_{12})f - \frac{1}{2}(f^Tv + v^Tf)\right) \\&lt;br&gt;
v &amp;amp; = L_{12}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}(L_{11}\mu_1 + L_{12}f_2 - K_{12}u^* - K_{11}u_1) - f_2^T\Sigma_{22}^{-1}\mu_2
\end{align*}
Thus we see that $[f_2|u_1]$ is normal with precision matrix:
\begin{align*}
Q_{f_2|u_1}
&amp;amp; = \Sigma_{22}^{-1} + L_{21}\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}L_{12} \\&lt;br&gt;
&amp;amp; = Q_{22} - Q_{21}Q_{11}^{-1}Q_{12} + L_{21}\bar{L}_{11}^{-1}(Q_{11} - Q_{12}Q_{22}^{-1}Q_{21})\bar{L}_{11}^{-1}L_{12} \\&lt;br&gt;
\mu_{f_2|u_1}
&amp;amp; = -Q_{f_2|u_1}^{-1}v
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posteriors</title>
      <link>/courses/gaussian-processes/posteriors/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/posteriors/</guid>
      <description>&lt;p&gt;The multivariate normal distribution has nicely behaved posterior distributions. In particular, the posterior given an observation of the vector is also multivariate normal.&lt;/p&gt;
&lt;h2 id=&#34;basic-posterior-&#34;&gt;Basic Posterior&lt;/h2&gt;
&lt;p&gt;Let $x$ be an $n\times 1$ vector partitioned into $x = (x_1,x_2)$, with $x_1$ having dimension $k$ and $x_2$ having dimension $n-k$. Partition the mean $\mu = (\mu_1,\mu_2)$ and covariance matrix $$\Sigma = \begin{bmatrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix}.$$ Then the posterior distribution for $x_1$ given $x_2=a$ is given by
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(a-\mu_2), \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;precision-formulation&#34;&gt;Precision Formulation&lt;/h2&gt;
&lt;p&gt;Often in my research we are interested in analyzing a MVN distribution with known sparse &lt;em&gt;precision&lt;/em&gt; matrix. It is expensive to invert matrices and cheap to work with sparse matrices so we wish to work directly with this precision matrix. Furthermore, we want to calculate the precision matrix for our posterior because it is likely to also be computational advantageous. Let the precision matrix be partitioned
$$Q = \begin{bmatrix} Q_{11} &amp;amp; Q_{12} \\ Q_{21} &amp;amp; Q_{22} \end{bmatrix}.$$
Then the equivalent posterior distribution is
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + Q_{11}^{-1}Q_{12}(a-\mu_2),Q_{11}\right).
\end{align*}
Furthermore, let $RR^T = Q$ be the Cholesky decomposition of $Q$ also be partitioned into blocks. If $Q$ is sparse then under mild conditions, $R$ is also sparse and we can work with it to do quick computation. The posterior distribution now is $$[x_1|x_2=a] = N\left(\mu_1 - R_{11}^{-1}(R_{11}^{-T}(Q_{12}(a-\mu_2))), Q_{11}\right).$$&lt;/p&gt;
&lt;h2 id=&#34;linear-observations&#34;&gt;Linear Observations&lt;/h2&gt;
&lt;p&gt;Often we are also interested in analyzing a MVN distribution where we observe not the individual components, but a linear combination thereof $y = Ax + \epsilon$ with some mean-zero MVN noise vector $\epsilon$. Then the observations are distributed $y|x \sim N( Ax , Q_\epsilon^{-1} )$. The posterior $x|y$ is given by:
\begin{align*}
[x|y=a] &amp;amp; = N\left(\mu_x + Q_{x|y}^{-1}A^TQ_{\epsilon}(y-A\mu_x), Q_{x|y}\right) \\&lt;br&gt;
Q_{x|y} &amp;amp; = Q + A^TQ_{\epsilon}A
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Properties of Linear Regression</title>
      <link>/courses/qsci483/linear-regression/properties/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/properties/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to analyze our previous results for linear regression analysis. Make sure to check out my previous posts on 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 and 
&lt;a href=&#34;/courses/qsci483/linear-regression/linear-regression&#34;&gt;simple linear regression&lt;/a&gt;
 before diving in.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Recall that linear regression is based upon the equation:
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. We have previously found that the estimate $\hat{\beta} = (X&amp;rsquo;X)^{-1}Xy$ minimizes the sum of squares. In this document we will show: (1) that $\hat{\beta}$ is also the maximum likelihood estimator, (2) that $\hat{\beta}$ is unbiased, and (3) the covariance matrix for the estimator $\hat{\beta}$.&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-&#34;&gt;Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;Our probability model has that $y_i$ are normally distributed, and are independent of each other given the predictors $X$ and the coefficients $\beta$. The &lt;em&gt;likelihood function&lt;/em&gt; is given by the probability (density) of the data given the parameters ($\beta$), expressed as a function of the parameter estimate ($\hat{\beta}$). This function in our case can be written as a product of Gaussian (normal) probability density functions (pdfs):
\begin{align*}
\ell(\hat{\beta}) &amp;amp; = \prod_{i=1}^n N(y_i|X\hat{\beta},\sigma^2) \\&lt;br&gt;
&amp;amp; = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - X\hat{\beta})}{2\sigma^2}\right) \\&lt;br&gt;
&amp;amp; = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right).
\end{align*}
We will now make use of a common trick in statistics: we will calculate the log-likelihood function $\lambda(\hat{\beta}) = \log(\ell(\hat{\beta}))$. Since the probability density function is always non-negative, and therefore the likelihood is always non-negative, the log-likelihood can be defined. Furthermore, the log is a convex function, and because of this it has the property that $\ell$ and $\lambda$ are minimized at the same value $\hat{\beta}$. Why this is is not important for this class.&lt;/p&gt;
&lt;p&gt;So taking the log we obtain
\begin{align*}
\lambda(\hat{\beta})
&amp;amp; = \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right)\right) \\&lt;br&gt;
&amp;amp; = \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\right) + \log\left(\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right)\right) \\&lt;br&gt;
&amp;amp; = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-X\hat{\beta})^2.
\end{align*}
Now remember that we are looking for the _maximum likelihood estimator_. So we want to find the value of $\hat{\beta}$ which maximizes the likelihood (i.e. maximizes the probability of that data, given the parameters). Viewing this as a function of $\hat{\beta}$ we see that maximizing $\lambda$ is equivalent to minimizing
$$
\sum_{i=1}^n (y_i-X\hat{\beta})^2.
$$
Therefore the maximum likelihood estimator (MLE) of $\hat{\beta}$ is exactly the least-squares estimator $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. Importantly, we are at this point omitting the parameter $\sigma^2$. In fact, the MLE for $\hat{\beta}$ is unchanged if we estimate this parameter as well. The MLE for $\hat{\sigma^2}$ is given by
\begin{align*}
\hat{\sigma^2}
&amp;amp; = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 \\&lt;br&gt;
&amp;amp; = \frac{1}{n}y&amp;rsquo;(I-X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)y.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;unbiasedness-&#34;&gt;Unbiasedness&lt;/h3&gt;
&lt;p&gt;It is important to remember that estimators (such as $\hat{\beta}) are themselves &lt;em&gt;random&lt;/em&gt;. If we were to simulate from our model, holding $\beta$ fixed, we would fit a different estimator $\hat{\beta}$ to every simulation. Thus an important quality that an estimator can have is &lt;em&gt;unbiasedness&lt;/em&gt;. This means that, if the model is correct, the estimator will neither tend to overestimate nor underestimate the true parameter. In mathematical terms, its expectation (or mean) is correct: $E[\hat{\beta}] = \beta$.&lt;/p&gt;
&lt;p&gt;Using our matrix math this property can be derived fairly quickly. We know that $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. We also know that $y = X\beta + \epsilon$. Putting these together we see:
\begin{align*}
\hat{\beta}
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;(X\beta + \epsilon) \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;X\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon \\&lt;br&gt;
&amp;amp; = \beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon.
\end{align*}
Therefore the mean is
\begin{align*}
E[\hat{\beta}]
&amp;amp; = E\left[\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right].
\end{align*}
The mean has a useful property which we will use here, which is that it is _linear_. This means that the expectation of a sum is the sum of the expectations. In math we write this as $E[A+B] = E[A] + E[B]$. Since matrix operations are essentially just a bunch of sums, we can also write $E[Mv] = ME[v]$, if $v$ is random and $M$ is constant. Using this property we get
\begin{align*}
E[\hat{\beta}]
&amp;amp; = E\left[\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right] \\&lt;br&gt;
&amp;amp; = \beta + E\left[(X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right] \\&lt;br&gt;
&amp;amp; = \beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;E[\epsilon] \\&lt;br&gt;
&amp;amp; = \beta
\end{align*}
since we have assumed that $\epsilon$ is zero-mean noise. So, in fact, our estimator is unbiased!&lt;/p&gt;
&lt;h3 id=&#34;covariance-of-hatbeta-&#34;&gt;Covariance of $\hat{\beta}$&lt;/h3&gt;
&lt;p&gt;Remember that $\hat{\beta}$ is fundamentally a &lt;em&gt;random&lt;/em&gt; quantity. It is different in every realization (or simulation) of the statistical model we have written down. It is sensitive to the addition of random noise ($\epsilon_{i}$) to the data. Luckily, since we have written down a statistical model for our data $y$ we are able to do a statistical analysis for the estimator $\hat{\beta}$ to determine its random properties.&lt;/p&gt;
&lt;p&gt;We showed in the previous section that $\hat{\beta}$ was unbiased, that is, it has its mean $E[\hat{\beta}] = \beta$ at the correct place. We will now calculate the variance-covariance (or just covariance) matrix of $\hat{\beta}$. This tells us how much we can expect $\hat{\beta}$ to vary for different datasets. The &lt;em&gt;diagonal&lt;/em&gt; of this covariance matrix tells us the variances of $\hat{\beta}_{i}$, which are important quantities that get used, for example, in calculating Rs model summaries.&lt;/p&gt;
&lt;p&gt;The covariance of two random variables is given by $E[(A-E[A])(B-E[B])]$. In our case $A = \hat{\beta}_{i}$ and $B = \hat{\beta}_{j}$. This will give us the entry $\Sigma_{ij}$ in the covariance matrix. We already know that $E[\hat{\beta}_{i}] = \beta_{i}$ since the estimators are unbiased. Therefore the covariance is
$$
\Sigma_{ij} = E\left[(\hat{\beta}_{i} - \beta_{i})(\hat{\beta}_{j} - \beta_{j})\right].
$$
Expanding the quadratic in the middle, and remembering that expectations are linear, we get that
\begin{align*}
\Sigma_{ij}
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - E[\beta_i\hat{\beta}_j] - E[\hat{\beta}_i\beta_j] + E[\beta_i\beta_j] \\&lt;br&gt;
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - \beta_iE[\hat{\beta}_j] - E[\hat{\beta}_i]\beta_j + \beta_i\beta_j \\&lt;br&gt;
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - \beta_i\beta_j.
\end{align*}&lt;/p&gt;
&lt;p&gt;This is a useful formula, but it would be far more effective to analyze this problem in terms of matrix notation. Let&amp;rsquo;s go ahead and make that switch. Using matrix notation, the single entry
$$
\Sigma_{ij} = E\left[(\hat{\beta}_i - \beta_i)(\hat{\beta}_j - \beta_j)\right].
$$
can be written as a whole matrix
$$
\Sigma = E\left[(\hat{\beta} - E[\hat{\beta}])(\hat{\beta} - E[\hat{\beta}])&#39;\right].
$$
Since we know that the estimator is unbiased we can plug this into the matrix equation to get
\begin{align*}
\Sigma
&amp;amp; = E\left[(\hat{\beta} - \beta)(\hat{\beta}-\beta)&#39;\right] \\&lt;br&gt;
&amp;amp; = E\left[\hat{\beta}\hat{\beta}&#39;\right] - \beta\beta&amp;rsquo;.
\end{align*}
This is exactly the same formula we have above, just written in matrix notation. Now, we already have a formula for $\hat{\beta}$ in matrix notation, $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$, so let&amp;rsquo;s plug this in here. We get
\begin{align*}
\Sigma
&amp;amp; = E\left[(X&amp;rsquo;X)^{-1}X&amp;rsquo;yy&amp;rsquo;X(X&amp;rsquo;X)^{-1}\right] - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo; E[yy&amp;rsquo;] X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo;.
\end{align*}
Now we can plug in our formula for $y = X\beta + \epsilon$. Remembering that expectations are linear (i.e. can be split up over summations), we get
\begin{align*}
E[yy&amp;rsquo;]
&amp;amp; = E\left[(X\beta + \epsilon)(X\beta+\epsilon)&#39;\right] \\&lt;br&gt;
&amp;amp; = E\left[X\beta\beta&amp;rsquo;X&amp;rsquo; + \epsilon X\beta + \beta&amp;rsquo;X&amp;rsquo;\epsilon + \epsilon\epsilon&amp;rsquo;\right] \\&lt;br&gt;
&amp;amp; = X\beta\beta&amp;rsquo;X&amp;rsquo; + E[\epsilon]X\beta + \beta&amp;rsquo;X&amp;rsquo;E[\epsilon] + E[\epsilon\epsilon&amp;rsquo;]
\end{align*}
Now we already know that $E[\epsilon] = 0$. What about $E[\epsilon\epsilon&amp;rsquo;]$? Well,
$$
\mbox{cov}(\epsilon_i,\epsilon_j) = E[\epsilon_i\epsilon_j] - E[\epsilon_i]E[\epsilon_j] = E[\epsilon_i\epsilon_j].
$$
Since independent variables are uncorrelated (and we know that the $\epsilon_i$ are iid) we see that this matrix is diagonal with entries equal to $\sigma^2$, the variance of $\epsilon_i$. Thus:
$$
E[yy&amp;rsquo;] = X\beta\beta&amp;rsquo;X&amp;rsquo; + \sigma^2I.
$$
Finally, plugging this in, we can find that
\begin{align*}
\Sigma &amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo; E[yy&amp;rsquo;] X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;(X\beta\beta&amp;rsquo;X&amp;rsquo; + \sigma^2I)X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}(X&amp;rsquo;X)\beta\beta&amp;rsquo;(X&amp;rsquo;X)(X&amp;rsquo;X)^{-1} + (X&amp;rsquo;X)^{-1}X&amp;rsquo;(\sigma^2I)X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = \beta\beta&amp;rsquo; + \sigma^2(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = \sigma^2(X&amp;rsquo;X)^{-1}.
\end{align*}
This is the covariance matrix of $\hat{\beta}$! Notably, this has two important properties. One, it depends on $\sigma^2$, so if we want to use this we had better estimate $\sigma^2$ somehow. More on this in the next section. Two, it depends on the _inverse_ of $X&amp;rsquo;X$. There&amp;rsquo;s no guarantee that this matrix is invertible. For example, if we have more predictors than we have data points (i.e. $k &amp;gt; n$) this matrix will certainly _not_ be invertible. You may have been warned about this scenario in the past. However, even if you have many data points, other situations can crop up where $X&amp;rsquo;X$ is either numerically difficult to invert (i.e. difficult to calculate on a computer), or produces very large variances. One such case is where two of the predictor variables are very highly correlated. More on this later.&lt;/p&gt;
&lt;h3 id=&#34;other-quantities-&#34;&gt;Other Quantities&lt;/h3&gt;
&lt;p&gt;Here we will show briefly that $\hat{y}$ is also unbiased (assuming the model is correct). This can be seen by some matrix calculations:
\begin{align*}
E[\hat{y}]
&amp;amp; = E[Hy] \\&lt;br&gt;
&amp;amp; = E[X(X&amp;rsquo;X)^{-1}X&amp;rsquo;(X\beta+\epsilon)] \\&lt;br&gt;
&amp;amp; = E[X\beta + H\epsilon] \\&lt;br&gt;
&amp;amp; = X\beta + HE[\epsilon] \\&lt;br&gt;
&amp;amp; = X\beta.
\end{align*}
Meanwhile, $\hat{\epsilon}$ is:
\begin{align*}
\hat{\epsilon}
&amp;amp; = My \\&lt;br&gt;
&amp;amp; = (I-H)y \\&lt;br&gt;
&amp;amp; = y - (X\beta + H\epsilon) \\&lt;br&gt;
&amp;amp; = (I-H)\epsilon \\&lt;br&gt;
&amp;amp; = M\epsilon
\end{align*}
This is unbiased in the sense that it has the same mean as $\epsilon$. Unfortunately, although it might seem we can simply calculate $\epsilon = M^{-1}\hat{\epsilon}$, this matrix may not be invertible (TODO: I&amp;rsquo;m like 99% certain it is always uninvertible)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Moore-Penrose Pseudoinverse</title>
      <link>/courses/gaussian-processes/pseudoinverse/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/pseudoinverse/</guid>
      <description>&lt;p&gt;The Moore-Penrose Pseudoinverse is a generalization of the inverse. In particular it extends the inverse matrix to non-square matrices. The inverse of a matrix $A$ is defined by any matrix $A^+$ with the following four properties&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$AA^+A = A$&lt;/li&gt;
&lt;li&gt;$A^+AA^+ = A^+$&lt;/li&gt;
&lt;li&gt;$(AA^+)^* = AA^+$&lt;/li&gt;
&lt;li&gt;$(A^+A)^* = A^+A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = (A^&lt;em&gt;A)^{-1}A^&lt;/em&gt;$ and the pseudoinverse is a &lt;em&gt;left inverse&lt;/em&gt; since $A^+A = I$. If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = A^*(AA^*)^{-1}$ and the pseudoinverse is a _right inverse_ since $AA^+ = I$.&lt;/p&gt;
&lt;p&gt;Finally, consider $(AB)^+$. This is equal to $B^+A^+$ if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A$ has orthonormal columns (i.e. $A^*A = I$), or&lt;/li&gt;
&lt;li&gt;$B$ has orthonormal rows (i.e. $BB^* = I$), or&lt;/li&gt;
&lt;li&gt;$A$ has full column rank and $B$ has full row rank, or&lt;/li&gt;
&lt;li&gt;$B = A^*$
In general, however, $(AB)^+ \neq $B^+A^+$.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression Model Diagnostics</title>
      <link>/courses/qsci483/linear-regression/math-diagnostics/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/math-diagnostics/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to derive and analyze the statistics (calculated quantities) for a linear regression model. Make sure to check out my previous posts before diving in. Recall that linear regression is based upon the equation:
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. We have previously derived the estimators $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$, as well as for $\mbox{cov}(\hat{\beta}) = \sigma^2(X&amp;rsquo;X)^{-1}$.&lt;/p&gt;
&lt;h2 id=&#34;measures-of-fit&#34;&gt;Measures of Fit&lt;/h2&gt;
&lt;h3 id=&#34;the-t-statistic-&#34;&gt;The $t$ statistic&lt;/h3&gt;
&lt;p&gt;The most popular statistic used for linear regression is $t_i = \hat{\beta}_i/\hat{\sigma}(\hat{\beta}_i)$. Since we have shown that $\hat{\beta}$ is distributed as a multivariate normal random vector, $\hat{\beta}_i$ is distributed as a normal random variable. Furthermore, $\hat{\sigma}(\hat{\beta}_i)$, the standard error for this estimate, is a product of the standard error $\hat{\sigma}(\epsilon)$ and the analytical standard deviation of $\beta_i$ (for $\sigma = 1$) which is just the $\sqrt{(X&amp;rsquo;X)^{-1}_{ii}}$. The important thing is we have a normal variable and its standard error, and therefore this statistic is distributed as a $t$ random variable with $(n-k)$ degrees of freedom (recall $n$ is the number of data points and $k$ is the number of predictors).&lt;/p&gt;
&lt;h3 id=&#34;the-f-statistic-&#34;&gt;The $F$ statistic&lt;/h3&gt;
&lt;p&gt;The $F$ test for the overall model is a formal test with hypothesis:
\begin{align*}
H_0: &amp;amp; ~~~\beta_i = 0 \mbox{ for all } i \\&lt;br&gt;
H_1: &amp;amp; ~~~\beta_i \neq 0 \mbox{ for some }i.
\end{align*}
Notably we will be assuming that there _is_ still a non-zero constant mean term even under $H_0$. If we do not assume this then we can basically set all the $\overline{y}$ terms equal to zero and remove the 1 degree of freedom from the null hypothesis terms. To test this we need to calculate a few important quantities, all of which are sums of squares:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE (sum of squares: error)
&lt;ul&gt;
&lt;li&gt;This is basically just the sum of squares for all the residuals&lt;/li&gt;
&lt;li&gt;SSE~$ = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \epsilon_i^2$&lt;/li&gt;
&lt;li&gt;We showed 
&lt;a href=&#34;../standard-error&#34;&gt;previously&lt;/a&gt;
 that SSE$\sim \chi^2_{n-k}$&lt;/li&gt;
&lt;li&gt;If you look back at the derivation you will see that this is true &lt;em&gt;regardless&lt;/em&gt; of $\beta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SSM (sum of squares: model)
&lt;ul&gt;
&lt;li&gt;This is basically how much extra variation from the mean the model explains&lt;/li&gt;
&lt;li&gt;SSM~$ = \sum_{i=1}^n (\hat{y}_i - \overline{y})^2$&lt;/li&gt;
&lt;li&gt;This turns out to &lt;em&gt;also&lt;/em&gt; be a $\chi^2$ random variable&lt;/li&gt;
&lt;li&gt;We can prove this using the same basic argument. Let $J = 11^T$ be a matrix of all 1s. Then: $$\mbox{SSM} = y&amp;rsquo;(H-\frac{1}{n} J)&#39;(H-\frac{1}{n} J)y.$$ However $H - \frac{1}{n}J$ is symmetric and idempotent, with rank $k-1$. Thus SSM$\sim \chi^2_{k-1}$ following the same logic from 
&lt;a href=&#34;../standard-error&#34;&gt;before&lt;/a&gt;
.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SST (sum of squares: total)
&lt;ul&gt;
&lt;li&gt;This is how much total variation this is around the mean&lt;/li&gt;
&lt;li&gt;SST~$ = \sum_{i=1}^n (y_i-\overline{y})^2$&lt;/li&gt;
&lt;li&gt;This is also $\chi^2_{n-1}$ (prove this yourself?)
Remarkably SSE+SSM=SST, which can be used as an elementary proof that SST is $\chi^2_{n-1}$ (though there are other ways). This is the famous variance decomposition for ANOVA models. Why is this true?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO: what is an ANOVA?&lt;/p&gt;
&lt;h4 id=&#34;variance-decomposition-&#34;&gt;Variance Decomposition&lt;/h4&gt;
&lt;p&gt;We can think about the variance decomposition several ways. The simplest proof is geometric in nature but requires jumping through some linear-subspace hoops. We can think about three points in $\mathbb{R}^n$, $n$-dimensional space where our datapoints $y$ live. One point is just the data vector $y$. Another point is $\hat{y}_i = \hat{y}_i$ a third point is $\overline{y}_i = \overline{y}$. We can also define $k$ vectors in this space, corresponding to the values of our predictors $x^{(j)}_i = X_{ij}$. These $k$ vectors define a $k$-dimensional linear subspace of $\mathbb{R}^n$ (which you can think of as $X\beta$ for all of the possible $k$-dimensional values of $\beta$. The prediction vector $\hat{y}$ is nothing but the projection of $y$ onto the closest point of this subspace. Since one of the predictors is the mean (i.e. $X_{i1} = 1$) the mean point $\hat{y}$ falls &lt;em&gt;inside&lt;/em&gt; this linear subspace. This gives us the following geometric intuition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean is in the subspace&lt;/li&gt;
&lt;li&gt;The projection is in the subspace&lt;/li&gt;
&lt;li&gt;The projection is the &lt;em&gt;closest&lt;/em&gt; point in the subspace to the data. This is because the projection minimizes the sum of squares, or the squared distance between the subspace and the data.&lt;/li&gt;
&lt;li&gt;Therefore the vector leading from the data to the projection is &lt;em&gt;orthogonal&lt;/em&gt; or perpendicular to the subspace&lt;/li&gt;
&lt;li&gt;In particular the vector leading from the data to the projection is orthogonal to the vector leading from the projection to the mean
Therefore we can apply the Pythagorean theorem, to find that the squared distance between the data and the projection (prediction) plus the squared distance between the projection and the mean is equal to the squared distance between the data and the mean. If you sit down and think about what these &amp;ldquo;squared distance&amp;rdquo; mean in mathematical terms you will find that this is a geometric proof of SSE+SSM=SST.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also think about the variance decomposition in terms of raw linear algebra and matrices: TODO&lt;/p&gt;
&lt;p&gt;A common &lt;em&gt;mistake&lt;/em&gt; made in deriving the variance decomposition is noting that $$(y_i - \hat{y}_i) + (\hat{y}_i - \overline{y}) = (y_i-\overline{y}).$$ The &lt;em&gt;false&lt;/em&gt; argument then goes: square the terms and sum and the equality holds. However, as you likely know $A+B=C$ does &lt;em&gt;not&lt;/em&gt; imply that $A^2+B^2=C^2$. We require the additional structure of linear algebra and/or geometry to obtain this result&lt;/p&gt;
&lt;h4 id=&#34;back-to-the-f-test-&#34;&gt;Back to the $F$ test&lt;/h4&gt;
&lt;p&gt;We can now adjust SSE and SSM to get the &amp;ldquo;mean sum of squares,&amp;rdquo; by dividing by the degrees of freedom:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE = SSE$/(n-k)$&lt;/li&gt;
&lt;li&gt;MSM = SSM$/(n-1)$
Finally, our test statistic is $F = \mbox{MSM}/\mbox{MSE}$. In order for this to be a valid $F$ statistic, SSM and SSE must be independent. This turns out to be true but I will not prove it here (TODO).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that if the model explains a lot more variation than just the mean we expect for MSE to be small and MSM to be large: make sure this make sense to you. So if the model is &amp;ldquo;real&amp;rdquo; then we expect $F$ to be large. If the null hypothesis is true then we expect $F$ to be small. So we reject the null for large values of $F$.&lt;/p&gt;
&lt;h3 id=&#34;the-r2-&#34;&gt;The $R^2$&lt;/h3&gt;
&lt;p&gt;There are numerous ways to calculate different versions of $R^2$ but at the end of the day they all boil down to correlation coefficients: the correlation $R$ between the predictors and the response variable. We square it because this gives us a measure of the proportion of variance explained by the predictors.&lt;/p&gt;
&lt;h4 id=&#34;basic-r2-&#34;&gt;Basic $R^2$&lt;/h4&gt;
&lt;p&gt;The most basic version of this statistic is just the correlation coefficient between the the predictors and the response. It is also known as the coefficient of determination. It is calculated as $SSM/SST$. This gives it the nice interpretation of being the ratio of &amp;ldquo;variance explained by the model&amp;rdquo; to &amp;ldquo;total variance around the mean.&amp;rdquo; It can also be defined mathematically as the square of the Pearson correlation coefficient between $y$ and $\hat{y}$:
\begin{align*}
R^2 &amp;amp; = \left(\frac{\sum_{i=1}^n (y_i-\overline{y})(\hat{y}_i - \overline{y})}{\sqrt{\sum_{i=1}^n (y_i-\overline{y})^2}\sqrt{\sum_{i=1}^n (\hat{y}_i - \overline{y})^2}}\right)^2
\end{align*}&lt;/p&gt;
&lt;p&gt;However, the raw $R^2$ calculation has some problems as a measure of model fit. For one thing, $R^2$ &lt;em&gt;always&lt;/em&gt; increases whenever you add a new predictor. This is because $\hat{y}$ will always get closer to $y$ as our minimization procedure gains degrees of freedom to find the &amp;ldquo;best fit.&amp;rdquo; Basically this means that $R^2$ does not penalize overfitting. As we add more and more predictors to our model $R^2$ will just keep getting better and better and will fit coefficients that are highly dependent on the random noise. So we need something better.&lt;/p&gt;
&lt;h4 id=&#34;adjusted-r2-&#34;&gt;Adjusted $R^2$&lt;/h4&gt;
&lt;p&gt;The adjusted $R^2$ tries to account for overfitting by decreasing the original $R^2$ statistic. There are two formulas we can use for the adjusted $R^2$ (which we will denote $\overline{R}^2$):
\begin{align*}
\overline{R}^2
&amp;amp; = 1-(1-R^2)\frac{n-1}{n-k-1} \\&lt;br&gt;
&amp;amp; = 1-\frac{\mbox{SSE}/(n-k-1)}{\mbox{SST}/(n-1)}.
\end{align*}
There are two thiings we can note from this. One is we can see how this adjustment works by comparing it to the original formula for $R^2$:
\begin{align*}
R^2 &amp;amp; = \frac{\mbox{SSM}}{\mbox{SST}} \\&lt;br&gt;
&amp;amp; = 1 - \frac{\mbox{SSE}}{\mbox{SST}} \\&lt;br&gt;
&amp;amp; = 1 - \frac{\mbox{SSE}/n}{\mbox{SST}/n}.
\end{align*}
One way of thinking about $\overline{R}^2$ is that the original (un-adjusted) $R^2$ was using biased estimates $\frac{\mbox{SSE}}{n}$ and $\frac{\mbox{SST}}{n}$ which need to be adjusted for their degrees of freedom $n-k-1$ and $n-1$. TODO(what is SST/n an estimate of).&lt;/p&gt;
&lt;h2 id=&#34;residuals-&#34;&gt;Residuals&lt;/h2&gt;
&lt;h3 id=&#34;standardizing-&#34;&gt;Standardizing&lt;/h3&gt;
&lt;p&gt;Frequently when evaluating a model we will plot residuals against other things. Often when we do this we will standardize the residuals by dividing $\hat{\epsilon}_i$ by the residual standard error $\hat{\sigma}$. However, this is not necessarily the best way to achieve our goal of truly standardizing the residuals. The issue is that the &lt;em&gt;residuals&lt;/em&gt; are different from the &lt;em&gt;errors&lt;/em&gt;. Although the errors have equal variance, the residuals actually do not. This can be seen by remembering that: $\hat{\epsilon} = M\epsilon$. Thus, in reality the residuals are correlated and have different variances from one another.&lt;/p&gt;
&lt;h3 id=&#34;studentizing-&#34;&gt;Studentizing&lt;/h3&gt;
&lt;p&gt;Luckily we can account for this. The way we do this is by &amp;ldquo;studentizing&amp;rdquo; the residuals. There are several formulas we can use to represent this, but basically it boils down to the fact that $\hat{\epsilon} \sim N(0,\sigma^2M)$. Thus the true variance of $\hat{\epsilon}_i$ is not $\sigma^2$ but $\sigma^2M_{ii}$. Thus we can studentize by calculating
$$t_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}(\epsilon)\sqrt{M_{ii}}}.$$
We can equivalently express this in terms of the 
&lt;a href=&#34;#leverage&#34;&gt;leverage&lt;/a&gt;
 as
$$t_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}(\epsilon)\sqrt{1-h_{ii}}}.$$
TODO: some more on this&lt;/p&gt;
&lt;h3 id=&#34;deleting-&#34;&gt;Deleting&lt;/h3&gt;
&lt;p&gt;Since outliers may have outsized influence on the model fit (see the 
&lt;a href=&#34;#measures-of-influence&#34;&gt;next section&lt;/a&gt;
 we may want a more robust statistic for estimating the residuals. We can generate this by considering the &lt;em&gt;deleted residuals&lt;/em&gt;, or the residuals when comparing a data point to the model fitted &lt;em&gt;without that data point&lt;/em&gt;. We denote the vector of these as residuals $\hat{y}_{(i)}$ and a particular residual as $\hat{y}_{j(i)}$. The deleted residuals are expressed as:
$$d_i = y_i - \hat{y}_{i(i)}.$$
How can we express these mathematically?&lt;/p&gt;
&lt;p&gt;We will start by considering the whole vector $\hat{y}_{(i)}$. This is the vector of all $n$ predictions, generated using all the data &lt;em&gt;except&lt;/em&gt; the $i$-th point. We can re-express $\hat{y}_{(i)}$ in terms of matrices, although it takes a little thought. Recall that $\hat{y} = Hy = X(X&amp;rsquo;X)X&amp;rsquo;y = X\hat{\beta}$. To calculate $\hat{y}_{(i)}$ we essentially need to first calculate $\hat{\beta}_{(i)}$, the coefficient estimates without data point $i$. To do this we remove one entry from $y$ and therefore also one row from $X$. However the &lt;em&gt;dimension&lt;/em&gt; of $\hat{\beta}$ does not change. We can actually write this as:
$$\hat{\beta}_{(i)} = (X&amp;rsquo;X - x_i&amp;rsquo;x_i)^{-1}(X&amp;rsquo;y - x_i&amp;rsquo;y_i).$$
Then to calculate the predictions $\hat{y}_{(i)}$ we still just multiply by $X$, since we wish to predict all data points, including $y_i$. Thus:
$$\hat{y}_{(i)} = X(X&amp;rsquo;X - x_i&amp;rsquo;x_i)^{-1}(X&amp;rsquo;y - x_i&amp;rsquo;y_i).$$
Now we can do a little more algebra to make this nicer, using the very nice 
&lt;a href=&#34;/courses/gaussian-processes/linear-algebra&#34;&gt;Sherman-Morrison formula&lt;/a&gt;
 to calculate
$$(X&amp;rsquo;X - x_i&amp;rsquo;x_i)^{-1} = (X&amp;rsquo;X)^{-1} + \frac{(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;x_i(X&amp;rsquo;X)^{-1}}{1 - x_i(X&amp;rsquo;X)^{-1}x_i}.$$
The denominator here involves $x_i(X&amp;rsquo;X)^{-1}x_i$. This is just the leverage $h_{ii}$. We will denote the $n\times 1$ vector representing a column of $H$ as $h_i$. Using this notation, our adjusted predictions become:
\begin{align*}
\hat{y}_{(i)}
&amp;amp; = X\left((X&amp;rsquo;X)^{-1} + \frac{(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;x_i(X&amp;rsquo;X)^{-1}}{1 - h_{ii}}\right)(X&amp;rsquo;y - x_i&amp;rsquo;y_i) \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo;y - X(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;y_i + \frac{1}{1-h_{ii}}X(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;x_i(X&amp;rsquo;X)^{-1}X&amp;rsquo;y - \frac{1}{1-h_{ii}}X(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;x_i(X&amp;rsquo;X)^{-1}x_i&amp;rsquo;y_i \\&lt;br&gt;
&amp;amp; = Hy - h_iy_i + \frac{1}{1-h_{ii}}h_ih_i&amp;rsquo;y - \frac{1}{1-h_{ii}}h_ih_{ii}y_i \\&lt;br&gt;
&amp;amp; = \hat{y} - \left(1 + \frac{h_{ii}}{1-h_{ii}}\right)h_iy_i + \frac{h_ih_i&amp;rsquo;y}{1-h_{ii}} \\&lt;br&gt;
&amp;amp; = \hat{y} - \frac{h_i(y_i-h_i&amp;rsquo;y)}{1-h_{ii}} \\&lt;br&gt;
&amp;amp; = \hat{y} - \frac{y_i-\hat{y}_i}{1-h_{ii}}h_i \\&lt;br&gt;
&amp;amp; = \hat{y} - \frac{\hat{\epsilon}_i}{1-h_{ii}}h_i
\end{align*}
So the $i$th entry of the vector, $\hat{y}_{i(i)} = \hat{y}_i - \frac{\hat{\epsilon}_ih_{ii}}{1-h_{ii}}$. So the deleted residuals become
\begin{align*}
d_i &amp;amp; = y_i - \hat{y}_{i(i)} \\&lt;br&gt;
&amp;amp; = y_i - \hat{y}_i - \frac{\hat{\epsilon}_ih_{ii}}{1-h_{ii}} \&lt;br&gt;
&amp;amp; = \hat{\epsilon_i}\left(1 - \frac{h_{ii}}{1-h_{ii}}\right) \&lt;br&gt;
&amp;amp; = \frac{1-2h_{ii}}{1-h_{ii}}\hat{\epsilon}_i
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;measures-of-influence-&#34;&gt;Measures of Influence&lt;/h2&gt;
&lt;p&gt;One other way of evaluating a regression fit is by considering the influence of each point. An unfortunate side effect of the least squares criterion is that it punishes point that are &lt;em&gt;very far&lt;/em&gt; away from the fit more than it punishes points that are &lt;em&gt;kinda far&lt;/em&gt; away from the fit. Basically this means that outliers are very influential. We can assess precisely how influential in a variety of ways.&lt;/p&gt;
&lt;h3 id=&#34;leverage-&#34;&gt;Leverage&lt;/h3&gt;
&lt;p&gt;One of the most popular ways of assessing this infuence is with the &lt;em&gt;leverage&lt;/em&gt;. The leverage is defined as a measure of &amp;ldquo;observation self-sensitivity.&amp;rdquo; It is defined as the partial derivative: $h_{ii} = \dfrac{\partial \hat{y}_i}{\partial y_i}$. That is, how quickly does the predicted value change as the data point changes. This can be easily calculated from our equation for the predicted values: $\hat{y} = Hy$ so that we can see that $h_{ii}$ is in fact just the $ii$-th entry of the hat matrix.&lt;/p&gt;
&lt;p&gt;Moreover we can show (TODO) that the leverage is bounded: $0\leq h_{ii}\leq 1$. So the predicted value will always increase when the datapoint increases, but will never increase quite as fast. That fits our intuition that predictions should be consistent with the data, but that no one data point should completely determine the fit.&lt;/p&gt;
&lt;p&gt;Because least-squares is so sensitive to outliers, observations with leverages close to 1, or much larger than all the other leverages, should be suspect and might be considered outliers.&lt;/p&gt;
&lt;h3 id=&#34;cooks-distance-&#34;&gt;Cook&amp;rsquo;s Distance&lt;/h3&gt;
&lt;p&gt;Another way of assessing influence is with the Cook&amp;rsquo;s distance. This estimates the effect of deleting a particular observation. It is defined to be $D_i$: the squared distance between the predictions $\hat{y}$ and the predictions $\hat{y}_{(i)}$ when observation $i$ is removed from the sample, divided by $k$ times the mean squared error. In math: $$D_i = \frac{\sum_{j= 1}^n (\hat{y}_j - \hat{y}_{j(i)})^2}{k\hat{\sigma^2}}.$$&lt;/p&gt;
&lt;p&gt;Therefore the Cook&amp;rsquo;s distance can also be expressed as
\begin{align*}
D_i &amp;amp; = \frac{\sum_{j= 1}^n (\hat{y}_j - \hat{y}_{j(i)})^2}{k\hat{\sigma^2}} \\&lt;br&gt;
&amp;amp; = \frac{\left(\frac{\hat{\epsilon}_i}{1-h_{ii}}\right)^2h_i&amp;rsquo;h_i}{k\hat{\sigma}^2} \\&lt;br&gt;
&amp;amp; = \frac{\hat{\epsilon}_i}{k\hat{\sigma}^2}\frac{h_{ii}}{(1-h_{ii})^2}
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;dffits-&#34;&gt;DFFITS&lt;/h3&gt;
&lt;p&gt;A third measure of influence is DFFITS (which may stand for Difference of Fits). It is sort of like a cross between leverage and Cook&amp;rsquo;s distance. Like leverage, it is a measure of self-sensitivity, but like Cook&amp;rsquo;s distance it uses the predictions when a point is left out. The actual mathematical definition is:
$$\mbox{DFFITS}_i = \frac{\hat{y}_i - \hat{y}_{i(i)}}{s_{(i)}\sqrt{h_{ii}}},$$
where $\hat{y}_{i(i)}$ is the predicted value for $y_i$ when $y_i$ is removed from the dataset and $s_{(i)}$ is the standard error estimated without $y_i$.&lt;/p&gt;
&lt;p&gt;Although the formulas for DFFITS and Cook&amp;rsquo;s distance are different, it turns out to be possible to convert from one to another. We will not show this here (TODO) but the general relationship is
$$D_i = \frac{\mbox{DFFITS}^2_i\hat{\sigma^2}_{(i)}}{k\hat{\sigma^2}}.$$
In particular except for very small datasets the mean squared error should not change too much when data point $i$ is remove, so the approximate relationship
$$D_i \approx \frac{\mbox{DFFITS}_i^2}{k}$$
holds.&lt;/p&gt;
&lt;h2 id=&#34;normality-&#34;&gt;Normality&lt;/h2&gt;
&lt;h3 id=&#34;moments-&#34;&gt;Moments&lt;/h3&gt;
&lt;p&gt;A common way of quantifying a statistical distribution is with its &lt;em&gt;moments&lt;/em&gt;. These are defined to be the expectations $E[X], E[X^2], E[X^3], $ and so on ($E[X^i]$ in general). The central moments are defined to be $E[(X-\mu)^i]$ and the standardized moments are $E\left[\left(\frac{X-\mu}{\sigma}\right)^i\right]$. The normal distribution is entirely defined by its first and second moments. In particular its third standardized moment (its &lt;em&gt;skewness&lt;/em&gt;) is 0 and its fourth standardized moment (its &lt;em&gt;kurtosis&lt;/em&gt;) is 3. By calculating the sample skewness and sample kurtosis we can heuristically evaluate whether the residuals from our model seem to follow a normal distribution.&lt;/p&gt;
&lt;h3 id=&#34;shapiro-wilk-&#34;&gt;Shapiro-Wilk&lt;/h3&gt;
&lt;p&gt;We can apply a more formal test of normality using the Shapiro-Wilk test. This test statistic is defined by considering the &lt;em&gt;order statistics&lt;/em&gt; $x_{(i)}$ from a sample and calculating:
$$W = \frac{\left(\sum_{i=1}^n a_ix_{(i)}\right)^2}{\sum_{i=1}^n (x_i-\overline{x})^2}.$$
The coefficients $a_i$ are obtained as the vector $\frac{V^{-1}m}{C}$ where $m$ is the expected values of the order statistics for iid standard normal random variables, and $V$ is the covariance matrix for those order statistics. $C$ is the length of the vector $V^{-1}m$. There is no classical distribution that fits $W$, so testing is usually done using simulation methods.&lt;/p&gt;
&lt;h2 id=&#34;heteroskedasticity-&#34;&gt;Heteroskedasticity&lt;/h2&gt;
&lt;p&gt;The most common way of testing for heteroskedaticity is the Breusch-Pagan test (implemented in R as ncv.test). This tests whether the variance of the residuals is dependent on the values of the predictors. This tests works by conducting regression as ordinary and calculating the residuals $\hat{\epsilon}$. Then we consider the possibility that the square of the residuals $\hat{\epsilon}^2$ depends upon the predictors. We test this by fitting a new linear model: $$\hat{\epsilon}^2 = X\gamma + \eta.$$
The test statistic is $nR^2$ where $R^2$ is the coefficient of determination for the second model. Under the null hypothesis that the predictors have no effect. this test statistic is distributed $\chi^2_{k-1}$. (TODO: why?)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Model Diagnostics</title>
      <link>/courses/qsci483/linear-regression/diagnostics/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/diagnostics/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to derive and analyze the model diagnostics for a simple linear regression model. Make sure to check out my previous posts on 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 and 
&lt;a href=&#34;/courses/qsci483/linear-regression/linear-regression&#34;&gt;linear regression&lt;/a&gt;
 before diving in.&lt;/p&gt;
&lt;p&gt;Plotting fitted vs residuals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;should see no trend/pattern&lt;/li&gt;
&lt;li&gt;centered on zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plot histogram of residuals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;basically centered on zero?&lt;/li&gt;
&lt;li&gt;skewed/kurtosis?&lt;/li&gt;
&lt;li&gt;Shapiro-Wilks test (can be very sensitive to small deviations with large datasets)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QQ plot&lt;/p&gt;
&lt;p&gt;DFFITS
Deleted residuals
Cook&amp;rsquo;s distance and leverage&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/27</title>
      <link>/post/notebook/2020/spring/week6/mon/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week6/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-9:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up for the week&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 9:30-10:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Respond to emails&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning 10:30-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrate pilot study and optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAing&lt;/li&gt;
&lt;li&gt;Work on TAing notes when possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find Neotoma dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 6</title>
      <link>/post/notebook/2020/spring/week6/post-1/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week6/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Select a pollen dataset to use&lt;/li&gt;
&lt;li&gt;Find a trade winds dataset to use&lt;/li&gt;
&lt;li&gt;Run the end-to-end pilot study/optimization procedure (and compare to spatially balanced)&lt;/li&gt;
&lt;li&gt;Complete VL Ch 7-10&lt;/li&gt;
&lt;li&gt;Look ahead to QSCI HW 3&lt;/li&gt;
&lt;li&gt;Write up the SSN scratch paper before I forget it all&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/24</title>
      <link>/post/notebook/2020/spring/week5/fri/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debug sampling design code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 10-11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look into paleopollen and email Mevin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 11-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write on SSN paper&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read and take notes on VL Ch 7/8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 2-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write on Mevin paper&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debug sampling design code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 4-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read fun papers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Lots of scattered goals today so not all of them actually happened. I did look into the paleopollen stuff but there&amp;rsquo;s a lot there so still need to sift through what may be most applicable. I took some notes on Ch 7 but not Ch 8, and I did read some fun papers. Spent much more than 1 hour debugging the sampling design code but I did finally get it working!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/23</title>
      <link>/post/notebook/2020/spring/week5/thurs/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finish grading and respond to TAing emails&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 11-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debug covariance parameter estimation for optimal sampling design&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computer lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-4:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Meet with Tyler&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Meetings and lab went well! Implemented profile likelihood method for parameter estimation in the sampling design case study but it still doesn&amp;rsquo;t seem to be working. Not sure what could be going on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/22</title>
      <link>/post/notebook/2020/spring/week5/wedn/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Just grade all day!&lt;/p&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;I did that! I got almost everything finished just one or two more assignments to go.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/21</title>
      <link>/post/notebook/2020/spring/week5/tues/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9:30-11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Vecchia-Laplace paper&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 11-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Space-time reading group&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look for applications of Vecchia-Laplace paper&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend lecture, grading&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;I did these things! I think the Vecchia-Laplace paper could potentially be useful for the SSN stuff if it turns out that calculating the covariance matrix isn&amp;rsquo;t so so hard, or it could at least be useful for the existing SSN framework. Only got a few assignments done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/20</title>
      <link>/post/notebook/2020/spring/week5/mon/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-9:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lab checkins and set up for the week&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 9:30-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grade&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAing&lt;/li&gt;
&lt;li&gt;Work on TAing notes when possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on optimal design synthetic experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 5</title>
      <link>/post/notebook/2020/spring/week5/post-1/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week5/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Email Mevin about animal movement and pollen&lt;/li&gt;
&lt;li&gt;Keep working on write-up and looking for datasets&lt;/li&gt;
&lt;li&gt;Select and simulate a pilot study procedure for optimal sampling design&lt;/li&gt;
&lt;li&gt;Read and take notes on VL Ch 7 and 8&lt;/li&gt;
&lt;li&gt;Grade for QSCI 483&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did email Mevin and got pointed to a really cool paleo dataset to use for that. Selected a cluster sampling pilot study procedure for the optimal sampling design project. Read and took (some) notes for VL Ch 7. Did a lot of gradnig for QSCI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/17</title>
      <link>/post/notebook/2020/spring/week4/fri/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Respond to emails&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 10-11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lab meeting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Morning: 11-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write up animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read and take notes on VL Ch 5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 2-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look for wind speed data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on sampling design case study&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 4-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on SSN modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did all these things although I did not finish writing up the animal movement case study &amp;ndash; there&amp;rsquo;s a lot more background to include. Read and took notes on both VL Ch 5 and 6. Turns out the pollen dataset that I thought was from AAAAI/NAB was actually not. It is more paleoecological than really pollen counts that we&amp;rsquo;re looking for. Only real pollen counts I&amp;rsquo;ve been able to find so far are AAAAI and EAN (Europe) both of which are closely guarded and cannot be made public. I&amp;rsquo;d really like to use a public dataset so I&amp;rsquo;m not sure what to do about this&amp;hellip;however I did find a wind speed dataset that seems to be pretty decent although right now it only seems to be marine measurements so I&amp;rsquo;m trying to see if there&amp;rsquo;s a land-based sister dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The recitation method and the nature of classroom learning</title>
      <link>/post/teaching/visible-learning/ch-06/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-06/</guid>
      <description>&lt;p&gt;The recitation method, also known as the IRE cycle (initiation-response-evaluation) or the CDR method (conventional-direct-recitation) refers to a particular pattern used commonly by teachers. This pattern has stayed in place despite considerable criticism for hundreds of years. The cycle progresses from a teacher initiating an interaction, inviting a response (e.g. asking a question), and evaluating the response before beginning the cycle anew.&lt;/p&gt;
&lt;p&gt;This system has been criticized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Questions asked are mainly low level, calling only for simplistic answers&lt;/li&gt;
&lt;li&gt;Only one student is active at a time&lt;/li&gt;
&lt;li&gt;Education is reduced to receiving pre-packaged knowledge and demonstrating its retention&lt;/li&gt;
&lt;li&gt;Classroom conversation is inherently predictable, task-oriented, but unstimulating. Learning becomes sterile, non-emotional, and rule-bound.
This system is ubiquitous around the world. However, about 75% of classtime in an average classroom is spent on the instruction stage. Less than 1% of time was spent on open questions that might ask for complex responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The recitation method has some advantages associated with overcrowded classrooms and the need to teach a rigid set curriculum. It is appealing to teachers because they remain in control of the interaction at virtual all stages and may speed up or slow down to any pace. However students learn little from just hearing teachers talk.&lt;/p&gt;
&lt;p&gt;One of the major principles of learning is that the learner needs to be actively responding to get anything out of it. They do not necessarily need to respond overtly, they just need to be actively engaged. This is difficult to maintain for the long periods of time that teachers generally speak for.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;redundancy effect&lt;/em&gt; identified by cognitive load researchers states that when teachers talk for long periods of time, students are unable to determine which information is relevant and which is not. Effective teachers will explain material extremely well, but very briefly. Students will not learn simply by listening for longer periods of time. Mental focus drops up after perhaps 10 minutes.&lt;/p&gt;
&lt;p&gt;Two theories underly this drop in mental focus, or &lt;em&gt;mind wandering&lt;/em&gt;. One theory, known as &lt;em&gt;ego depletion&lt;/em&gt; is that one&amp;rsquo;s ability to focus intensely actually literally runs out through exhaustion, measured by brain glucose. So, your mind wanders in order to build up energy for the next upcoming demand that will be placed upon it. The second theory is known as &lt;em&gt;cascading inattention&lt;/em&gt;. This refers to when the mind is unable to clearly process and organize incoming information and fit it into a simple framework and structure.&lt;/p&gt;
&lt;p&gt;An alternative is formulated as the PDC (progressive-discovery-constructivist) approach although the chapter did not explain what this approach entails. Another alternative is tha Paideia model which states that learning takes place in three parts: didactic instruction, Socratic questioning, and coached product. Each of these hsould take up a third of time. However, Socratic questioning is the key and entails more than just students talking. Questions must promote higher order thinking; student talk is a means not an end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/16</title>
      <link>/post/notebook/2020/spring/week4/thurs/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on animal movement and synthetic data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computer lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-4:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Meet with Tyler&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Found the bug in my animal movement code!!! Finally I can move on to other parts of this case study. Ran MCMC and it is giving me good and reasonable answers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Time as a global indicator of classroom learning</title>
      <link>/post/teaching/visible-learning/ch-05/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-05/</guid>
      <description>&lt;p&gt;Your pedagogical philosophy has no bearing on student outcomes unless you spend time on the issues you claim to emphasize.&lt;/p&gt;
&lt;p&gt;Studies used four different measures of time (see VL pg. 37) each a subset of the last&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allocated time
&lt;ul&gt;
&lt;li&gt;time as programmed proactively, on documents and curriculum plans&lt;/li&gt;
&lt;li&gt;problems: interruptions, visitors, announcements, transitions&lt;/li&gt;
&lt;li&gt;improved by: school mandated policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instructional time
&lt;ul&gt;
&lt;li&gt;actual time genuinely available for instruction&lt;/li&gt;
&lt;li&gt;problems: poor classroom management, allowing time to be hijacked by low-priority issues&lt;/li&gt;
&lt;li&gt;improved by: managerial skill and time prioritization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Engaged time
&lt;ul&gt;
&lt;li&gt;time student actually pays attention&lt;/li&gt;
&lt;li&gt;problems: students not knowing what to focus on, distractions, boredom, fatigue&lt;/li&gt;
&lt;li&gt;improved by: clear instructions and meaningful tasks, encouragement, feedback&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Academic learning time
&lt;ul&gt;
&lt;li&gt;time when student is learning and responding successfully&lt;/li&gt;
&lt;li&gt;problems: student may be unsuccessful despite effort, gaps in prior knowledge, tasks too challenging&lt;/li&gt;
&lt;li&gt;improved by: individualized guidance, encouragement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Amount of time dedicated to a particular task varies wildly between different teachers. However the most critical measure of time is academic learning time. Higher tiers can be used to help maximize academic learning time but this is the level that is most consistently correlated with student success. However, just spending time with a topic does not necessarily imply improvement. Rather deliberate practice with a focus on improvement is necessary, with critical components including guidance, instruction, goal setting, and feedback.&lt;/p&gt;
&lt;p&gt;Time is particular important for forming deep understanding and connections between topics. Comparing curriculums streamlined to cover the same material in shorter or longer periods of time, there was no difference in test scores on surface-level multiple choice questions. However, free response questions meant to test deep understanding and connections between topics showed wild variation with much higher scores for the students that took the longer courses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/15</title>
      <link>/post/notebook/2020/spring/week4/wedn/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Respond to Sándor and find spatial dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend class&lt;/li&gt;
&lt;li&gt;Take notes on VL Ch 4&lt;/li&gt;
&lt;li&gt;Work on SNN modeling math&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did most of these things. Started working on the synthetic data case study for the optimization project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/14</title>
      <link>/post/notebook/2020/spring/week4/tues/</link>
      <pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9:30-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continue work on animal movement&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NSF Activity Report&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Finished the activity report, held office hours, worked on movement. Still stumped by my movement bugs. I&amp;rsquo;m getting not-PSD errors from a function that should not possibly be able to produce a non-PSD matrix.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/13</title>
      <link>/post/notebook/2020/spring/week4/mon/</link>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-9:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lab checkins and set up for the week&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 9:30-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debug animal movement code&lt;/li&gt;
&lt;li&gt;While animal movement code is running, work on VL, then SNN math&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAing&lt;/li&gt;
&lt;li&gt;Work on TAing notes when possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on NSF progress report&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Worked on animal movement code. It seems as though my bug is the &amp;ldquo;wiggles&amp;rdquo; common to FEM advection-type equations so I am attempting to implement FD. It wasn&amp;rsquo;t too hard for this simple example and is slightly better practice anyways. Spent pretty much all day on that however.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/10</title>
      <link>/post/notebook/2020/spring/week3/fri/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make notes on VL Ch 3 and 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 2-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Email Mevin and Sándor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on pollen case study&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Mostly worked on the case study. Made some notes on VL Ch 3 but the movement stuff was very buggy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The teacher-student relationship</title>
      <link>/post/teaching/visible-learning/ch-03/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-03/</guid>
      <description>&lt;p&gt;An &lt;em&gt;empathy gap&lt;/em&gt; occurs when people are relatively unable to put themselves in the place of another person. Deep levels of empathy are difficult if you have never had the same experience as another person, or if you are not currently experiencing anything similar. It is easiest to empathize with people whose experiences you are currently mirroring, harder if you have mirrored them in the past, and hardest if you have never experienced them at all. Children in particular do not develop the ability to deeply understand others&amp;rsquo; viewpoints until late adolescence or early adulthood.&lt;/p&gt;
&lt;p&gt;Inability to empathize with each other results in &lt;em&gt;negative escalations&lt;/em&gt;. When one person wrongs or hurts another, the other person retaliates. The first underestimates the harm they incurred and so views the retaliation as out of proportion, which justifies further retaliation, leading to a &lt;em&gt;snowball effect&lt;/em&gt; or &lt;em&gt;negative cascade&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand establishing positive relationships cascades and results in lasting benefits. Marked improvements have been observed to occur the year &lt;em&gt;following&lt;/em&gt; relationship-based interventions. Longitudinal studies have found persistent and profound benefits to positive teacher student relationships on a decadal scale. These positive relationships may help because they build trust that allows students to make mistakes and ask for help, and builds the confidence in students to try again.&lt;/p&gt;
&lt;p&gt;Developing positive relationships can be difficult if the focus is on responding to uncooperative students. Social psychology shows that &lt;em&gt;emotional leakage&lt;/em&gt; is common. Hiding emotions is difficult, and when a teacher has negative emotions towards a student the student may realize this. No major theory of learning recommends punitive action in response to difficult students. For example, high school students tend to attribute disciplinary action to the teacher (&amp;ldquo;teacher hats me&amp;rdquo;) rather than accept personal responsibility.&lt;/p&gt;
&lt;p&gt;When a child has an unsupportive home environment, school can become a major source of social and cultural learning. Positive teacher-student relationships can be very influential in personal development in this stage and can mitigate the risk of negastive outcomes for children. On the other hand, positive at-home relationships can buffer negative relationships at school.&lt;/p&gt;
&lt;p&gt;Can these positive relationships be manufactured? A study found that just spending a few minute devoted to non-directive non-coercive friendly and child-centered activities positively influences teacher&amp;rsquo;s views for at-risk students.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your personality as a teacher</title>
      <link>/post/teaching/visible-learning/ch-04/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-04/</guid>
      <description>&lt;p&gt;Nearly a century of research has shown that there is &lt;em&gt;NOT&lt;/em&gt; a &amp;ldquo;best&amp;rdquo; personality for a teacher to have. Students value being treated with fairness, dignity, and respect much more than they value a particular personality type.&lt;/p&gt;
&lt;p&gt;It has been found that children lose respect for adults who violate basic social rules, exhibit cruelty, or otherwise break social conventions. Studies show that very young children possess a strong sense of what is right and wrong and have a deep sense of fairness. They may not behave in this manner, since social judgment and risk assessment develop much later. Almost universally though, basic misdeeds such as being caught lying incur significant degradation of trust and positive relationships.&lt;/p&gt;
&lt;p&gt;Studies show that students evaluate teachers after exposure for as little as 10 seconds. This is an example of the &lt;em&gt;blink effect&lt;/em&gt;. Moreover, these blink assessments are at least somewhat accurate, being correlated with ratings from actual students (although possible this could be an example of the blink effect itself being cemented in place in those genuine students). Deliberate use of warm and inviting body language, direct eye contact, and friendly intonation increased scores on academic tests for 7-year old students.&lt;/p&gt;
&lt;p&gt;It is very difficult to detect when children tell lies. Children develop skillful deceit at a very young age and from that point accuracy for telling lies is about 60% (barely above random chance). Rather, we are very good at detecting emotions, but emotions are poor indicators of deceit since truth-tellers often show signs of anxiety when placed under suspicion. Meanwhile liars may prepare for being grilled. Takeaway: do not expect to be able to tell if a student is lying from social cues.&lt;/p&gt;
&lt;p&gt;Seeking help in the classroom is a good thing, and should not be conflated with dependency (relying on a single source excessively and consistently). Help-seeking is associated with &amp;ldquo;mastery goal orientation&amp;rdquo; where students are motivated by factors involving understanding and acquiring knowledge. This contrasts with &amp;ldquo;ego orientation&amp;rdquo; or &amp;ldquo;performance orientation&amp;rdquo; where the goal is to look good or outperform one&amp;rsquo;s immediate peers. Both orientatios coexist, but mastery orientation is associated with higher performance and deeper understanding.&lt;/p&gt;
&lt;p&gt;Students are more likely to seek help if they trust their teacher, and if their teacher encourages help-seeking behavour. As students age they begin to associate question-asking with low ability. However, with supportive teachers, students associate question asking more positively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/9</title>
      <link>/post/notebook/2020/spring/week3/thurs/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make edits to animal movement modeling section&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computer lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-4:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Meet with Tyler&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did these things! Was hoping to finish the animal movement section but it didn&amp;rsquo;t quite happen. Gotta make some decisions about presenting IOU vs OUF results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/8</title>
      <link>/post/notebook/2020/spring/week3/wedn/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write personal statement and finish all my portion of the concurrent MS application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend class and read VL Ch 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Mevin&amp;rsquo;s sources&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Finished application by 11:30. Got Dirichlet BC code up and running and am not seeing any obvious bugs right away. Looking good! Skimmed VL Ch 3 and Ch 4, will take notes tomorrow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/7</title>
      <link>/post/notebook/2020/spring/week3/tues/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Visible Learning Chapter 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hold office hours&lt;/li&gt;
&lt;li&gt;Take notes on VL Ch 3 and read VL Ch 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Email Sándor&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Got side tracked with adminstrative stuff again. Evidently I have to formally apply to the concurrent MS program in the stats department so that will take up much of my week. I did get the Dirichlet code up and running although there are probably a few lingering bugs. MCMC run is going now. Did not get to VL or emailing Sándor, although I actually looked at his email and there is not too much to respond to directly. I do owe him an update soon though.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/6</title>
      <link>/post/notebook/2020/spring/week3/mon/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prep for the week and finish up my preliminary analysis for the spatial stream network model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 11-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;li&gt;Try to run it at least once&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TAing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look at bypass responses&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Morning stuff went well but I fell into administrative tasks and did not get the code done. I did email my committee, email with Erica about my TAing position, and email with the stats department about the concurrent master&amp;rsquo;s program. I took a brief look at Mevin&amp;rsquo;s response in the afternoon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 3</title>
      <link>/post/notebook/2020/spring/week3/post-1/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week3/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Still need to finish up my Dirichlet BC code from last week but I got a lot off my plate! I am done with the bypass and the dam passage paper and can get back to research more-or-less full time (less my new TAing position). My next few goals for spring quarter are all focused on my source project, but I would like to make some more progress on the optimal design front to keep some forward momentum on that front.&lt;/p&gt;
&lt;p&gt;I would like to spend this week doing a mix of: finishing up the Dirichlet code and hopefully the animal movement case study, TAing, and responding to feedback from my bypass proposal. So my goals are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finish up Dirichlet BC code by Tuesday&lt;/li&gt;
&lt;li&gt;Read Visible Learning Chapters 3 and 4 by Wednesday/Friday&lt;/li&gt;
&lt;li&gt;Debug animal movement code and get it run at least once by Thursday&lt;/li&gt;
&lt;li&gt;Respond to Sándor&amp;rsquo;s email by Tuesday&lt;/li&gt;
&lt;li&gt;Read two of Mevin&amp;rsquo;s suggested papers by Wednesday, the rest by Friday&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Finished up the Dirichlet BC code&lt;/li&gt;
&lt;li&gt;Read but have not yet taken notes on VL Chs 3 and 4&lt;/li&gt;
&lt;li&gt;Ran the animal movement code many times, but still debugging&lt;/li&gt;
&lt;li&gt;Still need to get back to Mevin and Sándor with updates.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/3</title>
      <link>/post/notebook/2020/spring/week2/fri/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on QSCI483 stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Made some really good progress on Dirichlet BC stuff, but did not manage to get the code to a working point. I feel like I have a good handle on QSCI for this week, and I made some good progress on stream network analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/2</title>
      <link>/post/notebook/2020/spring/week2/thurs/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look at QSCI stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computer lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Made some good progress on remembering how all the 
&lt;a href=&#34;/courses/qsci-483/linear-algebra/math-diagnostics&#34;&gt;model diagnostics&lt;/a&gt;
 work. Taking this as an opportunity to go beyond what the course really calls for and cement my understanding of these things because they&amp;rsquo;re important to know.&lt;/p&gt;
&lt;p&gt;Lab went well.&lt;/p&gt;
&lt;p&gt;Dirichlet BC stuff is chugging along. I definitely did something wrong last time, I think, because I&amp;rsquo;m running into new challenges. Good that I&amp;rsquo;m reimplementing it. I think I have figured out how to deal with these challenges (i.e. get sparse posterior $Q_u$ and then sample from $f$ as a degenerate MVN conditional on $u$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Knowledge an Obstacle to Teaching?</title>
      <link>/post/teaching/visible-learning/ch-02/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-02/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Experts in a subject matter area seem to have worse intuition for teaching that subject matter, compared to novices
&lt;ul&gt;
&lt;li&gt;In theory this is because they have learned to think about it in a structured and organized way that is not the most effective way to &lt;em&gt;learn&lt;/em&gt; that material&lt;/li&gt;
&lt;li&gt;Experts consistently underestimated how difficult a task is. Novices, having just learned it, know how difficult it is to learn and teach a more basic level&lt;/li&gt;
&lt;li&gt;Although students rated novices higher and performed better on a post-teaching assessment, the expert-taught students performed better on transferring their learning to a related task&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Students appreciate being taught by knowledgeable, motivated, passionate individuals
&lt;ul&gt;
&lt;li&gt;Students will rate their best teachers highly on competency, credibility, and fariness&lt;/li&gt;
&lt;li&gt;More closely related to student motivation than actual learning. However children learn less from adults they view as ignorant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge about subject matter may make it harder to teach in a group setting, but on an individual level enables you to provide helpful feedback, and contextualize a student&amp;rsquo;s ideas and progress against your own knowledge base.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/1</title>
      <link>/post/notebook/2020/spring/week2/wedn/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take notes from Visible Learning Chapter 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend class and continue looking at QSCI HW 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Wrote up some 
&lt;a href=&#34;/courses/gaussian-processes&#34;&gt;resources&lt;/a&gt;
 so that I can stop rederiving/looking up common GP/MVN properties that I need to use. Also included on there the math for Dirichlet BCs. Now that the derivation is there I can more effectively write the code for the implementation, rather than writing stuff down on paper every time I try to do this.&lt;/p&gt;
&lt;p&gt;Forgot to account in my scheduling today for time to edit and submit my dissertation proposal. Did that.&lt;/p&gt;
&lt;p&gt;Wrote up notes for VL Ch 2, and continued to expand on my own 
&lt;a href=&#34;/courses/qsci-483&#34;&gt;notes&lt;/a&gt;
 for QSCI, reminding myself how all the 
&lt;a href=&#34;/courses/qsci-483/linear-regression/diagnostics&#34;&gt;model diagnostics&lt;/a&gt;
 work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/31</title>
      <link>/post/notebook/2020/spring/week2/tues/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look over the rest of QSCI HW 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take notes from Visible Learning Chapter 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hold office hours&lt;/li&gt;
&lt;li&gt;When no one shows up finish up looking at QSCI HW 1&lt;/li&gt;
&lt;li&gt;Then read Visible Learning Chapter 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Didn&amp;rsquo;t make it through the whole thing, had to look up some of the model diagnostics Tim is using, but did get through VL Ch 1 and 2. Struggled with website formatting in the afternoon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-07/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-07/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-08/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-09/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-09/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-10/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-11/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-12/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-13/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-14/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-15/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-16/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-17/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-18/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-19/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-20/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-21/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-22/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-23/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-24/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-24/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-25/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-25/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-26/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-26/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-27/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-27/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-28/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-28/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-29/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-29/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-30/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-30/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-31/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-31/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Broad Strokes of Learning</title>
      <link>/post/teaching/visible-learning/core-takeaways/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/core-takeaways/</guid>
      <description>&lt;h2 id=&#34;nine-basic-principles-&#34;&gt;Nine basic principles&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Explanations of human learning in terms of innate talent are consistently undermined by research which states that substantial amounts of time, energy, instruction, and effort are required to develop mastery of any subject area.&lt;/li&gt;
&lt;li&gt;We can naturally learn from any information, but to learn effectively that information has to be organized in a way that matches how our minds are organized. Minds are organized differently for different people, and even change as we age.&lt;/li&gt;
&lt;li&gt;The brain has severe, inherent limitations and deep processing becomes impossible when those limits are reached (cognitive load principle)&lt;/li&gt;
&lt;li&gt;People learn particularly well from other people, through directed instruction and feedback (social learning theory)&lt;/li&gt;
&lt;li&gt;People put in a lot more effort when they are confident that worthwhile goals are achievable in the short term. Activating effort and motivation is difficult but not impossible.&lt;/li&gt;
&lt;li&gt;Short-term goals are highly motivating, but when they conflict with valuable long-term goals we must develop and use strategies to control impulses and delay gratificatoin (personal regulation through self-control).&lt;/li&gt;
&lt;li&gt;Learners are humans, and other parts of them &amp;ndash; self esteem, sociality, etc &amp;ndash; must be maintained and acknowledged during the learning process.&lt;/li&gt;
&lt;li&gt;Humans are fundamentally social, down to the neurological level. This sociality can be used as a tool (social brain hypothesis)&lt;/li&gt;
&lt;li&gt;Ideas about learning that are contradicted by scientific evidence abound. Many of these ideas can be harmful.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;section-1-&#34;&gt;Section 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The mind may not be &amp;ldquo;designed for thinking&amp;rdquo; and requires time and effort to think&lt;/li&gt;
&lt;li&gt;Teachers need to recognize how difficult tasks are for beginners. Focus should be not on material but on the actual process of moving from not knowing to knowing. To do this students need a safe environment to acknowledge they don&#39;t understand. We can only holld so much in our cognitive centers, so it is important to ``overlearn&amp;rsquo;&amp;rsquo; basic concepts until it is ingrained and automatic&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chapter-1-&#34;&gt;Chapter 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Students do not dislike school, but tolerate it.&lt;/li&gt;
&lt;li&gt;The brain may not be &amp;ldquo;designed&amp;rdquo; for abstract thinking and as a result school is a taxing process.&lt;/li&gt;
&lt;li&gt;Thinking requires a large investment of resources for an uncertain outcome (understanding) and risk-averse humans prefer to invest these resources in crossing seemingly achievable &amp;ldquo;knowledge gaps&amp;rdquo; rather than &amp;ldquo;knowledge chasms&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chapter-2-&#34;&gt;Chapter 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Experts may be &amp;ldquo;worse&amp;rdquo; at teaching than novices. The &amp;ldquo;expert blind spot effect&amp;rdquo; explains this:
&lt;ul&gt;
&lt;li&gt;experts forget how difficult it was to learn something in the first place&lt;/li&gt;
&lt;li&gt;there is a gap between the advanced level on which experts conceptualize of a topic and the short-term learning needs of students&lt;/li&gt;
&lt;li&gt;however, this abstract thinking may help students transfer their learning to related areas.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;section-2-&#34;&gt;Section 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Major ideas about acquisition, memory retention, mental storage, and overload&lt;/li&gt;
&lt;li&gt;Learning need not be conscious, we can only think about so much at once&lt;/li&gt;
&lt;li&gt;We need to develop a vocabulary for learning, and need multiple strategies for learning&lt;/li&gt;
&lt;li&gt;Challenges: learning styles (i.e. spatial, verbal, kinaesthetic), Mozart effects, multitasking, digital natives, and whether the Internet is really changing how we think&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;section-3-&#34;&gt;Section 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Self-esteem follows success more often than predicting it&lt;/li&gt;
&lt;li&gt;Building confidence is still important though: in order to maintain positive views of ourselves we build explanations (I cannot; rather than I did not work hard enough) that help our self esteem but hurt our learning&lt;/li&gt;
&lt;li&gt;Learning situations are often distracting and knowing how to pay attention to learning is important, but tiring. It is important to know when to stop thinking to save cognitive resources.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/30</title>
      <link>/post/notebook/2020/spring/week2/mon/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-11:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do QSCI HW and prep for TAing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 12:30-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Visible Learning Chapter 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did the first half of the QSCI HW, read Ch 1, but did not end up having time to work on the BC code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 2</title>
      <link>/post/notebook/2020/spring/week2/post-1/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;First real week of the quarter and I am surprise TAing! I also found some frustrating uncommitted code from my Colorado trip which I need to reimplement, but rewriting code is often a way to make it better anyways&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make all suggested changes to bypass proposal by Wednesday and send to committee&lt;/li&gt;
&lt;li&gt;Read chapters 1 and 2 of Visible Learning by Wednesday/Friday&lt;/li&gt;
&lt;li&gt;Do the QSCI hw ahead of time to make sure I&amp;rsquo;ve got it down by Tuesday&lt;/li&gt;
&lt;li&gt;Rewrite Dirichlet BC code by Friday&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Spent a lot of time on TAing this week, getting back up to speed on the basics of linear regression and model diagnostics. Mostly for myself, I made up a bunch of notes on the 
&lt;a href=&#34;/courses/qsci-483&#34;&gt;math&lt;/a&gt;
 behind these topics, to cement my own understanding, although the students certainly won&amp;rsquo;t need to know much of that. I made it through the first two chapters of Visible Learning, although I still feel like I need to discuss it with someone to really digest it properly.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t finish off the Dirichlet code but I&amp;rsquo;m close: I got the theory worked out again, and I think I actually had it wrong the first time. I also got my bypass submitted and approved which is awesome and a big relief!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Don&#39;t Students Like School?</title>
      <link>/post/teaching/visible-learning/ch-01/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-01/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There is no serious evidence that students, on average, actively &lt;em&gt;dislike&lt;/em&gt; school. Rather they are neutral, or mildly positive towards school.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A frequent challenge faced by teachers is the apathy of students on an individual level, after working so hard to provide an engaging learning experience.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VL puts forward the following hypothesis, following researcher Daniel Willingham:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The human brain is not primarily designed for &amp;ldquo;thinking&amp;rdquo;: it is designed for sociality, for language learning, for spatial reasoning, etc. Higher level reasoning is not the primary purpose and is, on average, very difficult.&lt;/li&gt;
&lt;li&gt;This means that thinking uses up limited resources very quickly&lt;/li&gt;
&lt;li&gt;Asking someone to invest this level of effort/resources is a hard task. The outcome of a &amp;ldquo;thinking&amp;rdquo; task could be positive (better understanding) or negative (no progress). Humans are risk-averse and are unlikely to invest effort if they cannot see a likely short-term pay-off.&lt;/li&gt;
&lt;li&gt;This means the best way to get effort out of students is to show them short-term goals that seem achievable: &amp;ldquo;we are motivated by knowledge gaps, but put off by knowledge chasms.&amp;rdquo; We cannot be curious about everything, and are instead curious about the things we feel we can understand with little additional effort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ease with which people can access information has a large effect on how they use that information, and how they feel about that information&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We rely much more heavily on memory than thinking: it is generally easier to remember a previous result than to derive it from scratch&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/27</title>
      <link>/post/notebook/2020/spring/week1/fri/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;I might be surprise-TAing next quarter, so I&amp;rsquo;m focusing all my energy on TA prep today. I&amp;rsquo;m going to try to get through the whole UW TA resources page and will be adding notes in 
&lt;a href=&#34;/post/teaching&#34;&gt;posts&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;I did that! I also met with Tim the course instructor to outline my responsibilities as a TA and did some editing on the Dirichlet BC code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Remote Teaching</title>
      <link>/post/teaching/remote-teaching/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/remote-teaching/</guid>
      <description>&lt;p&gt;Online Office Hours:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zoom Pro + Canvas Discussions&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TAing</title>
      <link>/post/teaching/taing/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/taing/</guid>
      <description>&lt;h2 id=&#34;expectations-&#34;&gt;Expectations&lt;/h2&gt;
&lt;p&gt;Understand early on what the professors expectations are, in detail&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Am I expected to attend class?&lt;/li&gt;
&lt;li&gt;Will we hold regular TA meetings?&lt;/li&gt;
&lt;li&gt;Am I grading, writing answer keys, giving guest lectures, holding office hours, holding review sessions, leading discussion sections, responding to online questions, communicating from the instructor to the class, etc?&lt;/li&gt;
&lt;li&gt;Know what the instructor is emphasizing in class&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;demeanor-&#34;&gt;Demeanor&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Assessments</title>
      <link>/post/teaching/assessment/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/assessment/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/assessing-and-improving-teaching/assessing-student-lerning-grading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Four recommendations from UW:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create clear grading criteria&lt;/li&gt;
&lt;li&gt;Communicate these criteria to students&lt;/li&gt;
&lt;li&gt;Give constructive feedback&lt;/li&gt;
&lt;li&gt;Employ time management strategies when grading large amounts of work&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Expect students to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus their time and effort on the things with the highest grading weights&lt;/li&gt;
&lt;li&gt;be sensitive about their grades, and ask about grading-related topics frequently&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Concrete Actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create, communicate, and remind students about clear grading criteria&lt;/li&gt;
&lt;li&gt;create clear expectations about edge cases like late papers, exam timing, grade changes, and typos on assignments&lt;/li&gt;
&lt;li&gt;keep thorough records of evaluations, and keep for a while after the quarter is over&lt;/li&gt;
&lt;li&gt;promptly document interactions with unhappy students&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See further resources on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/post/teaching/writing-tests&#34;&gt;writing tests&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://cft.vanderbilt.edu/guides-sub-pages/grading-student-work&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;assigning grades&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teachingcommons.stanford.edu/resources/teaching/evaluating-students&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;evaluating students&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;grading-&#34;&gt;Grading&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/grading-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grading&lt;/a&gt;
 is really hard. First thing to make clear is that I am using mastery-based (i.e. individual skills) rather than norm-referenced (i.e. relative to each other) grading.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be consistent&lt;/li&gt;
&lt;li&gt;Make sure students know what to expect
&lt;ul&gt;
&lt;li&gt;What is being measured, how is it being measured, what does this have to do with the course at large?&lt;/li&gt;
&lt;li&gt;Do students need to recall information, recognize patterns, draw inferences, make connections, construct an argument, or what?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mastery-based grading requires very clear measurable goals and objectives
&lt;ul&gt;
&lt;li&gt;Likely want to make some re-adjustment of pre-set expectations based on actual performance. Perhaps the test really was too hard?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cheating-&#34;&gt;Cheating&lt;/h2&gt;
&lt;p&gt;How to stop it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explain clearly what is and is not allowed&lt;/li&gt;
&lt;li&gt;Encourage students to seek help:
&lt;ul&gt;
&lt;li&gt;Often students cheat because they are doing poorly&lt;/li&gt;
&lt;li&gt;Encourage them to schedule a meeting and/or come to office hours&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Give alternating versions of exams (different orders if not different questions)&lt;/li&gt;
&lt;li&gt;Regularly walk around the room and observe students&lt;/li&gt;
&lt;li&gt;After the exam mark answer sheets in such a way that alterations cannot be made (possibly scan a copy of each exam if possible)&lt;/li&gt;
&lt;li&gt;Give an alternate exam version for make-ups&lt;/li&gt;
&lt;li&gt;Homework assignments are difficult to monitor
&lt;ul&gt;
&lt;li&gt;UW recommends lowering the amount of the grade HW is worth&amp;hellip;I don&amp;rsquo;t know about that&lt;/li&gt;
&lt;li&gt;Replace homework with in-class quizzes?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Final projects
&lt;ul&gt;
&lt;li&gt;Think about whether students may have done similar projects in other classes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What to do when it happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a specific 
&lt;a href=&#34;https://www.washington.edu/cssc/facultystaff/report-academic-misconduct&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reporting tool&lt;/a&gt;
 but for TAs it probably goes through the instructor first&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Classroom Practices</title>
      <link>/post/teaching/classroom-practices/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/classroom-practices/</guid>
      <description>&lt;p&gt;I want to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn everyone&amp;rsquo;s name by week 1&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First day:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduce myself:
&lt;ul&gt;
&lt;li&gt;Name/pronouns, background, formality, how/when to reach me&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Notecards:
&lt;ul&gt;
&lt;li&gt;Preferred name/pronunciation/pronouns&lt;/li&gt;
&lt;li&gt;Accommodation requests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Expectations:
&lt;ul&gt;
&lt;li&gt;grading/deadline criteria&lt;/li&gt;
&lt;li&gt;classroom policies&lt;/li&gt;
&lt;li&gt;my commitments as a teacher&lt;/li&gt;
&lt;li&gt;how I intend to teach/assess&lt;/li&gt;
&lt;li&gt;add codes/course conflicts&lt;/li&gt;
&lt;li&gt;course schedule&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quick (ungraded) quiz to gauge background? Self-reported pre-assessment of background knowledge? Resources for filling in gaps? ``Common Sense Inventory:&amp;rsquo;&amp;rsquo; determine whether 15 statements related to course content are true or false?&lt;/li&gt;
&lt;li&gt;Homework 0: voluntary/mandatory office hour?&lt;/li&gt;
&lt;li&gt;If there is time left to teach, model what the rest of the course will be like. Think about whether registration has stabilized and whether to teach foundational material, or just interesting stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General Practices&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tie things into what I&amp;rsquo;m excited about. Students can tell if you&amp;rsquo;re into it&lt;/li&gt;
&lt;li&gt;Give real world examples/applications, particularly in my own work&lt;/li&gt;
&lt;li&gt;Keep up a high degree of connectivity, explain how things relate to other things&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.washington.edu/teaching/topics/engaging-students-in-learning/promoting-student-engagement-through-active-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Active learning&lt;/a&gt;
 is good. Give students a chance to practice that thing they just learned&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;office-hours-&#34;&gt;Office Hours&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most important thing is getting people to come in the first place&lt;/li&gt;
&lt;li&gt;Give feedback on what&amp;rsquo;s been done so far and offer direction and general guidance&lt;/li&gt;
&lt;li&gt;Ask them to explain their work and thought process so far
&lt;ul&gt;
&lt;li&gt;If that leaves a blank, ask them to solve an easier problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try to get them to ask specific questions:
&lt;ul&gt;
&lt;li&gt;Is X true?&lt;/li&gt;
&lt;li&gt;Does this specific logic follow?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ask leading questions&lt;/li&gt;
&lt;li&gt;Try to understand what their thought process is, they are likely coming from a different background than me and will think about things in a different way&lt;/li&gt;
&lt;li&gt;Work through the homework and exams ahead of time
&lt;ul&gt;
&lt;li&gt;Try to anticipate sticking points and wrong turns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Know and communicate the instructors expectations and emphasize this material&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Course Design</title>
      <link>/post/teaching/course-design/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/course-design/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://washington.edu/teaching/topics/preparing-to-teach/designing-your-course-and-syllabus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://cft.vanderbilt.edu/guides-sub-pages/understanding-by-design/#resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Backward Design&lt;/a&gt;
:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design a course in reverse by considering
&lt;ul&gt;
&lt;li&gt;What are your learning goals?&lt;/li&gt;
&lt;li&gt;How will you assess those goals?&lt;/li&gt;
&lt;li&gt;How will you achieve those goals through teaching?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In this way everything done in the course is intentional and working towards an established goal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;who-are-my-students-&#34;&gt;Who Are My Students&lt;/h2&gt;
&lt;p&gt;Before I even get to learning goals, also consider: who are my students? Why are they taking my course? What can I expect that they already know? What range of backgrounds might the have? What do I expect them to struggle with?&lt;/p&gt;
&lt;h2 id=&#34;what-are-my-learning-goals-&#34;&gt;What Are My Learning Goals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://teaching.uncc.edu/sites/teaching.uncc.edu/files/media/files/file/GoalsAndObjectives/Bloom.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blooms Taxonomy&lt;/a&gt;
:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A way of classifying increasingly ambitious learning objectives&lt;/li&gt;
&lt;li&gt;Three taxonomies for:
&lt;ul&gt;
&lt;li&gt;Knowledge-based goals&lt;/li&gt;
&lt;li&gt;Skills-based foals&lt;/li&gt;
&lt;li&gt;Affective goals (related to values, attitudes, interests)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-will-i-assess-those-goals&#34;&gt;How Will I Assess Those Goals&lt;/h2&gt;
&lt;p&gt;Both during the course and at the end of the course. Designing 
&lt;a href=&#34;/post/teaching/assessment&#34;&gt;assessments&lt;/a&gt;
 is a whole separate can of worms.&lt;/p&gt;
&lt;h2 id=&#34;how-will-i-achieve-those-goals-&#34;&gt;How Will I Achieve Those Goals&lt;/h2&gt;
&lt;p&gt;Before I get into the nitty-gritty of lesson planning, etc, now is also a good time to write a 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/designing-your-course-and-syllabus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syllabus&lt;/a&gt;
. Once I&amp;rsquo;ve done that I think I personally will benefit from calendaring. This will help me remember little things like: two weeks from today is the exam, I need to write it so I can tell students about format, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/26</title>
      <link>/post/notebook/2020/spring/week1/thurs/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fix Anaconda errors and start animal movement MCMC running overnight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start reading UW TA resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Think more about second-order inclusion probabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Fixed Anaconda error, then discovered issue where some code was not pushed to GitHub. Unable to find the original code (mostly my implementation of Dirichlet BCs), but re-writing it should be a useful (if annyoing) exercise.&lt;/p&gt;
&lt;p&gt;Started reading UW resources and made several 
&lt;a href=&#34;/post/teaching&#34;&gt;posts&lt;/a&gt;
. Not all of it is relevant just yet, but I&amp;rsquo;d like to get through all of it and start thinking about everything together.&lt;/p&gt;
&lt;p&gt;Found 
&lt;a href=&#34;https://www.jstor.org/stable/23357227&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;
 about random sampling with given second-order inclusion probabilities. It gives results about which inclusion probability matrices (IPMs) are valid (the answer is complicated) and constructed a maximum-entropy distribution for fixed-size sampling with fixed IPM.&lt;/p&gt;
&lt;p&gt;Thoughts on IPMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Want to incorporate logistics somehow&lt;/li&gt;
&lt;li&gt;Can we estimate IPMs after filtering for feasible designs?&lt;/li&gt;
&lt;li&gt;Can we calculate IPMs after filtering for feasible designs?&lt;/li&gt;
&lt;li&gt;Can we calculate probability of infeasibility for CP(2) designs?&lt;/li&gt;
&lt;li&gt;How do CP(2) designs work in space-time?&lt;/li&gt;
&lt;li&gt;I feel like there&amp;rsquo;s something in here somewhere but it&amp;rsquo;s buried deep&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tests</title>
      <link>/post/teaching/writing-tests/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/writing-tests/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/constructing-tests&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why am I testing:
&lt;ul&gt;
&lt;li&gt;to monitor progress and adjust the pace of the course?&lt;/li&gt;
&lt;li&gt;to motivate students&lt;/li&gt;
&lt;li&gt;to provide data for a grade?&lt;/li&gt;
&lt;li&gt;to challenge students to apply concepts?&lt;/li&gt;
&lt;li&gt;Use this information to design the exam&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is my test consistent with my teaching:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;post/teaching/course-design&#34;&gt;Backwards design&lt;/a&gt;
 will help with this&lt;/li&gt;
&lt;li&gt;If the test emphasizes analysis and synthesis, make sure class time does as well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do the students know what to expect?
&lt;ul&gt;
&lt;li&gt;Does the test match my stated course goals&lt;/li&gt;
&lt;li&gt;Have I reviewed the material on the test/explained what will be tested?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does my exam test a range of learning?
&lt;ul&gt;
&lt;li&gt;Do students who have not mastered everything have room to demonstrate growth?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiple choice exams:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to grade&lt;/li&gt;
&lt;li&gt;Good for testing recall and facts, difficult to test analysis&lt;/li&gt;
&lt;li&gt;Make sure question is clear without reading the answers
&lt;ul&gt;
&lt;li&gt;Can a prepared student or colleague answer the question as a free response?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Avoid making the right answer stand out for the wrong reasons (i.e. grammer, length)&lt;/li&gt;
&lt;li&gt;Assess the exam questions afterwards:
&lt;ul&gt;
&lt;li&gt;Which questions were most difficult?&lt;/li&gt;
&lt;li&gt;Were there questions which most students with high-grades missed?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardless of exam format, assessing it after the fact is important:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Did I test what I thought I was testing?&lt;/li&gt;
&lt;li&gt;Did I test what I tught?&lt;/li&gt;
&lt;li&gt;Did I test what I emphasized?&lt;/li&gt;
&lt;li&gt;Did I test what I really wanted the students to learn?&lt;/li&gt;
&lt;li&gt;If answers to these questions come back no, possibly go back to the Backwards Design and rethink course/assessment structure.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/25</title>
      <link>/post/notebook/2020/spring/week1/wedn/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make schedule for reading about teaching, and begin browsing UW resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make edits from yesterdays read-through of bypass proposal and look for last couple references&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Schedule:
The UW resource page has 20 sections. The CMU page has 16 sections, and the Visual Learning book has 31 chapters. I plan on reading roughly 1 section, 1 section, and 2 chapters each week, starting next week.&lt;/p&gt;
&lt;p&gt;Proposal:
Completed and sent to Andrew for feedback.&lt;/p&gt;
&lt;p&gt;Animal Movement:
Looked over the code, tried to re-run it. Got an Anaconda version conflict that I don&amp;rsquo;t remember. Will plan on dealing with that tomorrow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook Structure</title>
      <link>/post/notebook/outline/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/outline/</guid>
      <description>&lt;p&gt;In this post I will outline the structure of my virtual lab notebook, for use during this time of woe. This is necessitated by the current COVID-19 work-from-home crisis, but will be helpful in general as a tool for accountability and productivity.&lt;/p&gt;
&lt;h2 id=&#34;post-frequency-&#34;&gt;Post Frequency&lt;/h2&gt;
&lt;p&gt;I will post to the lab notebook at the beginning and end of each year, quarter, and week. I will also post to the lab notebook once per day.&lt;/p&gt;
&lt;h2 id=&#34;post-content-&#34;&gt;Post Content&lt;/h2&gt;
&lt;p&gt;In each multi-day post I will outline my goals for the current time period, along with a self-imposed due date, and (particularly for yearly and quarterly posts) a rough outline of intermediate steps. Then at the end of the period I will summarize my progress toward those goals as well as any major progress in addition to those goals that I undertook in that time.&lt;/p&gt;
&lt;p&gt;In each daily post I will outline what my goals were for the day, along with what I actually spent my time on. I will post mathematical and scientific outputs such as documents and plots that I produced that day, and reflect on my productivity and time management.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 2020</title>
      <link>/post/notebook/2020/annualpost-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/annualpost-1/</guid>
      <description>&lt;p&gt;I am just starting this lab notebook now in March 2020, so almost a third of the year is behind me already. That said, one of my big goals for the year, submitting my 
&lt;a href=&#34;content/project/dam-passage-time&#34;&gt;dam passage&lt;/a&gt;
 paper to Proc B, I have already accomplished! Hooray! Hopefully I can continue the momentum now that COVID-19 has well and truly set in.&lt;/p&gt;
&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;I have a tendency to be overly ambitious with my goal timelines. I think I&amp;rsquo;m okay with that for now, but I&amp;rsquo;m trying to temper my expectations a little bit. I&amp;rsquo;m hopeful that with my dam passage project behind me I can focus my time and make more steady progress on my other projects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete my bypass to the PhD track by April 1
&lt;ul&gt;
&lt;li&gt;This is a pretty near-term goal but has been a long time coming&lt;/li&gt;
&lt;li&gt;At this point most of what I need is just to do a bunch of lit reviewing and write up a beefier dissertation proposal than I had the first time round&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Submit my source inference paper by August 1 2020
&lt;ul&gt;
&lt;li&gt;Hopefully the timeline on this is not too ambitious. I have a lot of the theory and code done already, but case studies, editting, and background reading take forever.&lt;/li&gt;
&lt;li&gt;Sub goal: Write up animal movement case study by April 10&lt;/li&gt;
&lt;li&gt;Sub goal: Set up model and begin data analysis for pollen case study by April 17&lt;/li&gt;
&lt;li&gt;Sub goal: Write up pollen case study by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete third case study by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete all background reading by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete final editing by July 3&lt;/li&gt;
&lt;li&gt;This leaves one month of wiggle room for things to take longer than I hope&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prepare to TA by September 15
&lt;ul&gt;
&lt;li&gt;This fall will be my first time TAing in any capacity since I was an undergrad. I will have more responsibility now than I did then and I want to make sure I am prepared. To do this I plan on doing background reading on teaching and TAing ahead of time. I know the best experience comes from doing but this is all I can do for now.&lt;/li&gt;
&lt;li&gt;Sub goal: Browse UW TAing 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Browse CMU CollectedWisdom TAing 
&lt;a href=&#34;https://www.cmu.edu/teaching/resources/PublicationsArchives/CollectedWisdom/collectwisdom-teachingstrategies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resource&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Read some of the evidence-based 
&lt;a href=&#34;https://visible-learning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visible Learning&lt;/a&gt;
 resources&lt;/li&gt;
&lt;li&gt;I haven&amp;rsquo;t assigned sub-goal deadlines because these are resources I intend to digest slowly over the period. Instead my goal is to do a little of this every week.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Submit my spatial survey design paper by Jan 1
&lt;ul&gt;
&lt;li&gt;I feel pretty good about getting the other goals done, if not on time, at least this year. This one is the stretch-iest of all of them. But I certainly won&amp;rsquo;t do it if I give up on it now!&lt;/li&gt;
&lt;li&gt;Sub goal: Find good spatial dataset by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete background reading by July 3&lt;/li&gt;
&lt;li&gt;Sub goal: Complete simulated case study by September 18&lt;/li&gt;
&lt;li&gt;Sub goal: Complete final edits by December 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/24</title>
      <link>/post/notebook/2020/spring/week1/tues/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add references for optimal design chapter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add references for optimal design chapter
&lt;ul&gt;
&lt;li&gt;including major reference to Lee 1998, which actually does do logistically constrained optimal design but with a different objective function&lt;/li&gt;
&lt;li&gt;propose to compare performance of my method with his, as well as to speed up his method with sparse matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 4-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on new idea for logistically-constrained design
&lt;ul&gt;
&lt;li&gt;logistically-constrained random sampling?&lt;/li&gt;
&lt;li&gt;a number of possibilities for how to achieve it&lt;/li&gt;
&lt;li&gt;all rely upon being able to calculate or estimate $\pi_i$ and $\pi_{ij}$ the first- and second-order inclusion probabilities&lt;/li&gt;
&lt;li&gt;if we want to estimate them, can we define a Gaussian-like spatial process over second-order inclusion probabilities&lt;/li&gt;
&lt;li&gt;how can we do that? what is the domain of valid second-order inclusion probability matrices?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 1</title>
      <link>/post/notebook/2020/spring/week1/post-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Part of the week has already gone by, but I&amp;rsquo;ll go ahead and fill in some meta goals anyway&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure by Tuesday&lt;/li&gt;
&lt;li&gt;Make schedule for reading about teaching and browse UW 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
 by Wednesday&lt;/li&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study By Thursday&lt;/li&gt;
&lt;li&gt;Complete all references for optimal design chapter in bypass proposal by Friday&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did all these things! Hooray!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Spring 2020</title>
      <link>/post/notebook/2020/spring/spring-post-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/spring-post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Much of this is copied and pasted from my goals for the year:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete my bypass to the PhD track by April 1&lt;/li&gt;
&lt;li&gt;Make progress on source inference
&lt;ul&gt;
&lt;li&gt;Sub goal: Write up animal movement case study by April 10&lt;/li&gt;
&lt;li&gt;Sub goal: Set up model and begin data analysis for pollen case study by April 17&lt;/li&gt;
&lt;li&gt;Sub goal: Write up pollen case study by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete third case study by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete all background reading by June 5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prepare to TA
&lt;ul&gt;
&lt;li&gt;My real goal here is to do just a little bit of reading, every single week&lt;/li&gt;
&lt;li&gt;Sub goal: Browse UW TAing 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Browse CMU CollectedWisdom TAing 
&lt;a href=&#34;https://www.cmu.edu/teaching/resources/PublicationsArchives/CollectedWisdom/collectwisdom-teachingstrategies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resource&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Read some of the evidence-based 
&lt;a href=&#34;https://visible-learning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visible Learning&lt;/a&gt;
 resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Make progress on optimal survey design
&lt;ul&gt;
&lt;li&gt;Sub goal: Find good spatial dataset by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete half of background reading by June 15&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Source Reconstruction for Linear PDEs</title>
      <link>/talk/spacetime-feb-2020/</link>
      <pubDate>Wed, 12 Feb 2020 13:30:00 +0000</pubDate>
      <guid>/talk/spacetime-feb-2020/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt;
 feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;
 | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;
: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/spacetime-feb-2020/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/spacetime-feb-2020/</guid>
      <description>&lt;h1 id=&#34;the-spde-method-for-source-inference&#34;&gt;The SPDE Method for Source Inference&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pollution
&lt;ul&gt;
&lt;li&gt;Say, PCBs in the Duwamish&lt;/li&gt;
&lt;li&gt;Where did it come from?&lt;/li&gt;
&lt;li&gt;Easy case: point source
&lt;ul&gt;
&lt;li&gt;Measure all pipe outlets&lt;/li&gt;
&lt;li&gt;Use regularization method for unknown source&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hard case: non-point source
&lt;ul&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;possible-approaches&#34;&gt;Possible Approaches&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Basically this is the advection-diffusion equation
&lt;ul&gt;
&lt;li&gt;Possibly with linear decay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Existing methods
&lt;ul&gt;
&lt;li&gt;FFT/Kalman filter (Sigrist et. al 2014)
&lt;ul&gt;
&lt;li&gt;complex method&lt;/li&gt;
&lt;li&gt;non-local basis functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;finite-difference/element method (Stroud et. al 2010)
&lt;ul&gt;
&lt;li&gt;comparatively simple, very popular&lt;/li&gt;
&lt;li&gt;local basis functions&lt;/li&gt;
&lt;li&gt;possibly slower in some contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian functional regression (Nguyen and Peraire 2015)
&lt;ul&gt;
&lt;li&gt;also complex, relatively unknown&lt;/li&gt;
&lt;li&gt;basically a fancy &amp;ldquo;kernel trick&amp;rdquo; method&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-math&#34;&gt;The Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our model is $\mathcal{L}u = f$&lt;/li&gt;
&lt;li&gt;Assume $\mathcal{L}$ is a linear operator&lt;/li&gt;
&lt;li&gt;Then we can discretize this as $Lu = f$&lt;/li&gt;
&lt;li&gt;We model $f\sim X\beta + Z\gamma + \epsilon$&lt;/li&gt;
&lt;li&gt;If everything is normal $[f|d] \sim \mbox{MVN}(\mu_{\rm post},Q_{\rm post}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;proscons&#34;&gt;Pros/Cons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fast, lots of sparse matrices&lt;/li&gt;
&lt;li&gt;Flexible modeling on $f$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires known physics&lt;/li&gt;
&lt;li&gt;Requires linear PDE&lt;/li&gt;
&lt;li&gt;Rigid statistical modeling of everything else&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stationary distribution of a space-time SPDE process&lt;/li&gt;
&lt;li&gt;Stochastic perturbation analysis&lt;/li&gt;
&lt;li&gt;More case studies
&lt;ul&gt;
&lt;li&gt;Animal movement data&lt;/li&gt;
&lt;li&gt;Snowpack data&lt;/li&gt;
&lt;li&gt;Pollen data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Modeling alternative stable state in Caribbean coral reefs</title>
      <link>/publication/coral-reefs/</link>
      <pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/publication/coral-reefs/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slowing the Evolution and Outbreak of Antibiotic Resistance</title>
      <link>/publication/antibiotic-resistance/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      <guid>/publication/antibiotic-resistance/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collective Behavior During Dam Passage</title>
      <link>/project/dam-passage/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/dam-passage/</guid>
      <description>&lt;p&gt;In this project I use a bootstrap method to test a radio-telemetry dataset of Chinook and sockeye salmon for evidence of collective navigation while using fish ladders to pass dams.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistically-Constrained Optimal Spatial Sampling Design</title>
      <link>/project/opt-design/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/opt-design/</guid>
      <description>&lt;p&gt;In this project I investigate the use of sparse precision matrices and mixed integer/linear programs to efficient conduct logistically-constrained optimal spatial sampling design.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Source Inference</title>
      <link>/project/source/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/source/</guid>
      <description>&lt;p&gt;In this project I investigate the use of stochastic PDEs to conduct inference on the source term of a linear PDE.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/notebook/2020/spring/week4/post-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week4/post-1/</guid>
      <description>&lt;p&gt;\1;5202;0c&amp;mdash;
authors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;admin
categories:&lt;/li&gt;
&lt;li&gt;Notebook
date: &amp;ldquo;2020-04-13T00:00:00Z&amp;rdquo;
draft: false
featured: false
image:
projects: []
subtitle:
summary: &amp;lsquo;My weekly lab notebook goals post for week 4 of spring 2020&amp;rsquo;
tags:&lt;/li&gt;
&lt;li&gt;Notebook
title: &amp;lsquo;Lab Notebook: Sp20 Week 4&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Need to debug and write up the animal movement case study, as I have fallen behind on my goal for that&lt;/li&gt;
&lt;li&gt;Need to import pollen data and set up a preliminary model for it&lt;/li&gt;
&lt;li&gt;Keep working a little bit on the SNN modeling math to get it into shape&lt;/li&gt;
&lt;li&gt;Finish up notes on VL Ch 3 and 4&lt;/li&gt;
&lt;li&gt;Read and take notes on VL Ch 5 and 6&lt;/li&gt;
&lt;li&gt;Find large spatial dataset for optimal sampling design&lt;/li&gt;
&lt;li&gt;Need to write up NSF GRF progress report&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did debug and partially write up the animal movement case study but there is still more writing/background to do there. Found a problem with the pollen data so that also will take longer than expected. Worked a little bit on the SSN modeling and found a matrix formulation for it (another DT lyapunov equation) although in the end it&amp;rsquo;ll probably all be finite difference anyways. Took notes on VL Ch 3-6, and decided with Sándor that we will just do a purely synthetic dataset to start for the sampling design. Wrote up NSF progress report, although still waiting on Andrew to sign off.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
