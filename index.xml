<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Colin Okasaki</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Colin Okasaki</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>/img/icon.png</url>
      <title>Colin Okasaki</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Notation</title>
      <link>/courses/qsci483/notation/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/notation/</guid>
      <description>&lt;h2 id=&#34;matrixvector-notation-&#34;&gt;Matrix/Vector Notation&lt;/h2&gt;
&lt;p&gt;In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector&amp;rsquo;s components are not specified I will bold it such as $\mathbf{v}$. I will write $X&#39;$ to mean the transpose of a matrix $X$ or $v&#39;$ to mean the transpose of a matrix $v$, as is common in regression analysis.&lt;/p&gt;
&lt;h3 id=&#34;advanced-notation-matrix-calculus-&#34;&gt;Advanced Notation: Matrix Calculus&lt;/h3&gt;
&lt;p&gt;For an $n\times 1$ vector $v$ and a scalar $f$ we will write $\dfrac{\partial v}{\partial f}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial v}{\partial f}\right)_i = \dfrac{\partial v_i}{\partial f}$. We will write $\dfrac{\partial f}{\partial v}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial f}{\partial v}\right)_i = \dfrac{\partial f}{\partial v_i}$. A useful resource for matrix calculus can be found on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
 or in the more extensive 
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notation and Terminology</title>
      <link>/courses/gaussian-processes/notation/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/notation/</guid>
      <description>&lt;h2 id=&#34;matrixvector-notation-&#34;&gt;Matrix/Vector Notation&lt;/h2&gt;
&lt;p&gt;In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector&amp;rsquo;s components are not specified I will bold it such as $\mathbf{v}$. I will write $X&#39;$ to mean the transpose of a matrix $X$ or $v&#39;$ to mean the transpose of a matrix $v$, as is common in regression analysis.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus-&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;p&gt;For an $n\times 1$ vector $v$ and a scalar $f$ we will write $\dfrac{\partial v}{\partial f}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial v}{\partial f}\right)_i = \dfrac{\partial v_i}{\partial f}$. We will write $\dfrac{\partial f}{\partial v}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial f}{\partial v}\right)_i = \dfrac{\partial f}{\partial v_i}$. A useful resource for matrix calculus can be found on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
 or in the more extensive 
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-process-notation-&#34;&gt;Gaussian Process Notation&lt;/h2&gt;
&lt;p&gt;I will write $f \sim GP(\mu(x),k(x,x&amp;rsquo;))$ to denote a Gaussian process with mean function $\mu$ and covariance function (kernel) $k(x,x&amp;rsquo;)$. I will in general &lt;em&gt;not&lt;/em&gt; assume that $k$ is stationary (see 
&lt;a href=&#34;#terminology&#34;&gt;terminology&lt;/a&gt;
). Under this definition $f$ is a stochastic process with the defining property that for any set of points ${x_i}$ (in whatever space $\Omega$ we choose), the vector $f(x)_i = f(x_i)$ is distributed as a multivariate normal (MVN) random vector with mean $\mu_i = \mu(x_i)$ and covariance matrix $\Sigma_{ij} = k(x_i,x_j)$. Many of the properties I will discuss on this page are actually properties of the MVN distribution.&lt;/p&gt;
&lt;h2 id=&#34;terminology-&#34;&gt;Terminology&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process is called &lt;em&gt;stationary&lt;/em&gt; (or &lt;em&gt;homogeneous&lt;/em&gt;) if $k(x,x&amp;rsquo;) = k(x-x&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;A Gaussian Process is called &lt;em&gt;isotropic&lt;/em&gt; if $k(x,x&amp;rsquo;) = k(|x-x&#39;|)$&lt;/li&gt;
&lt;li&gt;A matrix $M$ is said to be &lt;em&gt;positive semidefinite&lt;/em&gt; if it has the property that $v&amp;rsquo;Mv \geq 0$ for any vector $v$. Covariance matrices are positive semidefinite.&lt;/li&gt;
&lt;li&gt;A kernel is said to be positive semidefinite (psd) if $$\int_\Omega k(x,x&amp;rsquo;)f(x)f(x&amp;rsquo;)dxdx&amp;rsquo; \geq 0$$ for all $L_2$ functions $f$. Gram matrices (i.e. covariance matrices) from psd kernels are psd matrices.&lt;/li&gt;
&lt;li&gt;The inverse of the covariance matrix in a MVN distribution is $Q = \Sigma^{-1}$ and is called the &lt;em&gt;precision matrix&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Simple Linear Regression</title>
      <link>/courses/qsci483/linear-regression/simple-linear-regression/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/simple-linear-regression/</guid>
      <description>&lt;p&gt;In this document I will outline the math used in the analysis of a simple linear regression model. Make sure to check out my 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 document to clarify any confusing notation.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Simple Linear Regression is based upon the equation
$$
y_i \sim N(\beta_0 + \beta_1 x_i,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim \beta_0 + \beta_1 x_i + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. It is important to remember that the expectations, or means, of these variables are: $E[y_i] = \beta_0 + \beta_1 x_i$ and $E[\epsilon_i] = 0$. We will assume that there are $n$ data points and only 1 predictor. This is what makes it _simple_ linear regression. In more general linear regression models you have more than 1 predictor. In multivariate linear regression models you have more than one dependent variable as well. In addition to the predictor variable, we also have an intercept, or mean term, which can be thought of as a second predictor.&lt;/p&gt;
&lt;h3 id=&#34;fitting-&#34;&gt;Fitting&lt;/h3&gt;
&lt;h4 id=&#34;objective-function-&#34;&gt;Objective Function&lt;/h4&gt;
&lt;p&gt;To fit a simple linear regression we first must choose a measure of fit. Here we will choose least squares, since this corresponds to 
&lt;a href=&#34;/courses/qsci483/linear-regression/properties&#34;&gt;maximum likelihood esitmation&lt;/a&gt;
 in this model. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\hat{y}_i$, the value predicted by our fitted model. We can calculate $\hat{y}_i$ using our predictor $x_i$ for data point $i$ along with estimated coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$:
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x.
$$
Alternatively we can find the whole vector $\hat{y} = \hat{\beta_0}\mathbf{1} + x\hat{\beta}$ (where $\mathbf{1}$ is a vector of all ones). To find a single squared residuals we calculate $r_i^2 = (y_i - \hat{\beta}_0 - \hat{\beta}_1 x)^2$. We will define the function $f(\hat{\beta}_0,\hat{\beta}_1)$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as:
\begin{align*}
f(\hat{\beta})
&amp;amp; = \sum_{i=1}^n (\mbox{residual})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1)^2 \\&lt;br&gt;
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;optimization-&#34;&gt;Optimization&lt;/h4&gt;
&lt;p&gt;From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to both $\beta_0$ and $\beta_1$, to find the minimum sum of squared residuals (the &lt;em&gt;least squares&lt;/em&gt;). So let us take the partial derivative with respect to $\hat{\beta}_0$:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_0} }
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_0} \sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_0}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)\dfrac{\partial }{ \partial \hat{\beta}_0}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)
\end{align*}
To set this derivative equal to zero we need to set:
\begin{align*}
\sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) &amp;amp; = 0.
\end{align*}
This can be accomplished by splitting up the sum and getting
\begin{align*}
\sum_{i=1}^n y_i &amp;amp; = \sum_{i=1}^n \hat{\beta}_0 + \sum_{i=1}^n \hat{\beta}_1 x_i \\&lt;br&gt;
n\hat{\beta}_0 &amp;amp; = \sum_{i=1}^n y_i - \hat{\beta}_1 \sum_{i=1}^n x_i \\&lt;br&gt;
\hat{\beta}_0 &amp;amp; = \overline{y} - \hat{\beta}_1\overline{x}
\end{align*}
Now that we have calculated $\hat{\beta}_0$ in terms of $y,x,$ and $\hat{\beta_1}$ we can take the derivative with respect to $\beta_1$ to find the least squares estimator:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_1} }
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_1} \sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_1}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)\dfrac{\partial }{ \partial \hat{\beta}_1}\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2\left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)(x_i)
\end{align*}
To set this derivative equal to zero we need to set:
\begin{align*}
\sum_{i=1}^n \left(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i\right)x_i &amp;amp; = 0.
\end{align*}
But here we can plug in our estimator $\hat{\beta}_0$ to get
\begin{align*}
\sum_{i=1}^n \left(y_i - (\overline{y}-\hat{\beta}_1\overline{x}) - \hat{\beta}_1x_i\right)x_i &amp;amp; = 0.
\end{align*}
Then we can expand the sum on the left to get
\begin{align*}
\sum_{i=1}^n (y_i - \overline{y})x_i - \sum_{i=1}^n \hat{\beta}_1(x_i - \overline{x})x_i &amp;amp; = 0 \\&lt;br&gt;
\hat{\beta}_1 \sum_{i=1}^n (x_i - \overline{x})x_i &amp;amp; = \sum_{i=1}^n (y_i-\overline{y})x_i \\&lt;br&gt;
\hat{\beta}_1 &amp;amp; = \frac{\sum_{i=1}^n (y_i-\overline{y})x_i}{\sum_{i=1}^n (x_i - \overline{x})x_i}
\end{align*}
So we have found an estimator for $\hat{\beta}_1$ in terms of only the predictors and the responses. We also have an estimator for $\hat{\beta}_0$ in terms of the predictors, the responses, and $\hat{\beta}_1$. Now, traditionally, $\hat{\beta}_1$ is written in a slightly different form, as
\begin{align*}
\hat{\beta}_1 &amp;amp; = \frac{S_{xy}}{S_{xx}} \\&lt;br&gt;
S_{xx} &amp;amp; = \frac{1}{n-1}\sum (x_i-\overline{x})^2 \\&lt;br&gt;
S_{xy} &amp;amp; = \frac{1}{n-1}\sum (x_i-\overline{x})(y_i-\overline{y}).
\end{align*}
The reason for this is that $S_{xy}$ is the sample covariance of $x$ and $y$, and $S_{xx}$ is the sample variance of $x$, so it is nice to express $\hat{\beta}_1$ in terms of other statistics that we already know about. We can see that the two formulas for $\hat{\beta}_1$ are equivalent by doing a little more math, taking $S_{xx}$ and $S_{xy}$ and changing them to a slightly different format:
\begin{align*}
(N-1)S_{xy} &amp;amp; = \sum (x_i-\overline{x})(y_i-\overline{y}) \\&lt;br&gt;
&amp;amp; = \sum (y_i-\overline{y})x_i - \sum (y_i-\overline{y})\overline{x} \\&lt;br&gt;
&amp;amp; = \sum (y_i-\overline{y})x_i - \overline{x}\sum (y_i - \overline{y}) \\&lt;br&gt;
&amp;amp; = \sum (y_i -\overline{y})x_i - \overline{x}(\sum \left(y_i - \frac{1}{n}\sum y_i\right) \\&lt;br&gt;
&amp;amp; = \sum (y_i -\overline{y})x_i - \overline{x}(\sum y_i - \sum y_i) \\&lt;br&gt;
&amp;amp; = \sum (y_i - \overline{y})x_i
\end{align*}
I encourage you to try doing the same calculation with $S_{xx}$: you will find that it follows exactly the same format. So we can see that the formula we have derived for $\hat{\beta}_1$ is exactly the same as the traditional format in terms of the sample (co)variances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>General Linear Algebraic Properties</title>
      <link>/courses/gaussian-processes/linear-algebra/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/linear-algebra/</guid>
      <description>&lt;p&gt;The Woodbury matrix identity is $$(A+UCV)^{-1}=A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.$$ Simpler versions of this identity are
\begin{align*}
(I+UV)^{-1} &amp;amp; = I-U(I+VU)^{-1}V, \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-(I+P)^{-1}P \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-P(I+P)^{-1}
\end{align*}
In the special case that $u$ and $v$ are vectors and $C = I$ we get the Sherman-Morrison formula: $$(A+uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.$$ A similar formula is the matrix determinant lemma $$\mbox{det}(A+uv^T) = (1+v^TA^{-1}u)\mbox{det}(A).$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/courses/qsci483/linear-regression/linear-regression/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/linear-regression/</guid>
      <description>&lt;p&gt;In this document I will outline the math used in the analysis of a more general linear regression model, with more than one predictor variable. Make sure to check out my 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 document to clarify any confusing notation, and check out my 
&lt;a href=&#34;/courses/qsci483/linear-regression/simple-linear-regression&#34;&gt;simple linear regression&lt;/a&gt;
 document if you would like a slightly simpler analysis to start off with. This document will charge right into matrix notation from the start.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Linear regression is based upon the equation
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. In the context of regression, it is important to remember that: $E[y_i] = X\beta$ and $E[\epsilon_i] = 0$. We will assume that there are $n$ data points and $k$ predictors. This $\beta$ is a $k\times 1$ vector, $X$ is a $n\times k$ matrix and $y$ and $\epsilon$ are $n\times 1$ vectors.&lt;/p&gt;
&lt;h3 id=&#34;fitting-&#34;&gt;Fitting&lt;/h3&gt;
&lt;h4 id=&#34;objective-function-&#34;&gt;Objective Function&lt;/h4&gt;
&lt;p&gt;To fit a simple linear regression we choose a measure of fit: least squares. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\hat{y}_i$, the value predicted by our fitted model. We can calculate $\hat{y}_i$ using the $1\times k$ vector $x_i$ of predictors for data point $i$:
$$
\hat{y}_i = x_i\hat{\beta}.
$$
Alternatively we can find the whole vector $\hat{y} = X\beta$. To find a single squared residuals we calculate $r_i^2 = (y_i - x_i\hat{\beta})^2$. We will define the function $f(\hat{\beta})$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as:
\begin{align*}
f(\hat{\beta})
&amp;amp; = \sum_{i=1}^n (\mbox{residual})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n (y_i - x_i\hat{\beta})^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2
\end{align*}
In matrix notation we can rewrite this, however. The residual _vector_ can be written as the $n\times 1$ vector
$$
r = y - X\hat{\beta}.
$$
The sum of squares can then be written as $r&amp;rsquo;r$. So then
\begin{align*}
f(\hat{\beta})
&amp;amp; = r&amp;rsquo;r \\&lt;br&gt;
&amp;amp; = (y-X\hat{\beta})&#39;(y-X\hat{\beta}).
\end{align*}
We can expand this quadratic equation as
\begin{align*}
f(\hat{\beta})
&amp;amp; = y&amp;rsquo;y - y&amp;rsquo;X\hat{\beta} - \hat{\beta}&amp;lsquo;X&amp;rsquo;y + \hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}.
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;optimization-&#34;&gt;Optimization&lt;/h4&gt;
&lt;p&gt;From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to each $\beta_i$, to find the minimum sum of squared residuals (the &lt;em&gt;least squares&lt;/em&gt;). So let us take the partial derivative with respect to a particular coefficient:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_j}
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}_j} \sum_{i=1}^n \left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n \dfrac{\partial }{ \partial \hat{\beta}_j}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)^2 \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\dfrac{\partial }{ \partial \hat{\beta}_j}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right) \\&lt;br&gt;
&amp;amp; = \sum_{i=1}^n 2\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\left(-x_{ij}\right) \\&lt;br&gt;
&amp;amp; = -\sum_{i=1}^n 2x_{ij}\left(y_i - \sum_{j=1}^k x_{ij}\hat{\beta}_j\right)\end{align*}
Verify that this can be written in matrix notation as
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}_j}
&amp;amp; = -2\left(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta}\right)_j,
\end{align*}
or in matrix calculus notation
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}} &amp;amp; = -2\left(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta}\right).
\end{align*}
Remember that according to calculus, every single entry $\dfrac{\partial f}{ \partial \beta_j}$ must be equal to zero at any extremum (and in particular at the minimum). Thus we can set this whole matrix equation equal to zero, and we get
$$
X&amp;rsquo;y = X&amp;rsquo;X\hat{\beta}.
$$
Since we are trying to derive an equation for $\hat{\beta}$ we move the matrix over to the other side and we get
$$
\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y.
$$
We can derive this same equation in fewer steps using the more complex matrix calculus notation, which for example allows us to take the derivative of matrix products and use the matrix chain rule:
\begin{align*}
\dfrac{\partial f}{ \partial \hat{\beta}}
&amp;amp; = \dfrac{\partial }{ \partial \hat{\beta}}\left(y&amp;rsquo;y - y&amp;rsquo;X\hat{\beta} - \hat{\beta}&amp;lsquo;X&amp;rsquo;y + \hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}\right) \\&lt;br&gt;
&amp;amp; = - \dfrac{\partial }{ \partial \hat{\beta}}\left(y&amp;rsquo;X\hat{\beta}\right) - \dfrac{\partial }{ \partial \hat{\beta}}\left(\hat{\beta}&amp;lsquo;X&amp;rsquo;y\right) + \dfrac{\partial }{ \partial \hat{\beta}}\left(\hat{\beta}&amp;lsquo;X&amp;rsquo;X\hat{\beta}\right) \\&lt;br&gt;
&amp;amp; = -y&amp;rsquo;X-(X&amp;rsquo;y)&amp;lsquo;+\hat{\beta}&amp;lsquo;X&amp;rsquo;X + (X&amp;rsquo;X\hat{\beta})&amp;rsquo; \\&lt;br&gt;
&amp;amp; = -2(X&amp;rsquo;y - X&amp;rsquo;X\hat{\beta})&amp;rsquo; \\&lt;br&gt;
&amp;amp; = 0.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;important-quantities&#34;&gt;Important Quantities&lt;/h3&gt;
&lt;p&gt;Now we have shown that the ordinary least squares estimator for $\beta$ is $(X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. Using this we may calculate all sorts of other things.&lt;/p&gt;
&lt;h4 id=&#34;predicted-values-&#34;&gt;Predicted Values&lt;/h4&gt;
&lt;p&gt;The predicted values are
\begin{align*}
\hat{y}
&amp;amp; = X\hat{\beta} \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo;y \\&lt;br&gt;
&amp;amp; = Hy.
\end{align*}
Here we have defined the &amp;ldquo;hat matrix,&amp;rdquo; $H = X(X&amp;rsquo;X)^{-1}X&#39;$. This is a useful matrix in linear regression, which maps the data to its predicted values. This is sometimes also called the projection matrix $P$, since it projects the data onto a lower-dimensional linear space. It is sometimes also called the influence matrix. It has two nice properties which we will use in a moment: it is symmetric and idempotent. This means $H&#39;=H$ and $H^2 = H&amp;rsquo;H = H$. We can see this by calculating:
\begin{align*}
H&amp;rsquo; &amp;amp; = (X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)&amp;rsquo; \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = H \\&lt;br&gt;
H^2 &amp;amp; = (X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)(X(X&amp;rsquo;X)^{-1}X&amp;rsquo;) \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}(X&amp;rsquo;X)(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = X(X&amp;rsquo;X)^{-1}X&amp;rsquo; \\&lt;br&gt;
&amp;amp; = H
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;residuals-&#34;&gt;Residuals&lt;/h4&gt;
&lt;p&gt;The residuals are
\begin{align*}
\hat{\epsilon}
&amp;amp; = y-\hat{y} \\&lt;br&gt;
&amp;amp; = y-Hy \\&lt;br&gt;
&amp;amp; = (I-H)y \\&lt;br&gt;
&amp;amp; = (I-X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)y \\&lt;br&gt;
&amp;amp; = My
\end{align*}
Here we have defined the &amp;ldquo;residual maker&amp;rdquo; matrix, which can also be called the residual operator. This matrix takes the data and gives you the residuals of the model. It also inherits symmetry and idempotency from the hat matrix, since:
\begin{align*}
M&amp;rsquo; &amp;amp; = (I-H)&amp;rsquo; \\&lt;br&gt;
&amp;amp; = I-H \\&lt;br&gt;
M^2 &amp;amp; = (I-H)(I-H) \\&lt;br&gt;
&amp;amp; = I - 2H + H^2 \\&lt;br&gt;
&amp;amp; = I - 2H + H \\&lt;br&gt;
&amp;amp; = I-H
\end{align*}
With all of this put together we now have the tools to analyze the 
&lt;a href=&#34;../standard-error&#34;&gt;residual standard error&lt;/a&gt;
. First, however, we will analyze the properties of $\hat{\beta}$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Block Matrices</title>
      <link>/courses/gaussian-processes/block-matrices/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/block-matrices/</guid>
      <description>&lt;p&gt;Block matrices have some nice linear algebraic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\&lt;br&gt;
-(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; (D-CA^{-1}B)^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
(A-BD^{-1}C)^{-1} &amp;amp; -(A-BD^{-1}C)^{-1}BD^{-1} \\&lt;br&gt;
-D^{-1}C(A-BD^{-1}C)^{-1} &amp;amp; D^{-1} + D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\mbox{det}
\begin{pmatrix}
A &amp;amp; B \&lt;br&gt;
C &amp;amp; D
\end{pmatrix}
=
\mbox{det}(A)\times \mbox{det}(D-CA^{-1}B) = \mbox{det}(D)\times\mbox{det}(A-BD^{-1}C)$$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Residual Standard Error</title>
      <link>/courses/qsci483/linear-regression/standard-error/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/standard-error/</guid>
      <description>&lt;p&gt;A standard error is the estimated standard deviation $\hat{\sigma}$ for some variable. The residual standard error for linear regression is our estimate of the standard deviation of the noise $\epsilon$. Recall that we assume the noise is independent and heteroskedastic. This is important since it means we only need to estimate one single parameter for all of the noise variables.&lt;/p&gt;
&lt;p&gt;We estimate the residual variance using the equation $$\hat{\sigma}^2 = \frac{1}{n-k}\sum \hat{\epsilon}_i^2.$$ You can think of the $n-k$ term as accounting for the fact that we have estimated $k$ parameters before making this estimate. Recall that we fit our model by minimizing the sum of squared residuals. So we&amp;rsquo;ve actually optimized this model to minimize exactly $\sum \hat{\epsilon}_i^2$. The more parameters we have to work with the better that optimization will be. So we would probably get an answer that was too small if we just calculated the mean $$\frac{1}{n}\sum \hat{\epsilon}_i^2.$$ Dividing by a smaller number $n-k$ accounts for this, making $\hat{\sigma}^2$ an unbiased estimator of the residual variance.&lt;/p&gt;
&lt;h3 id=&#34;unbiasedness-&#34;&gt;Unbiasedness&lt;/h3&gt;
&lt;p&gt;We can see why this is by calculating the expectation of $\hat{\sigma}^2$. We won&amp;rsquo;t even both trying to avoid matrices in this derivation:
\begin{align*}
E[\hat{\sigma}^2]
&amp;amp; = E\left[\frac{1}{n-k}\hat{\epsilon}&#39;\hat{\epsilon}\right] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[y&amp;rsquo;M&amp;rsquo;My] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[y&amp;rsquo;My] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} E[(X\beta + \epsilon)&amp;lsquo;M(X\beta+\epsilon)] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k} \left( E[\beta&amp;rsquo;X&amp;rsquo;MX\beta] + E[\epsilon&amp;rsquo;M\epsilon] \right)
\end{align*}
where we have eliminated the cross terms since $E[\epsilon]=0$. Now we can calculate
\begin{align*}
X&amp;rsquo;MX
&amp;amp; = X&amp;rsquo;(I-H)X \\&lt;br&gt;
&amp;amp; = X&amp;rsquo;X - X&amp;rsquo;(X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)X \\&lt;br&gt;
&amp;amp; = X&amp;rsquo;X - X&amp;rsquo;X \\&lt;br&gt;
&amp;amp; = 0
\end{align*}
so that the first term drops out. At this point we could either do a lot of algebra or we could make use of a convenient statistical property (we will do the latter). The expectation of a _quadratic form_ (i.e. $v&amp;rsquo;Mv$ for some random vector $v$ and constant matrix $M$) can be written as $$E[v&amp;rsquo;Mv] = \tr[M\Sigma] + \mu^TM\mu$$ where $\mu$ is the mean of $v$ and $\Sigma$ the covariance of $v$. Using this property we find that
\begin{align*}
E[\hat{\sigma^2}]
&amp;amp; = \frac{1}{n-k}E[\epsilon&amp;rsquo;M\epsilon] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k}\tr[M\Sigma(\epsilon)] \\&lt;br&gt;
&amp;amp; = \frac{1}{n-k}\tr[\sigma^2M] \\&lt;br&gt;
&amp;amp; = \frac{\sigma^2}{n-k}\tr[I-H].
\end{align*}
Now, so long as $X$ is full rank (i.e. there are no redundant predictors) the hat matrix has trace $k$. This can be shown using complicated eigenvalue proofs that you can find on Google. Thus $\tr[I-H] = \tr[I]-\tr[H] = n-k$. Thus indeed the $n-k$ term drops out and we find that $\hat{\sigma^2}$ is an unbiased estimator of the true residual variance.&lt;/p&gt;
&lt;h3 id=&#34;distribution-&#34;&gt;Distribution&lt;/h3&gt;
&lt;p&gt;In fact, we have done more than show it is unbiased. Much of the algebra that we did actually did not depend on the outer expectation. We actually showed more generally that $$\hat{\sigma^2} = \frac{1}{n-k}\epsilon&amp;rsquo;M\epsilon.$$ This is useful because it is a &lt;em&gt;quadratic form&lt;/em&gt;, as we described above. Quadratic forms over multivariate normal random vectors have nice properties which we will now derive. Specifically we will show that: $$\hat{\sigma^2} \sim \frac{\sigma^2}{n-k}\chi^2_{n-k}.$$&lt;/p&gt;
&lt;p&gt;This proof will involve some additional use of linear algebraic terms and assumptions. Specifically, we will use the fact that a symmetric matrix $A$ can be decomposed into $A = PDP^T$ for an &lt;em&gt;orthogonal&lt;/em&gt; (i.e. $P^2 = I$) matrix $P$ and a diagonal matrix $D$. Orthogonal matrices are nice because $\tilde{z} = Pz$, the product of an orthogonal matrix with a standard normal vector, is still a standard normal vector.&lt;/p&gt;
&lt;p&gt;We will also use the fact that if $A$ is symmetric &lt;em&gt;and&lt;/em&gt; idempotent then all the diagonal entries are either 0 or 1. We will further use the fact that the number of entries that are 1 is equal to the trace of $A$. Since $M$ is symmetric and idempotent we will use all these properties to show:
\begin{align*}
\epsilon&amp;rsquo;M\epsilon
&amp;amp; = \epsilon&amp;rsquo;PDP^T\epsilon \\&lt;br&gt;
&amp;amp; = \sigma^2 z&amp;rsquo;PDP^Tz \\&lt;br&gt;
&amp;amp; = \sigma^2 \tilde{z}&amp;lsquo;D\tilde{z} \\&lt;br&gt;
&amp;amp; = \sigma^2 \sum_{i=1}^n D_{ii}\tilde{z}_i^2 \\&lt;br&gt;
&amp;amp; = \sigma^2 \sum_{i=1}^{n-k} \tilde{z}_i^2 \\&lt;br&gt;
&amp;amp; = \sim \sigma^2 \chi^2_{n-k}.
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dirichlet BCs</title>
      <link>/courses/gaussian-processes/dirichlet/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/dirichlet/</guid>
      <description>&lt;p&gt;The simplest version of the finite element method is said to have &amp;ldquo;natural&amp;rdquo; Neumann boundary conditions, meaning that Neumann boundary conditions are naturally satisfied without imposing any additional structure. Dirichlet boundary conditions then are said to be &amp;ldquo;essential,&amp;rdquo; meaning they must be explicitly imposed after the fact. Certain FEM formulations change up this Neumann=natural, Dirichlet=essential default but for our purposes we will treat these terms as interchangeable.&lt;/p&gt;
&lt;p&gt;Now, suppose that we are confronted with the FEM equation $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ K_{21} &amp;amp; K_{22} \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11} &amp;amp; L_{12} \\ L_{21} &amp;amp; L_{22} \end{bmatrix} \begin{bmatrix} f_1 \\ f_2 \end{bmatrix},$$
with the Dirichlet BC $u_2 = u^*$. We can modify our FEM equation to enforce this, to $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ 0 &amp;amp; I \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}.$$ We can simplify this to an equation for $u$ by inverting the matrix on the left: $$\begin{bmatrix} u_1 \\ u_2 \end{bmatrix} = \begin{bmatrix} K_{11}^{-1} &amp;amp; -K_{11}^{-1}K_{12} \\ 0 &amp;amp; I \end{bmatrix} \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}$$
Now, assuming that $f\sim N(\mu_f,Q_f)$ and $u^*$ is given (often $u^* = 0$) we obtain the following distribution for $u_1$:
\begin{align*}
u_1 &amp;amp; = K_{11}^{-1}L_1f - K_{11}^{-1}K_{12}u^* \\&lt;br&gt;
u_1 &amp;amp; \sim N\left(K_{11}^{-1}(L_1\mu_f - K_{12}u^*), K_{11}^{-1}L_1\Sigma_fL_1^TK_{11}^{-T}\right).
\end{align*}&lt;/p&gt;
&lt;p&gt;Now often we are interested in the posterior distribution for $u$ and/or $f$. Since we have assumed that $u^*$ is given the two are deterministically related. However, $u$ is of lower dimension than $f$. So, we will calculate the posterior distribution of $f$ since this can be used to calculate the posterior of $u$ but not vice versa. We will assume that we have made some observations $y = y_1 + y_2 = A_1u_1 + A_2u_2 + \epsilon$ of $u$ from which we wish to make inference. Since $u^*$ is known we essentially have observations:
\begin{align*}
y &amp;amp; = A_1K_{11}^{-1}(L_1f - K_{12}u^*) + A_2u^* + \epsilon \\&lt;br&gt;
&amp;amp; = A_1K_{11}^{-1}L_1f + (A_2 - A_1K_{11}^{-1}K_{12})u^* + \epsilon \\&lt;br&gt;
&amp;amp; = y_f + y^* + \epsilon
\end{align*}
Using the 
&lt;a href=&#34;../posteriors/#linear-observations&#34;&gt;results&lt;/a&gt;
 for a posterior from a linearly-observed MVN distribution we see that:
\begin{align*}
[f|y=a] &amp;amp; = N\left(\mu_f + Q_{f|y}^{-1}B^TQ_{\epsilon}(y - y^* - B^T\mu_f), Q_{f|y}^{-1}\right) \\&lt;br&gt;
Q_{f|y} &amp;amp; = Q_f + B^TQ_{\epsilon}B \\&lt;br&gt;
B &amp;amp; = A_1K_{11}^{-1}L_1
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posteriors</title>
      <link>/courses/gaussian-processes/posteriors/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/posteriors/</guid>
      <description>&lt;p&gt;The multivariate normal distribution has nicely behaved posterior distributions. In particular, the posterior given an observation of the vector is also multivariate normal.&lt;/p&gt;
&lt;h2 id=&#34;basic-posterior-&#34;&gt;Basic Posterior&lt;/h2&gt;
&lt;p&gt;Let $x$ be an $n\times 1$ vector partitioned into $x = (x_1,x_2)$, with $x_1$ having dimension $k$ and $x_2$ having dimension $n-k$. Partition the mean $\mu = (\mu_1,\mu_2)$ and covariance matrix $$\Sigma = \begin{bmatrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix}.$$ Then the posterior distribution for $x_1$ given $x_2=a$ is given by
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(a-\mu_2), \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;precision-formulation&#34;&gt;Precision Formulation&lt;/h2&gt;
&lt;p&gt;Often in my research we are interested in analyzing a MVN distribution with known sparse &lt;em&gt;precision&lt;/em&gt; matrix. It is expensive to invert matrices and cheap to work with sparse matrices so we wish to work directly with this precision matrix. Furthermore, we want to calculate the precision matrix for our posterior because it is likely to also be computational advantageous. Let the precision matrix be partitioned
$$Q = \begin{bmatrix} Q_{11} &amp;amp; Q_{12} \\ Q_{21} &amp;amp; Q_{22} \end{bmatrix}.$$
Then the equivalent posterior distribution is
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + Q_{11}^{-1}Q_{12}(a-\mu_2),Q_{11}\right).
\end{align*}
Furthermore, let $RR^T = Q$ be the Cholesky decomposition of $Q$ also be partitioned into blocks. If $Q$ is sparse then under mild conditions, $R$ is also sparse and we can work with it to do quick computation. The posterior distribution now is $$[x_1|x_2=a] = N\left(\mu_1 - R_{11}^{-1}(R_{11}^{-T}(Q_{12}(a-\mu_2))), Q_{11}\right).$$&lt;/p&gt;
&lt;h2 id=&#34;linear-observations&#34;&gt;Linear Observations&lt;/h2&gt;
&lt;p&gt;Often we are also interested in analyzing a MVN distribution where we observe not the individual components, but a linear combination thereof $y = Ax + \epsilon$ with some mean-zero MVN noise vector $\epsilon$. Then the observations are distributed $y|x \sim N( Ax , Q_\epsilon^{-1} )$. The posterior $x|y$ is given by:
\begin{align*}
[x|y=a] &amp;amp; = N\left(\mu_x + Q_{x|y}^{-1}A^TQ_{\epsilon}(y-A\mu_x), Q_{x|y}\right) \\&lt;br&gt;
Q_{x|y} &amp;amp; = (Q + A^TQ_{\epsilon}A)^{-1}
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Properties of Linear Regression</title>
      <link>/courses/qsci483/linear-regression/properties/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/properties/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to analyze our previous results for linear regression analysis. Make sure to check out my previous posts on 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 and 
&lt;a href=&#34;/courses/qsci483/linear-regression/linear-regression&#34;&gt;simple linear regression&lt;/a&gt;
 before diving in.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Recall that linear regression is based upon the equation:
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. We have previously found that the estimate $\hat{\beta} = (X&amp;rsquo;X)^{-1}Xy$ minimizes the sum of squares. In this document we will show: (1) that $\hat{\beta}$ is also the maximum likelihood estimator, (2) that $\hat{\beta}$ is unbiased, and (3) the covariance matrix for the estimator $\hat{\beta}$.&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood-&#34;&gt;Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;Our probability model has that $y_i$ are normally distributed, and are independent of each other given the predictors $X$ and the coefficients $\beta$. The &lt;em&gt;likelihood function&lt;/em&gt; is given by the probability (density) of the data given the parameters ($\beta$), expressed as a function of the parameter estimate ($\hat{\beta}$). This function in our case can be written as a product of Gaussian (normal) probability density functions (pdfs):
\begin{align*}
\ell(\hat{\beta}) &amp;amp; = \prod_{i=1}^n N(y_i|X\hat{\beta},\sigma^2) \\&lt;br&gt;
&amp;amp; = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - X\hat{\beta})}{2\sigma^2}\right) \\&lt;br&gt;
&amp;amp; = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right).
\end{align*}
We will now make use of a common trick in statistics: we will calculate the log-likelihood function $\lambda(\hat{\beta}) = \log(\ell(\hat{\beta}))$. Since the probability density function is always non-negative, and therefore the likelihood is always non-negative, the log-likelihood can be defined. Furthermore, the log is a convex function, and because of this it has the property that $\ell$ and $\lambda$ are minimized at the same value $\hat{\beta}$. Why this is is not important for this class.&lt;/p&gt;
&lt;p&gt;So taking the log we obtain
\begin{align*}
\lambda(\hat{\beta})
&amp;amp; = \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right)\right) \\&lt;br&gt;
&amp;amp; = \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\right) + \log\left(\exp\left(-\sum_{i=1}^n \frac{(y_i - X\hat{\beta})}{2\sigma^2}\right)\right) \\&lt;br&gt;
&amp;amp; = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-X\hat{\beta})^2.
\end{align*}
Now remember that we are looking for the _maximum likelihood estimator_. So we want to find the value of $\hat{\beta}$ which maximizes the likelihood (i.e. maximizes the probability of that data, given the parameters). Viewing this as a function of $\hat{\beta}$ we see that maximizing $\lambda$ is equivalent to minimizing
$$
\sum_{i=1}^n (y_i-X\hat{\beta})^2.
$$
Therefore the maximum likelihood estimator (MLE) of $\hat{\beta}$ is exactly the least-squares estimator $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. Importantly, we are at this point omitting the parameter $\sigma^2$. In fact, the MLE for $\hat{\beta}$ is unchanged if we estimate this parameter as well. The MLE for $\hat{\sigma^2}$ is given by
\begin{align*}
\hat{\sigma^2}
&amp;amp; = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 \\&lt;br&gt;
&amp;amp; = \frac{1}{n}y&amp;rsquo;(I-X(X&amp;rsquo;X)^{-1}X&amp;rsquo;)y.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;unbiasedness-&#34;&gt;Unbiasedness&lt;/h3&gt;
&lt;p&gt;It is important to remember that estimators (such as $\hat{\beta}) are themselves &lt;em&gt;random&lt;/em&gt;. If we were to simulate from our model, holding $\beta$ fixed, we would fit a different estimator $\hat{\beta}$ to every simulation. Thus an important quality that an estimator can have is &lt;em&gt;unbiasedness&lt;/em&gt;. This means that, if the model is correct, the estimator will neither tend to overestimate nor underestimate the true parameter. In mathematical terms, its expectation (or mean) is correct: $E[\hat{\beta}] = \beta$.&lt;/p&gt;
&lt;p&gt;Using our matrix math this property can be derived fairly quickly. We know that $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$. We also know that $y = X\beta + \epsilon$. Putting these together we see:
\begin{align*}
\hat{\beta}
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;(X\beta + \epsilon) \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;X\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon \\&lt;br&gt;
&amp;amp; = \beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon.
\end{align*}
Therefore the mean is
\begin{align*}
E[\hat{\beta}]
&amp;amp; = E\left[\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right].
\end{align*}
The mean has a useful property which we will use here, which is that it is _linear_. This means that the expectation of a sum is the sum of the expectations. In math we write this as $E[A+B] = E[A] + E[B]$. Since matrix operations are essentially just a bunch of sums, we can also write $E[Mv] = ME[v]$, if $v$ is random and $M$ is constant. Using this property we get
\begin{align*}
E[\hat{\beta}]
&amp;amp; = E\left[\beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right] \\&lt;br&gt;
&amp;amp; = \beta + E\left[(X&amp;rsquo;X)^{-1}X&amp;rsquo;\epsilon\right] \\&lt;br&gt;
&amp;amp; = \beta + (X&amp;rsquo;X)^{-1}X&amp;rsquo;E[\epsilon] \\&lt;br&gt;
&amp;amp; = \beta
\end{align*}
since we have assumed that $\epsilon$ is zero-mean noise. So, in fact, our estimator is unbiased!&lt;/p&gt;
&lt;h3 id=&#34;covariance-of-hatbeta-&#34;&gt;Covariance of $\hat{\beta}$&lt;/h3&gt;
&lt;p&gt;Remember that $\hat{\beta}$ is fundamentally a &lt;em&gt;random&lt;/em&gt; quantity. It is different in every realization (or simulation) of the statistical model we have written down. It is sensitive to the addition of random noise ($\epsilon_{i}$) to the data. Luckily, since we have written down a statistical model for our data $y$ we are able to do a statistical analysis for the estimator $\hat{\beta}$ to determine its random properties.&lt;/p&gt;
&lt;p&gt;We showed in the previous section that $\hat{\beta}$ was unbiased, that is, it has its mean $E[\hat{\beta}] = \beta$ at the correct place. We will now calculate the variance-covariance (or just covariance) matrix of $\hat{\beta}$. This tells us how much we can expect $\hat{\beta}$ to vary for different datasets. The &lt;em&gt;diagonal&lt;/em&gt; of this covariance matrix tells us the variances of $\hat{\beta}_{i}$, which are important quantities that get used, for example, in calculating Rs model summaries.&lt;/p&gt;
&lt;p&gt;The covariance of two random variables is given by $E[(A-E[A])(B-E[B])]$. In our case $A = \hat{\beta}_{i}$ and $B = \hat{\beta}_{j}$. This will give us the entry $\Sigma_{ij}$ in the covariance matrix. We already know that $E[\hat{\beta}_{i}] = \beta_{i}$ since the estimators are unbiased. Therefore the covariance is
$$
\Sigma_{ij} = E\left[(\hat{\beta}_{i} - \beta_{i})(\hat{\beta}_{j} - \beta_{j})\right].
$$
Expanding the quadratic in the middle, and remembering that expectations are linear, we get that
\begin{align*}
\Sigma_{ij}
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - E[\beta_i\hat{\beta}_j] - E[\hat{\beta}_i\beta_j] + E[\beta_i\beta_j] \\&lt;br&gt;
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - \beta_iE[\hat{\beta}_j] - E[\hat{\beta}_i]\beta_j + \beta_i\beta_j \\&lt;br&gt;
&amp;amp; = E[\hat{\beta}_i\hat{\beta}_j] - \beta_i\beta_j.
\end{align*}&lt;/p&gt;
&lt;p&gt;This is a useful formula, but it would be far more effective to analyze this problem in terms of matrix notation. Let&amp;rsquo;s go ahead and make that switch. Using matrix notation, the single entry
$$
\Sigma_{ij} = E\left[(\hat{\beta}_i - \beta_i)(\hat{\beta}_j - \beta_j)\right].
$$
can be written as a whole matrix
$$
\Sigma = E\left[(\hat{\beta} - E[\hat{\beta}])(\hat{\beta} - E[\hat{\beta}])&#39;\right].
$$
Since we know that the estimator is unbiased we can plug this into the matrix equation to get
\begin{align*}
\Sigma
&amp;amp; = E\left[(\hat{\beta} - \beta)(\hat{\beta}-\beta)&#39;\right] \\&lt;br&gt;
&amp;amp; = E\left[\hat{\beta}\hat{\beta}&#39;\right] - \beta\beta&amp;rsquo;.
\end{align*}
This is exactly the same formula we have above, just written in matrix notation. Now, we already have a formula for $\hat{\beta}$ in matrix notation, $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$, so let&amp;rsquo;s plug this in here. We get
\begin{align*}
\Sigma
&amp;amp; = E\left[(X&amp;rsquo;X)^{-1}X&amp;rsquo;yy&amp;rsquo;X(X&amp;rsquo;X)^{-1}\right] - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo; E[yy&amp;rsquo;] X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo;.
\end{align*}
Now we can plug in our formula for $y = X\beta + \epsilon$. Remembering that expectations are linear (i.e. can be split up over summations), we get
\begin{align*}
E[yy&amp;rsquo;]
&amp;amp; = E\left[(X\beta + \epsilon)(X\beta+\epsilon)&#39;\right] \\&lt;br&gt;
&amp;amp; = E\left[X\beta\beta&amp;rsquo;X&amp;rsquo; + \epsilon X\beta + \beta&amp;rsquo;X&amp;rsquo;\epsilon + \epsilon\epsilon&amp;rsquo;\right] \\&lt;br&gt;
&amp;amp; = X\beta\beta&amp;rsquo;X&amp;rsquo; + E[\epsilon]X\beta + \beta&amp;rsquo;X&amp;rsquo;E[\epsilon] + E[\epsilon\epsilon&amp;rsquo;]
\end{align*}
Now we already know that $E[\epsilon] = 0$. What about $E[\epsilon\epsilon&amp;rsquo;]$? Well,
$$
\mbox{cov}(\epsilon_i,\epsilon_j) = E[\epsilon_i\epsilon_j] - E[\epsilon_i]E[\epsilon_j] = E[\epsilon_i\epsilon_j].
$$
Since independent variables are uncorrelated (and we know that the $\epsilon_i$ are iid) we see that this matrix is diagonal with entries equal to $\sigma^2$, the variance of $\epsilon_i$. Thus:
$$
E[yy&amp;rsquo;] = X\beta\beta&amp;rsquo;X&amp;rsquo; + \sigma^2I.
$$
Finally, plugging this in, we can find that
\begin{align*}
\Sigma &amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo; E[yy&amp;rsquo;] X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}X&amp;rsquo;(X\beta\beta&amp;rsquo;X&amp;rsquo; + \sigma^2I)X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = (X&amp;rsquo;X)^{-1}(X&amp;rsquo;X)\beta\beta&amp;rsquo;(X&amp;rsquo;X)(X&amp;rsquo;X)^{-1} + (X&amp;rsquo;X)^{-1}X&amp;rsquo;(\sigma^2I)X(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = \beta\beta&amp;rsquo; + \sigma^2(X&amp;rsquo;X)^{-1} - \beta\beta&amp;rsquo; \\&lt;br&gt;
&amp;amp; = \sigma^2(X&amp;rsquo;X)^{-1}.
\end{align*}
This is the covariance matrix of $\hat{\beta}$! Notably, this has two important properties. One, it depends on $\sigma^2$, so if we want to use this we had better estimate $\sigma^2$ somehow. More on this in the next section. Two, it depends on the _inverse_ of $X&amp;rsquo;X$. There&amp;rsquo;s no guarantee that this matrix is invertible. For example, if we have more predictors than we have data points (i.e. $k &amp;gt; n$) this matrix will certainly _not_ be invertible. You may have been warned about this scenario in the past. However, even if you have many data points, other situations can crop up where $X&amp;rsquo;X$ is either numerically difficult to invert (i.e. difficult to calculate on a computer), or produces very large variances. One such case is where two of the predictor variables are very highly correlated. More on this later.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression Confidence Intervals</title>
      <link>/courses/qsci483/linear-regression/confidence-intervals/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/confidence-intervals/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to derive and analyze the model diagnostics for a simple linear regression model. Make sure to check out my previous posts on 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 and 
&lt;a href=&#34;/courses/qsci483/linear-regression/linear-regression&#34;&gt;linear regression&lt;/a&gt;
 before diving in.&lt;/p&gt;
&lt;p&gt;Confidence intervals
Simultaneous confidence intervals&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression Model Diagnostics</title>
      <link>/courses/qsci483/linear-regression/math-diagnostics/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/math-diagnostics/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to derive and analyze the model diagnostics for a linear regression model. Make sure to check out my previous posts before diving in.&lt;/p&gt;
&lt;h2 id=&#34;the-model-&#34;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Recall that linear regression is based upon the equation:
$$
y_i \sim N(X\beta,\sigma^2)
$$
or equivalently
\begin{align*}
y_i &amp;amp; \sim X\beta + \epsilon_i \\&lt;br&gt;
\epsilon_i &amp;amp; \sim N(0,\sigma^2)
\end{align*}
where the $\epsilon_i$ are iid. We have previously derived the estimators $\hat{\beta} = (X&amp;rsquo;X)^{-1}X&amp;rsquo;y$, as well as for $\mbox{cov}(\hat{\beta}) = \sigma^2(X&amp;rsquo;X)^{-1}$.&lt;/p&gt;
&lt;h3 id=&#34;the-t-statistic-&#34;&gt;The $t$ statistic&lt;/h3&gt;
&lt;p&gt;The most popular statistic used for linear regression is $t_i = \hat{\beta}_i/\hat{\sigma}(\hat{\beta}_i)}$. Since we have shown that $\hat{\beta}$ is distributed as a multivariate normal random vector, $\hat{\beta}_i$ is distributed as a normal random variable. Furthermore, $\hat{\sigma}(\hat{\beta}&lt;em&gt;i)$, the standard error for this estimate, is a product of the standard error $\hat{\sigma}(\epsilon)$ and the analytical standard deviation of $\beta_i$ (for $\sigma = 1$) which is just the $\sqrt{(X&amp;rsquo;X)^{-1}&lt;/em&gt;{ii}}$. The important thing is we have a normal variable and its standard error, and therefore this statistic is distributed as a $t$ random variable with $(n-k)$ degrees of freedom (recall $n$ is the number of data points and $k$ is the number of predictors).&lt;/p&gt;
&lt;h3 id=&#34;the-f-statistic-&#34;&gt;The $F$ statistic&lt;/h3&gt;
&lt;p&gt;The $F$ test for the overall model is a formal test with hypothesis:
\begin{align*}
H_0: &amp;amp; \beta_i = 0 \mbox{ for all } i \\&lt;br&gt;
H_1: &amp;amp; \beta_i \neq 0 \mbox{ for some }i.
\end{align*}
Notably we will be assuming that there _is_ still a non-zero constant mean term even under $H_0$. If we do not assume this then we can basically set all the $\overline{y}$ terms equal to zero and remove the 1 degree of freedom from the null hypothesis terms. To test this we need to calculate a few important quantities, all of which are sums of squares:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE (sum of squares: error)
&lt;ul&gt;
&lt;li&gt;This is basically just the sum of squares for all the residuals&lt;/li&gt;
&lt;li&gt;SSE$ = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \epsilon_i^2$&lt;/li&gt;
&lt;li&gt;We showed 
&lt;a href=&#34;../standard-error&#34;&gt;previously&lt;/a&gt;
 that SSE$\sim \chi^2_{n-k}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SSM (sum of squares: model)
&lt;ul&gt;
&lt;li&gt;This is basically how much extra variation from the mean the model explains&lt;/li&gt;
&lt;li&gt;SSM$ = \sum_{i=1}^n (\hat{y}_i - \overline{y})^2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SST (sum of squares: total)
&lt;ul&gt;
&lt;li&gt;This is how much total variation this is around the mean&lt;/li&gt;
&lt;li&gt;SST$ = \sum_{i=1}^n (y_i-\overline{y})^2
Remarkably SSE+SSM=SST. This is the famous variance decomposition for ANOVA models. Why is this true?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;variance-decomposition-&#34;&gt;Variance Decomposition&lt;/h4&gt;
&lt;p&gt;We can think about the variance decomposition several ways. The simplest proof is geometric in nature but requires jumping through some linear-subspace hoops. We can think about three points in $\mathbb{R}^n$, $n-dimensional space where our datapoints $y$ live. One point is $y_i = (y_i)$. Another point is $\hat{y}_i = (\hat{y}_i)$ a third point is $\overline{y}&lt;em&gt;i = (\overline{y})$. We can also define $k$ vectors in this space, corresponding to the values of our predictors $x^{(j)}&lt;em&gt;i = X&lt;/em&gt;{ij}$. These $k$ vectors define a $k$-dimensional linear subspace of $\mathbb{R}^n$ (which you can think of as $X\beta$ for all of the possible $k$-dimensional values of $\beta$. The prediction vector $\hat{y}$ is nothing but the projection of $y$ onto the closest point of this subspace. Since one of the predictors is the mean (i.e. $X&lt;/em&gt;{i1} = 1$) the mean point $\hat{y}$ falls &lt;em&gt;inside&lt;/em&gt; this linear subspace. This gives us the following geometric intuition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean is in the subspace&lt;/li&gt;
&lt;li&gt;The projection is in the subspace&lt;/li&gt;
&lt;li&gt;The projection is the &lt;em&gt;closest&lt;/em&gt; point in the subspace to the data&lt;/li&gt;
&lt;li&gt;Therefore the vector leading from the data to the projection is &lt;em&gt;orthogonal&lt;/em&gt; or perpendicular to the subspace&lt;/li&gt;
&lt;li&gt;In particular the vector leading from the data to the projection is orthogonal to the vector leading from the projection to the mean
Therefore we can apply the Pythagorean theorem, to find that the squared distance between the data and the projection (prediction) plus the squared distance between the projection and the mean is equal to the squared distance between the data and the mean. If you sit down and think about what these &amp;ldquo;squared distance&amp;rdquo; mean in mathematical terms you will find that this is a geometric proof of SSE+SSM=SST.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also think about the variance decomposition in terms of raw linear algebra and matrices: TODO&lt;/p&gt;
&lt;p&gt;A common &lt;em&gt;mistake&lt;/em&gt; made in deriving the variance decomposition is noting that $$(y_i - \hat{y}_i) + (\hat{y}_i - \overline{y}) = (y_i-\overline{y}).$$ The &lt;em&gt;false&lt;/em&gt; argument then goes: square the terms and sum and the equality holds. However, as you likely know $A+B=C$ does &lt;em&gt;not&lt;/em&gt; imply that $A^2+B^2=C^2$. We require the additional structure of linear algebra and/or geometry to obtain this result&lt;/p&gt;
&lt;h4 id=&#34;back-to-the-f-test-&#34;&gt;Back to the $F$ test&lt;/h4&gt;
&lt;p&gt;We can now adjust SSE and SSM to get the &amp;ldquo;mean sum of squares,&amp;rdquo; by dividing by the degrees of freedom:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE = SSE/$(n-k)$&lt;/li&gt;
&lt;li&gt;MSM = SSM/$(n-1)$
Finally, our test statistic is $F = MSM/MSE$. Note that if the model explains a lot more variation than just the mean we expect for MSE to be small and MSM to be large: make sure this make sense to you. So if the model is &amp;ldquo;real&amp;rdquo; then we expect $F$ to be large. If the null hypothesis is true then we expect $F$ to be small. So we reject the null for large values of $F$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiple vs. adjusted $R^2$
More on t-tests
F-test for whole model
Stundentized residuals ($t$ vs. standardized $N(0,1)$)
DFFITS
Cook&amp;rsquo;s distance ($F$-distribution)
Skewness
Kurtosis
Leverage
ANOVA
Shapiro-wilks test
ncV test&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Model Diagnostics</title>
      <link>/courses/qsci483/linear-regression/diagnostics/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/qsci483/linear-regression/diagnostics/</guid>
      <description>&lt;p&gt;In this document I will outline the math used to derive and analyze the model diagnostics for a simple linear regression model. Make sure to check out my previous posts on 
&lt;a href=&#34;/courses/qsci483/notation&#34;&gt;notation&lt;/a&gt;
 and 
&lt;a href=&#34;/courses/qsci483/linear-regression/linear-regression&#34;&gt;linear regression&lt;/a&gt;
 before diving in.&lt;/p&gt;
&lt;p&gt;Plotting fitted vs residuals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;should see no trend/pattern&lt;/li&gt;
&lt;li&gt;centered on zero&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plot histogram of residuals&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;basically centered on zero?&lt;/li&gt;
&lt;li&gt;skewed/kurtosis?&lt;/li&gt;
&lt;li&gt;Shapiro-Wilks test (can be very sensitive to small deviations with large datasets)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QQ plot&lt;/p&gt;
&lt;p&gt;DFFITS
Deleted residuals
Cook&amp;rsquo;s distance and leverage&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Knowledge an Obstacle to Teaching?</title>
      <link>/post/teaching/visible-learning/ch-02/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-02/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Experts in a subject matter area seem to have worse intuition for teaching that subject matter, compared to novices
&lt;ul&gt;
&lt;li&gt;In theory this is because they have learned to think about it in a structured and organized way that is not the most effective way to &lt;em&gt;learn&lt;/em&gt; that material&lt;/li&gt;
&lt;li&gt;Experts consistently underestimated how difficult a task is. Novices, having just learned it, know how difficult it is to learn and teach a more basic level&lt;/li&gt;
&lt;li&gt;Although students rated novices higher and performed better on a post-teaching assessment, the expert-taught students performed better on transferring their learning to a related task&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Students appreciate being taught by knowledgeable, motivated, passionate individuals
&lt;ul&gt;
&lt;li&gt;Students will rate their best teachers highly on competency, credibility, and fariness&lt;/li&gt;
&lt;li&gt;More closely related to student motivation than actual learning. However children learn less from adults they view as ignorant&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge about subject matter may make it harder to teach in a group setting, but on an individual level enables you to provide helpful feedback, and contextualize a student&amp;rsquo;s ideas and progress against your own knowledge base.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/1</title>
      <link>/post/notebook/2020/spring/week2/thurs/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look at QSCI stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computer lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 4/1</title>
      <link>/post/notebook/2020/spring/week2/wedn/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take notes from Visible Learning Chapter 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attend class and continue looking at QSCI HW 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Wrote up some 
&lt;a href=&#34;/courses/gaussian-processes&#34;&gt;resources&lt;/a&gt;
 so that I can stop rederiving/looking up common GP/MVN properties that I need to use. Also included on there the math for Dirichlet BCs. Now that the derivation is there I can more effectively write the code for the implementation, rather than writing stuff down on paper every time I try to do this.&lt;/p&gt;
&lt;p&gt;Forgot to account in my scheduling today for time to edit and submit my dissertation proposal. Did that.&lt;/p&gt;
&lt;p&gt;Wrote up notes for VL Ch 2, and continued to expand on my own 
&lt;a href=&#34;/courses/qsci-483&#34;&gt;notes&lt;/a&gt;
 for QSCI, reminding myself how all the 
&lt;a href=&#34;/courses/qsci-483/linear-regression/diagnostics&#34;&gt;model diagnostics&lt;/a&gt;
 work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/31</title>
      <link>/post/notebook/2020/spring/week2/tues/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look over the rest of QSCI HW 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take notes from Visible Learning Chapter 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1:30-3:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hold office hours&lt;/li&gt;
&lt;li&gt;When no one shows up finish up looking at QSCI HW 1&lt;/li&gt;
&lt;li&gt;Then read Visible Learning Chapter 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Didn&amp;rsquo;t make it through the whole thing, had to look up some of the model diagnostics Tim is using, but did get through VL Ch 1 and 2. Struggled with website formatting in the afternoon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-03/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-03/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-04/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-04/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-05/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-06/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-06/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-07/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-07/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-08/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-09/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-09/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-10/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-11/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-12/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-13/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-14/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-15/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-16/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-17/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-18/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-19/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-20/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-21/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-22/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-23/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-24/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-24/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-25/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-25/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-26/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-26/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-27/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-27/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-28/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-28/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-29/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-29/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-30/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-30/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/teaching/visible-learning/ch-31/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-31/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Broad Strokes of Learning</title>
      <link>/post/teaching/visible-learning/core-takeaways/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/core-takeaways/</guid>
      <description>&lt;h2 id=&#34;nine-basic-principles-&#34;&gt;Nine basic principles&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Explanations of human learning in terms of innate talent are consistently undermined by research which states that substantial amounts of time, energy, instruction, and effort are required to develop mastery of any subject area.&lt;/li&gt;
&lt;li&gt;We can naturally learn from any information, but to learn effectively that information has to be organized in a way that matches how our minds are organized. Minds are organized differently for different people, and even change as we age.&lt;/li&gt;
&lt;li&gt;The brain has severe, inherent limitations and deep processing becomes impossible when those limits are reached (cognitive load principle)&lt;/li&gt;
&lt;li&gt;People learn particularly well from other people, through directed instruction and feedback (social learning theory)&lt;/li&gt;
&lt;li&gt;People put in a lot more effort when they are confident that worthwhile goals are achievable in the short term. Activating effort and motivation is difficult but not impossible.&lt;/li&gt;
&lt;li&gt;Short-term goals are highly motivating, but when they conflict with valuable long-term goals we must develop and use strategies to control impulses and delay gratificatoin (personal regulation through self-control).&lt;/li&gt;
&lt;li&gt;Learners are humans, and other parts of them &amp;ndash; self esteem, sociality, etc &amp;ndash; must be maintained and acknowledged during the learning process.&lt;/li&gt;
&lt;li&gt;Humans are fundamentally social, down to the neurological level. This sociality can be used as a tool (social brain hypothesis)&lt;/li&gt;
&lt;li&gt;Ideas about learning that are contradicted by scientific evidence abound. Many of these ideas can be harmful.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;section-1-&#34;&gt;Section 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The mind may not be &amp;ldquo;designed for thinking&amp;rdquo; and requires time and effort to think&lt;/li&gt;
&lt;li&gt;Teachers need to recognize how difficult tasks are for beginners. Focus should be not on material but on the actual process of moving from not knowing to knowing. To do this students need a safe environment to acknowledge they don&#39;t understand. We can only holld so much in our cognitive centers, so it is important to ``overlearn&amp;rsquo;&amp;rsquo; basic concepts until it is ingrained and automatic&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chapter-1-&#34;&gt;Chapter 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Students do not dislike school, but tolerate it.&lt;/li&gt;
&lt;li&gt;The brain may not be &amp;ldquo;designed&amp;rdquo; for abstract thinking and as a result school is a taxing process.&lt;/li&gt;
&lt;li&gt;Thinking requires a large investment of resources for an uncertain outcome (understanding) and risk-averse humans prefer to invest these resources in crossing seemingly achievable &amp;ldquo;knowledge gaps&amp;rdquo; rather than &amp;ldquo;knowledge chasms&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;chapter-2-&#34;&gt;Chapter 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Experts may be &amp;ldquo;worse&amp;rdquo; at teaching than novices. The &amp;ldquo;expert blind spot effect&amp;rdquo; explains this:
&lt;ul&gt;
&lt;li&gt;experts forget how difficult it was to learn something in the first place&lt;/li&gt;
&lt;li&gt;there is a gap between the advanced level on which experts conceptualize of a topic and the short-term learning needs of students&lt;/li&gt;
&lt;li&gt;however, this abstract thinking may help students transfer their learning to related areas.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;section-2-&#34;&gt;Section 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Major ideas about acquisition, memory retention, mental storage, and overload&lt;/li&gt;
&lt;li&gt;Learning need not be conscious, we can only think about so much at once&lt;/li&gt;
&lt;li&gt;We need to develop a vocabulary for learning, and need multiple strategies for learning&lt;/li&gt;
&lt;li&gt;Challenges: learning styles (i.e. spatial, verbal, kinaesthetic), Mozart effects, multitasking, digital natives, and whether the Internet is really changing how we think&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;section-3-&#34;&gt;Section 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Self-esteem follows success more often than predicting it&lt;/li&gt;
&lt;li&gt;Building confidence is still important though: in order to maintain positive views of ourselves we build explanations (I cannot; rather than I did not work hard enough) that help our self esteem but hurt our learning&lt;/li&gt;
&lt;li&gt;Learning situations are often distracting and knowing how to pay attention to learning is important, but tiring. It is important to know when to stop thinking to save cognitive resources.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/30</title>
      <link>/post/notebook/2020/spring/week2/mon/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/mon/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-11:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do QSCI HW and prep for TAing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 12:30-1:30&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Visible Learning Chapter 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3:30-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on Dirichlet BC code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did the first half of the QSCI HW, read Ch 1, but did not end up having time to work on the BC code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 2</title>
      <link>/post/notebook/2020/spring/week2/post-1/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week2/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;First real week of the quarter and I am surprise TAing! I also found some frustrating uncommitted code from my Colorado trip which I need to reimplement, but rewriting code is often a way to make it better anyways&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make all suggested changes to bypass proposal by Wednesday and send to committee&lt;/li&gt;
&lt;li&gt;Read chapters 1 and 2 of Visible Learning by Wednesday/Friday&lt;/li&gt;
&lt;li&gt;Do the QSCI hw ahead of time to make sure I&amp;rsquo;ve got it down by Tuesday&lt;/li&gt;
&lt;li&gt;Rewrite Dirichlet BC code by Friday&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why Don&#39;t Students Like School?</title>
      <link>/post/teaching/visible-learning/ch-01/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/visible-learning/ch-01/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There is no serious evidence that students, on average, actively &lt;em&gt;dislike&lt;/em&gt; school. Rather they are neutral, or mildly positive towards school.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A frequent challenge faced by teachers is the apathy of students on an individual level, after working so hard to provide an engaging learning experience.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VL puts forward the following hypothesis, following researcher Daniel Willingham:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The human brain is not primarily designed for &amp;ldquo;thinking&amp;rdquo;: it is designed for sociality, for language learning, for spatial reasoning, etc. Higher level reasoning is not the primary purpose and is, on average, very difficult.&lt;/li&gt;
&lt;li&gt;This means that thinking uses up limited resources very quickly&lt;/li&gt;
&lt;li&gt;Asking someone to invest this level of effort/resources is a hard task. The outcome of a &amp;ldquo;thinking&amp;rdquo; task could be positive (better understanding) or negative (no progress). Humans are risk-averse and are unlikely to invest effort if they cannot see a likely short-term pay-off.&lt;/li&gt;
&lt;li&gt;This means the best way to get effort out of students is to show them short-term goals that seem achievable: &amp;ldquo;we are motivated by knowledge gaps, but put off by knowledge chasms.&amp;rdquo; We cannot be curious about everything, and are instead curious about the things we feel we can understand with little additional effort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ease with which people can access information has a large effect on how they use that information, and how they feel about that information&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We rely much more heavily on memory than thinking: it is generally easier to remember a previous result than to derive it from scratch&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/27</title>
      <link>/post/notebook/2020/spring/week1/fri/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/fri/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;I might be surprise-TAing next quarter, so I&amp;rsquo;m focusing all my energy on TA prep today. I&amp;rsquo;m going to try to get through the whole UW TA resources page and will be adding notes in 
&lt;a href=&#34;/post/teaching&#34;&gt;posts&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;I did that! I also met with Tim the course instructor to outline my responsibilities as a TA and did some editing on the Dirichlet BC code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Remote Teaching</title>
      <link>/post/teaching/remote-teaching/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/remote-teaching/</guid>
      <description>&lt;p&gt;Online Office Hours:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zoom Pro + Canvas Discussions&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TAing</title>
      <link>/post/teaching/taing/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/taing/</guid>
      <description>&lt;h2 id=&#34;expectations-&#34;&gt;Expectations&lt;/h2&gt;
&lt;p&gt;Understand early on what the professors expectations are, in detail&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Am I expected to attend class?&lt;/li&gt;
&lt;li&gt;Will we hold regular TA meetings?&lt;/li&gt;
&lt;li&gt;Am I grading, writing answer keys, giving guest lectures, holding office hours, holding review sessions, leading discussion sections, responding to online questions, communicating from the instructor to the class, etc?&lt;/li&gt;
&lt;li&gt;Know what the instructor is emphasizing in class&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;demeanor-&#34;&gt;Demeanor&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Assessments</title>
      <link>/post/teaching/assessment/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/assessment/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/assessing-and-improving-teaching/assessing-student-lerning-grading&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Four recommendations from UW:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create clear grading criteria&lt;/li&gt;
&lt;li&gt;Communicate these criteria to students&lt;/li&gt;
&lt;li&gt;Give constructive feedback&lt;/li&gt;
&lt;li&gt;Employ time management strategies when grading large amounts of work&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Expect students to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus their time and effort on the things with the highest grading weights&lt;/li&gt;
&lt;li&gt;be sensitive about their grades, and ask about grading-related topics frequently&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Concrete Actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create, communicate, and remind students about clear grading criteria&lt;/li&gt;
&lt;li&gt;create clear expectations about edge cases like late papers, exam timing, grade changes, and typos on assignments&lt;/li&gt;
&lt;li&gt;keep thorough records of evaluations, and keep for a while after the quarter is over&lt;/li&gt;
&lt;li&gt;promptly document interactions with unhappy students&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See further resources on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/post/teaching/writing-tests&#34;&gt;writing tests&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://cft.vanderbilt.edu/guides-sub-pages/grading-student-work&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;assigning grades&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://teachingcommons.stanford.edu/resources/teaching/evaluating-students&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;evaluating students&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;grading-&#34;&gt;Grading&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/grading-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grading&lt;/a&gt;
 is really hard. First thing to make clear is that I am using mastery-based (i.e. individual skills) rather than norm-referenced (i.e. relative to each other) grading.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be consistent&lt;/li&gt;
&lt;li&gt;Make sure students know what to expect
&lt;ul&gt;
&lt;li&gt;What is being measured, how is it being measured, what does this have to do with the course at large?&lt;/li&gt;
&lt;li&gt;Do students need to recall information, recognize patterns, draw inferences, make connections, construct an argument, or what?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mastery-based grading requires very clear measurable goals and objectives
&lt;ul&gt;
&lt;li&gt;Likely want to make some re-adjustment of pre-set expectations based on actual performance. Perhaps the test really was too hard?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cheating-&#34;&gt;Cheating&lt;/h2&gt;
&lt;p&gt;How to stop it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explain clearly what is and is not allowed&lt;/li&gt;
&lt;li&gt;Encourage students to seek help:
&lt;ul&gt;
&lt;li&gt;Often students cheat because they are doing poorly&lt;/li&gt;
&lt;li&gt;Encourage them to schedule a meeting and/or come to office hours&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Give alternating versions of exams (different orders if not different questions)&lt;/li&gt;
&lt;li&gt;Regularly walk around the room and observe students&lt;/li&gt;
&lt;li&gt;After the exam mark answer sheets in such a way that alterations cannot be made (possibly scan a copy of each exam if possible)&lt;/li&gt;
&lt;li&gt;Give an alternate exam version for make-ups&lt;/li&gt;
&lt;li&gt;Homework assignments are difficult to monitor
&lt;ul&gt;
&lt;li&gt;UW recommends lowering the amount of the grade HW is worth&amp;hellip;I don&amp;rsquo;t know about that&lt;/li&gt;
&lt;li&gt;Replace homework with in-class quizzes?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Final projects
&lt;ul&gt;
&lt;li&gt;Think about whether students may have done similar projects in other classes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What to do when it happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a specific 
&lt;a href=&#34;https://www.washington.edu/cssc/facultystaff/report-academic-misconduct&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reporting tool&lt;/a&gt;
 but for TAs it probably goes through the instructor first&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Classroom Practices</title>
      <link>/post/teaching/classroom-practices/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/classroom-practices/</guid>
      <description>&lt;p&gt;I want to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn everyone&amp;rsquo;s name by week 1&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First day:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduce myself:
&lt;ul&gt;
&lt;li&gt;Name/pronouns, background, formality, how/when to reach me&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Notecards:
&lt;ul&gt;
&lt;li&gt;Preferred name/pronunciation/pronouns&lt;/li&gt;
&lt;li&gt;Accommodation requests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Expectations:
&lt;ul&gt;
&lt;li&gt;grading/deadline criteria&lt;/li&gt;
&lt;li&gt;classroom policies&lt;/li&gt;
&lt;li&gt;my commitments as a teacher&lt;/li&gt;
&lt;li&gt;how I intend to teach/assess&lt;/li&gt;
&lt;li&gt;add codes/course conflicts&lt;/li&gt;
&lt;li&gt;course schedule&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Quick (ungraded) quiz to gauge background? Self-reported pre-assessment of background knowledge? Resources for filling in gaps? ``Common Sense Inventory:&amp;rsquo;&amp;rsquo; determine whether 15 statements related to course content are true or false?&lt;/li&gt;
&lt;li&gt;Homework 0: voluntary/mandatory office hour?&lt;/li&gt;
&lt;li&gt;If there is time left to teach, model what the rest of the course will be like. Think about whether registration has stabilized and whether to teach foundational material, or just interesting stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General Practices&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tie things into what I&amp;rsquo;m excited about. Students can tell if you&amp;rsquo;re into it&lt;/li&gt;
&lt;li&gt;Give real world examples/applications, particularly in my own work&lt;/li&gt;
&lt;li&gt;Keep up a high degree of connectivity, explain how things relate to other things&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.washington.edu/teaching/topics/engaging-students-in-learning/promoting-student-engagement-through-active-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Active learning&lt;/a&gt;
 is good. Give students a chance to practice that thing they just learned&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;office-hours-&#34;&gt;Office Hours&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most important thing is getting people to come in the first place&lt;/li&gt;
&lt;li&gt;Give feedback on what&amp;rsquo;s been done so far and offer direction and general guidance&lt;/li&gt;
&lt;li&gt;Ask them to explain their work and thought process so far
&lt;ul&gt;
&lt;li&gt;If that leaves a blank, ask them to solve an easier problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try to get them to ask specific questions:
&lt;ul&gt;
&lt;li&gt;Is X true?&lt;/li&gt;
&lt;li&gt;Does this specific logic follow?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ask leading questions&lt;/li&gt;
&lt;li&gt;Try to understand what their thought process is, they are likely coming from a different background than me and will think about things in a different way&lt;/li&gt;
&lt;li&gt;Work through the homework and exams ahead of time
&lt;ul&gt;
&lt;li&gt;Try to anticipate sticking points and wrong turns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Know and communicate the instructors expectations and emphasize this material&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Course Design</title>
      <link>/post/teaching/course-design/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/course-design/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://washington.edu/teaching/topics/preparing-to-teach/designing-your-course-and-syllabus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://cft.vanderbilt.edu/guides-sub-pages/understanding-by-design/#resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Backward Design&lt;/a&gt;
:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design a course in reverse by considering
&lt;ul&gt;
&lt;li&gt;What are your learning goals?&lt;/li&gt;
&lt;li&gt;How will you assess those goals?&lt;/li&gt;
&lt;li&gt;How will you achieve those goals through teaching?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In this way everything done in the course is intentional and working towards an established goal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;who-are-my-students-&#34;&gt;Who Are My Students&lt;/h2&gt;
&lt;p&gt;Before I even get to learning goals, also consider: who are my students? Why are they taking my course? What can I expect that they already know? What range of backgrounds might the have? What do I expect them to struggle with?&lt;/p&gt;
&lt;h2 id=&#34;what-are-my-learning-goals-&#34;&gt;What Are My Learning Goals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://teaching.uncc.edu/sites/teaching.uncc.edu/files/media/files/file/GoalsAndObjectives/Bloom.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blooms Taxonomy&lt;/a&gt;
:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A way of classifying increasingly ambitious learning objectives&lt;/li&gt;
&lt;li&gt;Three taxonomies for:
&lt;ul&gt;
&lt;li&gt;Knowledge-based goals&lt;/li&gt;
&lt;li&gt;Skills-based foals&lt;/li&gt;
&lt;li&gt;Affective goals (related to values, attitudes, interests)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-will-i-assess-those-goals&#34;&gt;How Will I Assess Those Goals&lt;/h2&gt;
&lt;p&gt;Both during the course and at the end of the course. Designing 
&lt;a href=&#34;/post/teaching/assessment&#34;&gt;assessments&lt;/a&gt;
 is a whole separate can of worms.&lt;/p&gt;
&lt;h2 id=&#34;how-will-i-achieve-those-goals-&#34;&gt;How Will I Achieve Those Goals&lt;/h2&gt;
&lt;p&gt;Before I get into the nitty-gritty of lesson planning, etc, now is also a good time to write a 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/designing-your-course-and-syllabus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syllabus&lt;/a&gt;
. Once I&amp;rsquo;ve done that I think I personally will benefit from calendaring. This will help me remember little things like: two weeks from today is the exam, I need to write it so I can tell students about format, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/26</title>
      <link>/post/notebook/2020/spring/week1/thurs/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/thurs/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fix Anaconda errors and start animal movement MCMC running overnight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start reading UW TA resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Think more about second-order inclusion probabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Fixed Anaconda error, then discovered issue where some code was not pushed to GitHub. Unable to find the original code (mostly my implementation of Dirichlet BCs), but re-writing it should be a useful (if annyoing) exercise.&lt;/p&gt;
&lt;p&gt;Started reading UW resources and made several 
&lt;a href=&#34;/post/teaching&#34;&gt;posts&lt;/a&gt;
. Not all of it is relevant just yet, but I&amp;rsquo;d like to get through all of it and start thinking about everything together.&lt;/p&gt;
&lt;p&gt;Found 
&lt;a href=&#34;https://www.jstor.org/stable/23357227&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;
 about random sampling with given second-order inclusion probabilities. It gives results about which inclusion probability matrices (IPMs) are valid (the answer is complicated) and constructed a maximum-entropy distribution for fixed-size sampling with fixed IPM.&lt;/p&gt;
&lt;p&gt;Thoughts on IPMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Want to incorporate logistics somehow&lt;/li&gt;
&lt;li&gt;Can we estimate IPMs after filtering for feasible designs?&lt;/li&gt;
&lt;li&gt;Can we calculate IPMs after filtering for feasible designs?&lt;/li&gt;
&lt;li&gt;Can we calculate probability of infeasibility for CP(2) designs?&lt;/li&gt;
&lt;li&gt;How do CP(2) designs work in space-time?&lt;/li&gt;
&lt;li&gt;I feel like there&amp;rsquo;s something in here somewhere but it&amp;rsquo;s buried deep&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tests</title>
      <link>/post/teaching/writing-tests/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/teaching/writing-tests/</guid>
      <description>&lt;p&gt;Notes from UWs TA 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/preparing-to-teach/constructing-tests&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why am I testing:
&lt;ul&gt;
&lt;li&gt;to monitor progress and adjust the pace of the course?&lt;/li&gt;
&lt;li&gt;to motivate students&lt;/li&gt;
&lt;li&gt;to provide data for a grade?&lt;/li&gt;
&lt;li&gt;to challenge students to apply concepts?&lt;/li&gt;
&lt;li&gt;Use this information to design the exam&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is my test consistent with my teaching:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;post/teaching/course-design&#34;&gt;Backwards design&lt;/a&gt;
 will help with this&lt;/li&gt;
&lt;li&gt;If the test emphasizes analysis and synthesis, make sure class time does as well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do the students know what to expect?
&lt;ul&gt;
&lt;li&gt;Does the test match my stated course goals&lt;/li&gt;
&lt;li&gt;Have I reviewed the material on the test/explained what will be tested?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does my exam test a range of learning?
&lt;ul&gt;
&lt;li&gt;Do students who have not mastered everything have room to demonstrate growth?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiple choice exams:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to grade&lt;/li&gt;
&lt;li&gt;Good for testing recall and facts, difficult to test analysis&lt;/li&gt;
&lt;li&gt;Make sure question is clear without reading the answers
&lt;ul&gt;
&lt;li&gt;Can a prepared student or colleague answer the question as a free response?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Avoid making the right answer stand out for the wrong reasons (i.e. grammer, length)&lt;/li&gt;
&lt;li&gt;Assess the exam questions afterwards:
&lt;ul&gt;
&lt;li&gt;Which questions were most difficult?&lt;/li&gt;
&lt;li&gt;Were there questions which most students with high-grades missed?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardless of exam format, assessing it after the fact is important:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Did I test what I thought I was testing?&lt;/li&gt;
&lt;li&gt;Did I test what I tught?&lt;/li&gt;
&lt;li&gt;Did I test what I emphasized?&lt;/li&gt;
&lt;li&gt;Did I test what I really wanted the students to learn?&lt;/li&gt;
&lt;li&gt;If answers to these questions come back no, possibly go back to the Backwards Design and rethink course/assessment structure.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/25</title>
      <link>/post/notebook/2020/spring/week1/wedn/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/wedn/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make schedule for reading about teaching, and begin browsing UW resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make edits from yesterdays read-through of bypass proposal and look for last couple references&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Schedule:
The UW resource page has 20 sections. The CMU page has 16 sections, and the Visual Learning book has 31 chapters. I plan on reading roughly 1 section, 1 section, and 2 chapters each week, starting next week.&lt;/p&gt;
&lt;p&gt;Proposal:
Completed and sent to Andrew for feedback.&lt;/p&gt;
&lt;p&gt;Animal Movement:
Looked over the code, tried to re-run it. Got an Anaconda version conflict that I don&amp;rsquo;t remember. Will plan on dealing with that tomorrow.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook Structure</title>
      <link>/post/notebook/outline/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/outline/</guid>
      <description>&lt;p&gt;In this post I will outline the structure of my virtual lab notebook, for use during this time of woe. This is necessitated by the current COVID-19 work-from-home crisis, but will be helpful in general as a tool for accountability and productivity.&lt;/p&gt;
&lt;h2 id=&#34;post-frequency-&#34;&gt;Post Frequency&lt;/h2&gt;
&lt;p&gt;I will post to the lab notebook at the beginning and end of each year, quarter, and week. I will also post to the lab notebook once per day.&lt;/p&gt;
&lt;h2 id=&#34;post-content-&#34;&gt;Post Content&lt;/h2&gt;
&lt;p&gt;In each multi-day post I will outline my goals for the current time period, along with a self-imposed due date, and (particularly for yearly and quarterly posts) a rough outline of intermediate steps. Then at the end of the period I will summarize my progress toward those goals as well as any major progress in addition to those goals that I undertook in that time.&lt;/p&gt;
&lt;p&gt;In each daily post I will outline what my goals were for the day, along with what I actually spent my time on. I will post mathematical and scientific outputs such as documents and plots that I produced that day, and reflect on my productivity and time management.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 2020</title>
      <link>/post/notebook/2020/annualpost-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/annualpost-1/</guid>
      <description>&lt;p&gt;I am just starting this lab notebook now in March 2020, so almost a third of the year is behind me already. That said, one of my big goals for the year, submitting my 
&lt;a href=&#34;content/project/dam-passage-time&#34;&gt;dam passage&lt;/a&gt;
 paper to Proc B, I have already accomplished! Hooray! Hopefully I can continue the momentum now that COVID-19 has well and truly set in.&lt;/p&gt;
&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;I have a tendency to be overly ambitious with my goal timelines. I think I&amp;rsquo;m okay with that for now, but I&amp;rsquo;m trying to temper my expectations a little bit. I&amp;rsquo;m hopeful that with my dam passage project behind me I can focus my time and make more steady progress on my other projects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete my bypass to the PhD track by April 1
&lt;ul&gt;
&lt;li&gt;This is a pretty near-term goal but has been a long time coming&lt;/li&gt;
&lt;li&gt;At this point most of what I need is just to do a bunch of lit reviewing and write up a beefier dissertation proposal than I had the first time round&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Submit my source inference paper by August 1 2020
&lt;ul&gt;
&lt;li&gt;Hopefully the timeline on this is not too ambitious. I have a lot of the theory and code done already, but case studies, editting, and background reading take forever.&lt;/li&gt;
&lt;li&gt;Sub goal: Write up animal movement case study by April 10&lt;/li&gt;
&lt;li&gt;Sub goal: Set up model and begin data analysis for pollen case study by April 17&lt;/li&gt;
&lt;li&gt;Sub goal: Write up pollen case study by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete third case study by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete all background reading by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete final editing by July 3&lt;/li&gt;
&lt;li&gt;This leaves one month of wiggle room for things to take longer than I hope&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prepare to TA by September 15
&lt;ul&gt;
&lt;li&gt;This fall will be my first time TAing in any capacity since I was an undergrad. I will have more responsibility now than I did then and I want to make sure I am prepared. To do this I plan on doing background reading on teaching and TAing ahead of time. I know the best experience comes from doing but this is all I can do for now.&lt;/li&gt;
&lt;li&gt;Sub goal: Browse UW TAing 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Browse CMU CollectedWisdom TAing 
&lt;a href=&#34;https://www.cmu.edu/teaching/resources/PublicationsArchives/CollectedWisdom/collectwisdom-teachingstrategies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resource&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Read some of the evidence-based 
&lt;a href=&#34;https://visible-learning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visible Learning&lt;/a&gt;
 resources&lt;/li&gt;
&lt;li&gt;I haven&amp;rsquo;t assigned sub-goal deadlines because these are resources I intend to digest slowly over the period. Instead my goal is to do a little of this every week.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Submit my spatial survey design paper by Jan 1
&lt;ul&gt;
&lt;li&gt;I feel pretty good about getting the other goals done, if not on time, at least this year. This one is the stretch-iest of all of them. But I certainly won&amp;rsquo;t do it if I give up on it now!&lt;/li&gt;
&lt;li&gt;Sub goal: Find good spatial dataset by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete background reading by July 3&lt;/li&gt;
&lt;li&gt;Sub goal: Complete simulated case study by September 18&lt;/li&gt;
&lt;li&gt;Sub goal: Complete final edits by December 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: 3/24</title>
      <link>/post/notebook/2020/spring/week1/tues/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/tues/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add references for optimal design chapter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 3-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Morning: 9-12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 1-4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add references for optimal design chapter
&lt;ul&gt;
&lt;li&gt;including major reference to Lee 1998, which actually does do logistically constrained optimal design but with a different objective function&lt;/li&gt;
&lt;li&gt;propose to compare performance of my method with his, as well as to speed up his method with sparse matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afternoon: 4-5&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work on new idea for logistically-constrained design
&lt;ul&gt;
&lt;li&gt;logistically-constrained random sampling?&lt;/li&gt;
&lt;li&gt;a number of possibilities for how to achieve it&lt;/li&gt;
&lt;li&gt;all rely upon being able to calculate or estimate $\pi_i$ and $\pi_{ij}$ the first- and second-order inclusion probabilities&lt;/li&gt;
&lt;li&gt;if we want to estimate them, can we define a Gaussian-like spatial process over second-order inclusion probabilities&lt;/li&gt;
&lt;li&gt;how can we do that? what is the domain of valid second-order inclusion probability matrices?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Sp20 Week 1</title>
      <link>/post/notebook/2020/spring/week1/post-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/week1/post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Part of the week has already gone by, but I&amp;rsquo;ll go ahead and fill in some meta goals anyway&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up virtual lab notebook and COVID-19 accountability structure by Tuesday&lt;/li&gt;
&lt;li&gt;Make schedule for reading about teaching and browse UW 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
 by Wednesday&lt;/li&gt;
&lt;li&gt;Refamiliarize myself with animal movement case study By Thursday&lt;/li&gt;
&lt;li&gt;Complete all references for optimal design chapter in bypass proposal by Friday&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reality-&#34;&gt;Reality&lt;/h2&gt;
&lt;p&gt;Did all these things! Hooray!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Notebook: Spring 2020</title>
      <link>/post/notebook/2020/spring/spring-post-1/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/notebook/2020/spring/spring-post-1/</guid>
      <description>&lt;h2 id=&#34;goals-&#34;&gt;Goals&lt;/h2&gt;
&lt;p&gt;Much of this is copied and pasted from my goals for the year:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete my bypass to the PhD track by April 1&lt;/li&gt;
&lt;li&gt;Make progress on source inference
&lt;ul&gt;
&lt;li&gt;Sub goal: Write up animal movement case study by April 10&lt;/li&gt;
&lt;li&gt;Sub goal: Set up model and begin data analysis for pollen case study by April 17&lt;/li&gt;
&lt;li&gt;Sub goal: Write up pollen case study by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete third case study by June 5&lt;/li&gt;
&lt;li&gt;Sub goal: Complete all background reading by June 5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prepare to TA
&lt;ul&gt;
&lt;li&gt;My real goal here is to do just a little bit of reading, every single week&lt;/li&gt;
&lt;li&gt;Sub goal: Browse UW TAing 
&lt;a href=&#34;https://www.washington.edu/teaching/topics/just-for-tas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resources&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Browse CMU CollectedWisdom TAing 
&lt;a href=&#34;https://www.cmu.edu/teaching/resources/PublicationsArchives/CollectedWisdom/collectwisdom-teachingstrategies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;resource&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Sub goal: Read some of the evidence-based 
&lt;a href=&#34;https://visible-learning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visible Learning&lt;/a&gt;
 resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Make progress on optimal survey design
&lt;ul&gt;
&lt;li&gt;Sub goal: Find good spatial dataset by May 1&lt;/li&gt;
&lt;li&gt;Sub goal: Complete half of background reading by June 15&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Source Reconstruction for Linear PDEs</title>
      <link>/talk/spacetime-feb-2020/</link>
      <pubDate>Wed, 12 Feb 2020 13:30:00 +0000</pubDate>
      <guid>/talk/spacetime-feb-2020/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt;
 feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;
 | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;
: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/spacetime-feb-2020/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/spacetime-feb-2020/</guid>
      <description>&lt;h1 id=&#34;the-spde-method-for-source-inference&#34;&gt;The SPDE Method for Source Inference&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pollution
&lt;ul&gt;
&lt;li&gt;Say, PCBs in the Duwamish&lt;/li&gt;
&lt;li&gt;Where did it come from?&lt;/li&gt;
&lt;li&gt;Easy case: point source
&lt;ul&gt;
&lt;li&gt;Measure all pipe outlets&lt;/li&gt;
&lt;li&gt;Use regularization method for unknown source&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hard case: non-point source
&lt;ul&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;possible-approaches&#34;&gt;Possible Approaches&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Basically this is the advection-diffusion equation
&lt;ul&gt;
&lt;li&gt;Possibly with linear decay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Existing methods
&lt;ul&gt;
&lt;li&gt;FFT/Kalman filter (Sigrist et. al 2014)
&lt;ul&gt;
&lt;li&gt;complex method&lt;/li&gt;
&lt;li&gt;non-local basis functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;finite-difference/element method (Stroud et. al 2010)
&lt;ul&gt;
&lt;li&gt;comparatively simple, very popular&lt;/li&gt;
&lt;li&gt;local basis functions&lt;/li&gt;
&lt;li&gt;possibly slower in some contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian functional regression (Nguyen and Peraire 2015)
&lt;ul&gt;
&lt;li&gt;also complex, relatively unknown&lt;/li&gt;
&lt;li&gt;basically a fancy &amp;ldquo;kernel trick&amp;rdquo; method&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-math&#34;&gt;The Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our model is $\mathcal{L}u = f$&lt;/li&gt;
&lt;li&gt;Assume $\mathcal{L}$ is a linear operator&lt;/li&gt;
&lt;li&gt;Then we can discretize this as $Lu = f$&lt;/li&gt;
&lt;li&gt;We model $f\sim X\beta + Z\gamma + \epsilon$&lt;/li&gt;
&lt;li&gt;If everything is normal $[f|d] \sim \mbox{MVN}(\mu_{\rm post},Q_{\rm post}^{-1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;proscons&#34;&gt;Pros/Cons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fast, lots of sparse matrices&lt;/li&gt;
&lt;li&gt;Flexible modeling on $f$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires known physics&lt;/li&gt;
&lt;li&gt;Requires linear PDE&lt;/li&gt;
&lt;li&gt;Rigid statistical modeling of everything else&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stationary distribution of a space-time SPDE process&lt;/li&gt;
&lt;li&gt;Stochastic perturbation analysis&lt;/li&gt;
&lt;li&gt;More case studies
&lt;ul&gt;
&lt;li&gt;Animal movement data&lt;/li&gt;
&lt;li&gt;Snowpack data&lt;/li&gt;
&lt;li&gt;Pollen data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Modeling alternative stable state in Caribbean coral reefs</title>
      <link>/publication/coral-reefs/</link>
      <pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/publication/coral-reefs/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slowing the Evolution and Outbreak of Antibiotic Resistance</title>
      <link>/publication/antibiotic-resistance/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      <guid>/publication/antibiotic-resistance/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collective Behavior During Dam Passage</title>
      <link>/project/dam-passage/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/dam-passage/</guid>
      <description>&lt;p&gt;In this project I use a bootstrap method to test a radio-telemetry dataset of Chinook and sockeye salmon for evidence of collective navigation while using fish ladders to pass dams.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistically-Constrained Optimal Spatial Sampling Design</title>
      <link>/project/opt-design/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/opt-design/</guid>
      <description>&lt;p&gt;In this project I investigate the use of sparse precision matrices and mixed integer/linear programs to efficient conduct logistically-constrained optimal spatial sampling design.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Source Inference</title>
      <link>/project/source/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/source/</guid>
      <description>&lt;p&gt;In this project I investigate the use of stochastic PDEs to conduct inference on the source term of a linear PDE.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
