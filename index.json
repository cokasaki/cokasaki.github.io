[{"authors":["admin"],"categories":null,"content":"I am a fourth-year PhD student in Quantitative Ecology and Resource Management at the University of Washington. I am advised by Andrew Berdahl in the School of Aquatic and Fishery Sciences. I am collaborating with Sándor Toth from the School of Environmental and Forestry Sciences on a project looking at optimal spatial sampling design under logistical constraints. I am also collaborating with Mevin Hooten from Colorado State University on a project looking at physics-based spatiotemporal statistics. My general research interests include bridging the gaps between mechanistic modeling, spatial statistics, and constrained optimization. I am particularly interested in taking mathematically complex methods and ideas and making them more accessible and useful to ecologists.\n","date":1596585600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1598811457,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a fourth-year PhD student in Quantitative Ecology and Resource Management at the University of Washington. I am advised by Andrew Berdahl in the School of Aquatic and Fishery Sciences. I am collaborating with Sándor Toth from the School of Environmental and Forestry Sciences on a project looking at optimal spatial sampling design under logistical constraints. I am also collaborating with Mevin Hooten from Colorado State University on a project looking at physics-based spatiotemporal statistics.","tags":null,"title":"Connie Okasaki","type":"authors"},{"authors":null,"categories":null,"content":"Over the course of my research I frequently encounter the same problems involving Gaussian Processes. In this page I will record the results of these analyses so as to avoid redoing them do frequently.\n","date":1585612800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1585788743,"objectID":"d3fcb2f042821d7d72361b669a34bb1e","permalink":"/courses/gaussian-processes/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/courses/gaussian-processes/","section":"courses","summary":"My notes for the analyses of Gaussian Process models.","tags":null,"title":"Gaussian Processes","type":"docs"},{"authors":null,"categories":null,"content":"In this course, instructor Timothy Jones will cover regression techniques from linear and nonlinear regression, time series and spatial analysis, and generalized linear modeling, with an emphasis on application, model diagnostics, and model selection. I will post on this site my own supplementary notes that I develop over the course of TAing, as I review this material. It is primarily meant as a tool to structure my own understanding, but it is my hope that it will be useful to students as well.\n","date":1585612800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1585697944,"objectID":"0a56ed321ee320cc754c62cecc79735b","permalink":"/courses/qsci483/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/courses/qsci483/","section":"courses","summary":"My notes for TAing QSCI 493.","tags":null,"title":"Regression Analysis for Ecologists","type":"docs"},{"authors":null,"categories":null,"content":"Matrix/Vector Notation In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector\u0026rsquo;s components are not specified I will bold it such as $\\mathbf{v}$. I will write $X'$ to mean the transpose of a matrix $X$ or $v'$ to mean the transpose of a matrix $v$, as is common in regression analysis.\nAdvanced Notation: Matrix Calculus For an $n\\times 1$ vector $v$ and a scalar $f$ we will write $\\dfrac{\\partial v}{\\partial f}$ to mean the $n\\times 1$ vector with entries $\\left(\\dfrac{\\partial v}{\\partial f}\\right)_i = \\dfrac{\\partial v_i}{\\partial f}$. We will write $\\dfrac{\\partial f}{\\partial v}$ to mean the $n\\times 1$ vector with entries $\\left(\\dfrac{\\partial f}{\\partial v}\\right)_i = \\dfrac{\\partial f}{\\partial v_i}$. A useful resource for matrix calculus can be found on Wikipedia or in the more extensive Matrix Cookbook .\n","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"2d08f287a3a374510fddef0790fed57b","permalink":"/courses/qsci483/notation/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/qsci483/notation/","section":"courses","summary":"Matrix/Vector Notation In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector\u0026rsquo;s components are not specified I will bold it such as $\\mathbf{v}$. I will write $X'$ to mean the transpose of a matrix $X$ or $v'$ to mean the transpose of a matrix $v$, as is common in regression analysis.","tags":null,"title":"Notation","type":"docs"},{"authors":null,"categories":null,"content":"Matrix/Vector Notation In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector\u0026rsquo;s components are not specified I will bold it such as $\\mathbf{v}$. I will write $X'$ to mean the transpose of a matrix $X$ or $v'$ to mean the transpose of a matrix $v$, as is common in regression analysis.\nMatrix Calculus For an $n\\times 1$ vector $v$ and a scalar $f$ we will write $\\dfrac{\\partial v}{\\partial f}$ to mean the $n\\times 1$ vector with entries $\\left(\\dfrac{\\partial v}{\\partial f}\\right)_i = \\dfrac{\\partial v_i}{\\partial f}$. We will write $\\dfrac{\\partial f}{\\partial v}$ to mean the $n\\times 1$ vector with entries $\\left(\\dfrac{\\partial f}{\\partial v}\\right)_i = \\dfrac{\\partial f}{\\partial v_i}$. A useful resource for matrix calculus can be found on Wikipedia or in the more extensive Matrix Cookbook .\nGaussian Process Notation I will write $f \\sim GP(\\mu(x),k(x,x\u0026rsquo;))$ to denote a Gaussian process with mean function $\\mu$ and covariance function (kernel) $k(x,x\u0026rsquo;)$. I will in general not assume that $k$ is stationary (see terminology ). Under this definition $f$ is a stochastic process with the defining property that for any set of points ${x_i}$ (in whatever space $\\Omega$ we choose), the vector $f(x)_i = f(x_i)$ is distributed as a multivariate normal (MVN) random vector with mean $\\mu_i = \\mu(x_i)$ and covariance matrix $\\Sigma_{ij} = k(x_i,x_j)$. Many of the properties I will discuss on this page are actually properties of the MVN distribution.\nTerminology  A Gaussian process is called stationary (or homogeneous) if $k(x,x\u0026rsquo;) = k(x-x\u0026rsquo;)$ A Gaussian Process is called isotropic if $k(x,x\u0026rsquo;) = k(|x-x'|)$ A matrix $M$ is said to be positive semidefinite if it has the property that $v\u0026rsquo;Mv \\geq 0$ for any vector $v$. Covariance matrices are positive semidefinite. A kernel is said to be positive semidefinite (psd) if $$\\int_\\Omega k(x,x\u0026rsquo;)f(x)f(x\u0026rsquo;)dxdx\u0026rsquo; \\geq 0$$ for all $L_2$ functions $f$. Gram matrices (i.e. covariance matrices) from psd kernels are psd matrices. The inverse of the covariance matrix in a MVN distribution is $Q = \\Sigma^{-1}$ and is called the precision matrix.  ","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"22f796f9bd4553bfd321a8325b944162","permalink":"/courses/gaussian-processes/notation/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/notation/","section":"courses","summary":"Matrix/Vector Notation In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector\u0026rsquo;s components are not specified I will bold it such as $\\mathbf{v}$. I will write $X'$ to mean the transpose of a matrix $X$ or $v'$ to mean the transpose of a matrix $v$, as is common in regression analysis.","tags":null,"title":"Notation and Terminology","type":"docs"},{"authors":null,"categories":null,"content":"In this document I will outline the math used in the analysis of a simple linear regression model. Make sure to check out my notation document to clarify any confusing notation.\nThe Model Simple Linear Regression is based upon the equation $$ y_i \\sim N(\\beta_0 + \\beta_1 x_i,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid. It is important to remember that the expectations, or means, of these variables are: $E[y_i] = \\beta_0 + \\beta_1 x_i$ and $E[\\epsilon_i] = 0$. We will assume that there are $n$ data points and only 1 predictor. This is what makes it _simple_ linear regression. In more general linear regression models you have more than 1 predictor. In multivariate linear regression models you have more than one dependent variable as well. In addition to the predictor variable, we also have an intercept, or mean term, which can be thought of as a second predictor.\nFitting Objective Function To fit a simple linear regression we first must choose a measure of fit. Here we will choose least squares, since this corresponds to maximum likelihood esitmation in this model. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\\hat{y}_i$, the value predicted by our fitted model. We can calculate $\\hat{y}_i$ using our predictor $x_i$ for data point $i$ along with estimated coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$: $$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x. $$ Alternatively we can find the whole vector $\\hat{y} = \\hat{\\beta_0}\\mathbf{1} + x\\hat{\\beta}$ (where $\\mathbf{1}$ is a vector of all ones). To find a single squared residuals we calculate $r_i^2 = (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x)^2$. We will define the function $f(\\hat{\\beta}_0,\\hat{\\beta}_1)$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as: \\begin{align*} f(\\hat{\\beta}) \u0026amp; = \\sum_{i=1}^n (\\mbox{residual})^2 \\\\\n\u0026amp; = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1)^2 \\\\\n\\end{align*}\nOptimization From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to both $\\beta_0$ and $\\beta_1$, to find the minimum sum of squared residuals (the least squares). So let us take the partial derivative with respect to $\\hat{\\beta}_0$: \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}_0} } \u0026amp; = \\dfrac{\\partial }{ \\partial \\hat{\\beta}_0} \\sum_{i=1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n \\dfrac{\\partial }{ \\partial \\hat{\\beta}_0}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n 2\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right)\\dfrac{\\partial }{ \\partial \\hat{\\beta}_0}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right) \\\\\n\u0026amp; = -\\sum_{i=1}^n 2\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right) \\end{align*} To set this derivative equal to zero we need to set: \\begin{align*} \\sum_{i=1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right) \u0026amp; = 0. \\end{align*} This can be accomplished by splitting up the sum and getting \\begin{align*} \\sum_{i=1}^n y_i \u0026amp; = \\sum_{i=1}^n \\hat{\\beta}_0 + \\sum_{i=1}^n \\hat{\\beta}_1 x_i \\\\\nn\\hat{\\beta}_0 \u0026amp; = \\sum_{i=1}^n y_i - \\hat{\\beta}_1 \\sum_{i=1}^n x_i \\\\\n\\hat{\\beta}_0 \u0026amp; = \\overline{y} - \\hat{\\beta}_1\\overline{x} \\end{align*} Now that we have calculated $\\hat{\\beta}_0$ in terms of $y,x,$ and $\\hat{\\beta_1}$ we can take the derivative with respect to $\\beta_1$ to find the least squares estimator: \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}_1} } \u0026amp; = \\dfrac{\\partial }{ \\partial \\hat{\\beta}_1} \\sum_{i=1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n \\dfrac{\\partial }{ \\partial \\hat{\\beta}_1}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n 2\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right)\\dfrac{\\partial }{ \\partial \\hat{\\beta}_1}\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right) \\\\\n\u0026amp; = -\\sum_{i=1}^n 2\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right)(x_i) \\end{align*} To set this derivative equal to zero we need to set: \\begin{align*} \\sum_{i=1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i\\right)x_i \u0026amp; = 0. \\end{align*} But here we can plug in our estimator $\\hat{\\beta}_0$ to get \\begin{align*} \\sum_{i=1}^n \\left(y_i - (\\overline{y}-\\hat{\\beta}_1\\overline{x}) - \\hat{\\beta}_1x_i\\right)x_i \u0026amp; = 0. \\end{align*} Then we can expand the sum on the left to get \\begin{align*} \\sum_{i=1}^n (y_i - \\overline{y})x_i - \\sum_{i=1}^n \\hat{\\beta}_1(x_i - \\overline{x})x_i \u0026amp; = 0 \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n (x_i - \\overline{x})x_i \u0026amp; = \\sum_{i=1}^n (y_i-\\overline{y})x_i \\\\\n\\hat{\\beta}_1 \u0026amp; = \\frac{\\sum_{i=1}^n (y_i-\\overline{y})x_i}{\\sum_{i=1}^n (x_i - \\overline{x})x_i} \\end{align*} So we have found an estimator for $\\hat{\\beta}_1$ in terms of only the predictors and the responses. We also have an estimator for $\\hat{\\beta}_0$ in terms of the predictors, the responses, and $\\hat{\\beta}_1$. Now, traditionally, $\\hat{\\beta}_1$ is written in a slightly different form, as \\begin{align*} \\hat{\\beta}_1 \u0026amp; = \\frac{S_{xy}}{S_{xx}} \\\\\nS_{xx} \u0026amp; = \\frac{1}{n-1}\\sum (x_i-\\overline{x})^2 \\\\\nS_{xy} \u0026amp; = \\frac{1}{n-1}\\sum (x_i-\\overline{x})(y_i-\\overline{y}). \\end{align*} The reason for this is that $S_{xy}$ is the sample covariance of $x$ and $y$, and $S_{xx}$ is the sample variance of $x$, so it is nice to express $\\hat{\\beta}_1$ in terms of other statistics that we already know about. We can see that the two formulas for $\\hat{\\beta}_1$ are equivalent by doing a little more math, taking $S_{xx}$ and $S_{xy}$ and changing them to a slightly different format: \\begin{align*} (N-1)S_{xy} \u0026amp; = \\sum (x_i-\\overline{x})(y_i-\\overline{y}) \\\\\n\u0026amp; = \\sum (y_i-\\overline{y})x_i - \\sum (y_i-\\overline{y})\\overline{x} \\\\\n\u0026amp; = \\sum (y_i-\\overline{y})x_i - \\overline{x}\\sum (y_i - \\overline{y}) \\\\\n\u0026amp; = \\sum (y_i -\\overline{y})x_i - \\overline{x}(\\sum \\left(y_i - \\frac{1}{n}\\sum y_i\\right) \\\\\n\u0026amp; = \\sum (y_i -\\overline{y})x_i - \\overline{x}(\\sum y_i - \\sum y_i) \\\\\n\u0026amp; = \\sum (y_i - \\overline{y})x_i \\end{align*} I encourage you to try doing the same calculation with $S_{xx}$: you will find that it follows exactly the same format. So we can see that the formula we have derived for $\\hat{\\beta}_1$ is exactly the same as the traditional format in terms of the sample (co)variances.\n","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"b9b30682261fa66aa2ba653f15912fec","permalink":"/courses/qsci483/linear-regression/simple-linear-regression/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/simple-linear-regression/","section":"courses","summary":"In this document I will outline the math used in the analysis of a simple linear regression model. Make sure to check out my notation document to clarify any confusing notation.\nThe Model Simple Linear Regression is based upon the equation $$ y_i \\sim N(\\beta_0 + \\beta_1 x_i,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid.","tags":null,"title":"Simple Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"In this section I will outline the descriptive statistics that are used to diagnose point process deviations from homogeneity. A standard assumption in the analysis of point processes is that equal areas should have equal densities of points. However, this assumption is often violated. We can diagnose such violations using descriptive statistics such as Ripley\u0026rsquo;s $K$ and $L$ functions.\nRipley\u0026rsquo;s $K$ function This function is defined as $$ \\hat{K}(t) = \\lambda^{-1}\\sum_{i\\neq j} \\frac{I(d_{ij} \u0026lt; t)}{n}. $$\nHere $\\lambda$ is the averasge intensity of the point process, which we will usually estimate as $n/A$ where $n$ is the ttoal number of points and $A$ is the area in question. $I$ is the indicator function which is 1 if the argument is true and 0 if it is false. The summation can be thought of as calculating the fraction of all pairs which are within $t$ units of distance from each other. If points are homogeneous in two dimensions then, ignoring boundary effects, we can expect that each point will contain approximately $\\lambda \\pi t^2$ points within $t$ distance units of it. So we should expect $\\hat{K}(t) \\approx \\pi t^2$.\nRipley\u0026rsquo;s $L$ function The $K$ function suffers from non-constant variance, which can make analyses difficult. Instead, we often use Ripley\u0026rsquo;s $L$ function, defined to be\n$$ \\hat{L}(t) = \\sqrt{\\frac{\\hat{K}(t)}{\\pi}}. $$\nThis function has mean $t$ and variance approximately constant for all $t$. We can interpret plots of Ripley\u0026rsquo;s $L$ function as follows. First, we expect that the $L$ function should follow a straight line through the origin with slope 1. If the $L$ function deviates too much we conclude that there is clustering or overdispersion at that relevant length scale. If the $L$ function deviates to the left of the line, then $\\hat{K}(t)$ is larger than expected, meaning more points fall within distance $t$ than expected. Thus leftward deviations denote clustered processes. Conversely, if the $L$ function is to the right of the line then the points are overdispersed.\n","date":1587942000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"a9f799079f2d8c4b0985c9f64da4d841","permalink":"/courses/qsci483/space-time/diagnostics/","publishdate":"2020-04-27T00:00:00+01:00","relpermalink":"/courses/qsci483/space-time/diagnostics/","section":"courses","summary":"In this section I will outline the descriptive statistics that are used to diagnose point process deviations from homogeneity. A standard assumption in the analysis of point processes is that equal areas should have equal densities of points. However, this assumption is often violated. We can diagnose such violations using descriptive statistics such as Ripley\u0026rsquo;s $K$ and $L$ functions.\nRipley\u0026rsquo;s $K$ function This function is defined as $$ \\hat{K}(t) = \\lambda^{-1}\\sum_{i\\neq j} \\frac{I(d_{ij} \u0026lt; t)}{n}.","tags":null,"title":"Descriptive Statistics","type":"docs"},{"authors":null,"categories":null,"content":"The Woodbury matrix identity is $$(A+UCV)^{-1}=A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.$$ Simpler versions of this identity are \\begin{align*} (I+UV)^{-1} \u0026amp; = I-U(I+VU)^{-1}V, \\\\\n(I+P)^{-1} \u0026amp; = I-(I+P)^{-1}P \\\\\n(I+P)^{-1} \u0026amp; = I-P(I+P)^{-1} \\end{align*} In the special case that $u$ and $v$ are vectors and $C = I$ we get the Sherman-Morrison formula: $$(A+uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.$$ A similar formula is the matrix determinant lemma $$\\mbox{det}(A+uv^T) = (1+v^TA^{-1}u)\\mbox{det}(A).$$\n","date":1585695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"49f3ce0de8104569278476cea840f027","permalink":"/courses/gaussian-processes/linear-algebra/","publishdate":"2020-04-01T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/linear-algebra/","section":"courses","summary":"The Woodbury matrix identity is $$(A+UCV)^{-1}=A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.$$ Simpler versions of this identity are \\begin{align*} (I+UV)^{-1} \u0026amp; = I-U(I+VU)^{-1}V, \\\\\n(I+P)^{-1} \u0026amp; = I-(I+P)^{-1}P \\\\\n(I+P)^{-1} \u0026amp; = I-P(I+P)^{-1} \\end{align*} In the special case that $u$ and $v$ are vectors and $C = I$ we get the Sherman-Morrison formula: $$(A+uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.$$ A similar formula is the matrix determinant lemma $$\\mbox{det}(A+uv^T) = (1+v^TA^{-1}u)\\mbox{det}(A).$$","tags":null,"title":"General Linear Algebraic Properties","type":"docs"},{"authors":null,"categories":null,"content":"In this document I will outline the math used in the analysis of a more general linear regression model, with more than one predictor variable. Make sure to check out my notation document to clarify any confusing notation, and check out my simple linear regression document if you would like a slightly simpler analysis to start off with. This document will charge right into matrix notation from the start.\nThe Model Linear regression is based upon the equation $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid. In the context of regression, it is important to remember that: $E[y_i] = X\\beta$ and $E[\\epsilon_i] = 0$. We will assume that there are $n$ data points and $k$ predictors. This $\\beta$ is a $k\\times 1$ vector, $X$ is a $n\\times k$ matrix and $y$ and $\\epsilon$ are $n\\times 1$ vectors.\nFitting Objective Function To fit a simple linear regression we choose a measure of fit: least squares. This means we are looking for the parameter estimates that minimize the sum of squared residuals. A residual is calculated by the difference between $y_i$ and $\\hat{y}_i$, the value predicted by our fitted model. We can calculate $\\hat{y}_i$ using the $1\\times k$ vector $x_i$ of predictors for data point $i$: $$ \\hat{y}_i = x_i\\hat{\\beta}. $$ Alternatively we can find the whole vector $\\hat{y} = X\\beta$. To find a single squared residuals we calculate $r_i^2 = (y_i - x_i\\hat{\\beta})^2$. We will define the function $f(\\hat{\\beta})$ to be the sum of squared residuals, as a function of the fitted coefficients. We want to find the coefficients which minimize this function. We can calculate this as: \\begin{align*} f(\\hat{\\beta}) \u0026amp; = \\sum_{i=1}^n (\\mbox{residual})^2 \\\\\n\u0026amp; = \\sum_{i=1}^n (y_i - x_i\\hat{\\beta})^2 \\\\\n\u0026amp; = \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)^2 \\end{align*} In matrix notation we can rewrite this, however. The residual _vector_ can be written as the $n\\times 1$ vector $$ r = y - X\\hat{\\beta}. $$ The sum of squares can then be written as $r\u0026rsquo;r$. So then \\begin{align*} f(\\hat{\\beta}) \u0026amp; = r\u0026rsquo;r \\\\\n\u0026amp; = (y-X\\hat{\\beta})'(y-X\\hat{\\beta}). \\end{align*} We can expand this quadratic equation as \\begin{align*} f(\\hat{\\beta}) \u0026amp; = y\u0026rsquo;y - y\u0026rsquo;X\\hat{\\beta} - \\hat{\\beta}\u0026lsquo;X\u0026rsquo;y + \\hat{\\beta}\u0026lsquo;X\u0026rsquo;X\\hat{\\beta}. \\end{align*}\nOptimization From calculus, we can use the fact that the extrema (minimums and maximums) of a function occur when the partial derivatives of that function are equal to zero. So we want to take the partial derivatives of $f$ with respect to each $\\beta_i$, to find the minimum sum of squared residuals (the least squares). So let us take the partial derivative with respect to a particular coefficient: \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}_j} \u0026amp; = \\dfrac{\\partial }{ \\partial \\hat{\\beta}_j} \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n \\dfrac{\\partial }{ \\partial \\hat{\\beta}_j}\\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)^2 \\\\\n\u0026amp; = \\sum_{i=1}^n 2\\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)\\dfrac{\\partial }{ \\partial \\hat{\\beta}_j}\\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right) \\\\\n\u0026amp; = \\sum_{i=1}^n 2\\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)\\left(-x_{ij}\\right) \\\\\n\u0026amp; = -\\sum_{i=1}^n 2x_{ij}\\left(y_i - \\sum_{j=1}^k x_{ij}\\hat{\\beta}_j\\right)\\end{align*} Verify that this can be written in matrix notation as \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}_j} \u0026amp; = -2\\left(X\u0026rsquo;y - X\u0026rsquo;X\\hat{\\beta}\\right)_j, \\end{align*} or in matrix calculus notation \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}} \u0026amp; = -2\\left(X\u0026rsquo;y - X\u0026rsquo;X\\hat{\\beta}\\right). \\end{align*} Remember that according to calculus, every single entry $\\dfrac{\\partial f}{ \\partial \\beta_j}$ must be equal to zero at any extremum (and in particular at the minimum). Thus we can set this whole matrix equation equal to zero, and we get $$ X\u0026rsquo;y = X\u0026rsquo;X\\hat{\\beta}. $$ Since we are trying to derive an equation for $\\hat{\\beta}$ we move the matrix over to the other side and we get $$ \\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y. $$ We can derive this same equation in fewer steps using the more complex matrix calculus notation, which for example allows us to take the derivative of matrix products and use the matrix chain rule: \\begin{align*} \\dfrac{\\partial f}{ \\partial \\hat{\\beta}} \u0026amp; = \\dfrac{\\partial }{ \\partial \\hat{\\beta}}\\left(y\u0026rsquo;y - y\u0026rsquo;X\\hat{\\beta} - \\hat{\\beta}\u0026lsquo;X\u0026rsquo;y + \\hat{\\beta}\u0026lsquo;X\u0026rsquo;X\\hat{\\beta}\\right) \\\\\n\u0026amp; = - \\dfrac{\\partial }{ \\partial \\hat{\\beta}}\\left(y\u0026rsquo;X\\hat{\\beta}\\right) - \\dfrac{\\partial }{ \\partial \\hat{\\beta}}\\left(\\hat{\\beta}\u0026lsquo;X\u0026rsquo;y\\right) + \\dfrac{\\partial }{ \\partial \\hat{\\beta}}\\left(\\hat{\\beta}\u0026lsquo;X\u0026rsquo;X\\hat{\\beta}\\right) \\\\\n\u0026amp; = -y\u0026rsquo;X-(X\u0026rsquo;y)\u0026lsquo;+\\hat{\\beta}\u0026lsquo;X\u0026rsquo;X + (X\u0026rsquo;X\\hat{\\beta})\u0026rsquo; \\\\\n\u0026amp; = -2(X\u0026rsquo;y - X\u0026rsquo;X\\hat{\\beta})\u0026rsquo; \\\\\n\u0026amp; = 0. \\end{align*}\nImportant Quantities Now we have shown that the ordinary least squares estimator for $\\beta$ is $(X\u0026rsquo;X)^{-1}X\u0026rsquo;y$. Using this we may calculate all sorts of other things.\nPredicted Values The predicted values are \\begin{align*} \\hat{y} \u0026amp; = X\\hat{\\beta} \\\\\n\u0026amp; = X(X\u0026rsquo;X)^{-1}X\u0026rsquo;y \\\\\n\u0026amp; = Hy. \\end{align*} Here we have defined the \u0026ldquo;hat matrix,\u0026rdquo; $H = X(X\u0026rsquo;X)^{-1}X'$. This is a useful matrix in linear regression, which maps the data to its predicted values. This is sometimes also called the projection matrix $P$, since it projects the data onto a lower-dimensional linear space. It is sometimes also called the influence matrix. It has two nice properties which we will use in a moment: it is symmetric and idempotent. This means $H'=H$ and $H^2 = H\u0026rsquo;H = H$. We can see this by calculating: \\begin{align*} H\u0026rsquo; \u0026amp; = (X(X\u0026rsquo;X)^{-1}X\u0026rsquo;)\u0026rsquo; \\\\\n\u0026amp; = X(X\u0026rsquo;X)^{-1}X\u0026rsquo; \\\\\n\u0026amp; = H \\\\\nH^2 \u0026amp; = (X(X\u0026rsquo;X)^{-1}X\u0026rsquo;)(X(X\u0026rsquo;X)^{-1}X\u0026rsquo;) \\\\\n\u0026amp; = X(X\u0026rsquo;X)^{-1}(X\u0026rsquo;X)(X\u0026rsquo;X)^{-1}X\u0026rsquo; \\\\\n\u0026amp; = X(X\u0026rsquo;X)^{-1}X\u0026rsquo; \\\\\n\u0026amp; = H \\end{align*}\nResiduals The residuals are \\begin{align*} \\hat{\\epsilon} \u0026amp; = y-\\hat{y} \\\\\n\u0026amp; = y-Hy \\\\\n\u0026amp; = (I-H)y \\\\\n\u0026amp; = (I-X(X\u0026rsquo;X)^{-1}X\u0026rsquo;)y \\\\\n\u0026amp; = My \\end{align*} Here we have defined the \u0026ldquo;residual maker\u0026rdquo; matrix, which can also be called the residual operator. This matrix takes the data and gives you the residuals of the model. It also inherits symmetry and idempotency from the hat matrix, since: \\begin{align*} M\u0026rsquo; \u0026amp; = (I-H)\u0026rsquo; \\\\\n\u0026amp; = I-H \\\\\nM^2 \u0026amp; = (I-H)(I-H) \\\\\n\u0026amp; = I - 2H + H^2 \\\\\n\u0026amp; = I - 2H + H \\\\\n\u0026amp; = I-H \\end{align*} With all of this put together we now have the tools to analyze the residual standard error . First, however, we will analyze the properties of $\\hat{\\beta}$.\n","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585850384,"objectID":"af5ff0cbbab5da3c1ad5d3b242a90cee","permalink":"/courses/qsci483/linear-regression/linear-regression/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/linear-regression/","section":"courses","summary":"In this document I will outline the math used in the analysis of a more general linear regression model, with more than one predictor variable. Make sure to check out my notation document to clarify any confusing notation, and check out my simple linear regression document if you would like a slightly simpler analysis to start off with. This document will charge right into matrix notation from the start.\nThe Model Linear regression is based upon the equation $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\","tags":null,"title":"Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"Block matrices have some nice linear algebraic properties:\n $$\\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix} = \\begin{bmatrix} A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} \u0026amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n-(D-CA^{-1}B)^{-1}CA^{-1} \u0026amp; (D-CA^{-1}B)^{-1} \\end{bmatrix}$$ $$\\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix} = \\begin{bmatrix} (A-BD^{-1}C)^{-1} \u0026amp; -(A-BD^{-1}C)^{-1}BD^{-1} \\\\\n-D^{-1}C(A-BD^{-1}C)^{-1} \u0026amp; D^{-1} + D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1} \\end{bmatrix}$$ $$\\mbox{det} \\begin{pmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{pmatrix} = \\mbox{det}(A)\\times \\mbox{det}(D-CA^{-1}B) = \\mbox{det}(D)\\times\\mbox{det}(A-BD^{-1}C)$$  ","date":1585695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585936094,"objectID":"b0f5ebabb246eb63ed6b300a9e2672a9","permalink":"/courses/gaussian-processes/block-matrices/","publishdate":"2020-04-01T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/block-matrices/","section":"courses","summary":"Block matrices have some nice linear algebraic properties:\n $$\\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix} = \\begin{bmatrix} A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} \u0026amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n-(D-CA^{-1}B)^{-1}CA^{-1} \u0026amp; (D-CA^{-1}B)^{-1} \\end{bmatrix}$$ $$\\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix} = \\begin{bmatrix} (A-BD^{-1}C)^{-1} \u0026amp; -(A-BD^{-1}C)^{-1}BD^{-1} \\\\\n-D^{-1}C(A-BD^{-1}C)^{-1} \u0026amp; D^{-1} + D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1} \\end{bmatrix}$$ $$\\mbox{det} \\begin{pmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{pmatrix} = \\mbox{det}(A)\\times \\mbox{det}(D-CA^{-1}B) = \\mbox{det}(D)\\times\\mbox{det}(A-BD^{-1}C)$$  ","tags":null,"title":"Block Matrices","type":"docs"},{"authors":null,"categories":null,"content":"A standard error is the estimated standard deviation $\\hat{\\sigma}$ for some variable. The residual standard error for linear regression is our estimate of the standard deviation of the noise $\\epsilon$. Recall that we assume the noise is independent and heteroskedastic. This is important since it means we only need to estimate one single parameter for all of the noise variables.\nWe estimate the residual variance using the equation $$\\hat{\\sigma}^2 = \\frac{1}{n-k}\\sum \\hat{\\epsilon}_i^2.$$ You can think of the $n-k$ term as accounting for the fact that we have estimated $k$ parameters before making this estimate. Recall that we fit our model by minimizing the sum of squared residuals. So we\u0026rsquo;ve actually optimized this model to minimize exactly $\\sum \\hat{\\epsilon}_i^2$. The more parameters we have to work with the better that optimization will be. So we would probably get an answer that was too small if we just calculated the mean $$\\frac{1}{n}\\sum \\hat{\\epsilon}_i^2.$$ Dividing by a smaller number $n-k$ accounts for this, making $\\hat{\\sigma}^2$ an unbiased estimator of the residual variance.\nUnbiasedness We can see why this is by calculating the expectation of $\\hat{\\sigma}^2$. We won\u0026rsquo;t even both trying to avoid matrices in this derivation: \\begin{align*} E[\\hat{\\sigma}^2] \u0026amp; = E\\left[\\frac{1}{n-k}\\hat{\\epsilon}'\\hat{\\epsilon}\\right] \\\\\n\u0026amp; = \\frac{1}{n-k} E[y\u0026rsquo;M\u0026rsquo;My] \\\\\n\u0026amp; = \\frac{1}{n-k} E[y\u0026rsquo;My] \\\\\n\u0026amp; = \\frac{1}{n-k} E[(X\\beta + \\epsilon)\u0026lsquo;M(X\\beta+\\epsilon)] \\\\\n\u0026amp; = \\frac{1}{n-k} \\left( E[\\beta\u0026rsquo;X\u0026rsquo;MX\\beta] + E[\\epsilon\u0026rsquo;M\\epsilon] \\right) \\end{align*} where we have eliminated the cross terms since $E[\\epsilon]=0$. Now we can calculate \\begin{align*} X\u0026rsquo;MX \u0026amp; = X\u0026rsquo;(I-H)X \\\\\n\u0026amp; = X\u0026rsquo;X - X\u0026rsquo;(X(X\u0026rsquo;X)^{-1}X\u0026rsquo;)X \\\\\n\u0026amp; = X\u0026rsquo;X - X\u0026rsquo;X \\\\\n\u0026amp; = 0 \\end{align*} so that the first term drops out. At this point we could either do a lot of algebra or we could make use of a convenient statistical property (we will do the latter). The expectation of a _quadratic form_ (i.e. $v\u0026rsquo;Mv$ for some random vector $v$ and constant matrix $M$) can be written as $$E[v\u0026rsquo;Mv] = \\mbox{tr}[M\\Sigma] + \\mu^TM\\mu$$ where $\\mu$ is the mean of $v$ and $\\Sigma$ the covariance of $v$. Using this property we find that \\begin{align*} E[\\hat{\\sigma}^2] \u0026amp; = \\frac{1}{n-k}E[\\epsilon\u0026rsquo;M\\epsilon] \\\\\n\u0026amp; = \\frac{1}{n-k}\\mbox{tr}[M\\Sigma(\\epsilon)] \\\\\n\u0026amp; = \\frac{1}{n-k}\\mbox{tr}[\\sigma^2M] \\\\\n\u0026amp; = \\frac{\\sigma^2}{n-k}\\mbox{tr}[I-H]. \\end{align*} Now, so long as $X$ is full rank (i.e. there are no redundant predictors) the hat matrix has trace $k$. This can be shown using complicated eigenvalue proofs that you can find on Google. Thus $\\mbox{tr}[I-H] = \\mbox{tr}[I]-\\mbox{tr}[H] = n-k$. Thus indeed the $n-k$ term drops out and we find that $\\hat{\\sigma}^2$ is an unbiased estimator of the true residual variance.\nDistribution In fact, we have done more than show it is unbiased. Much of the algebra that we did actually did not depend on the outer expectation. We actually showed more generally that $$\\hat{\\sigma}^2 = \\frac{1}{n-k}\\epsilon\u0026rsquo;M\\epsilon.$$ This is useful because it is a quadratic form, as we described above. Quadratic forms over multivariate normal random vectors have nice properties which we will now derive. Specifically we will show that: $$\\hat{\\sigma}^2 \\sim \\frac{\\sigma^2}{n-k}\\chi^2_{n-k}.$$\nThis proof will involve some additional use of linear algebraic terms and assumptions. Specifically, we will use the fact that a symmetric matrix $A$ can be decomposed into $A = PDP^T$ for an orthogonal (i.e. $P^2 = I$) matrix $P$ and a diagonal matrix $D$. Orthogonal matrices are nice because $\\tilde{z} = Pz$, the product of an orthogonal matrix with a standard normal vector, is still a standard normal vector.\nWe will also use the fact that if $A$ is symmetric and idempotent then all the diagonal entries are either 0 or 1. We will further use the fact that the number of entries that are 1 is equal to the trace of $A$. Since $M$ is symmetric and idempotent we will use all these properties to show: \\begin{align*} \\epsilon\u0026rsquo;M\\epsilon \u0026amp; = \\epsilon\u0026rsquo;PDP^T\\epsilon \\\\\n\u0026amp; = \\sigma^2 z\u0026rsquo;PDP^Tz \\\\\n\u0026amp; = \\sigma^2 \\tilde{z}\u0026lsquo;D\\tilde{z} \\\\\n\u0026amp; = \\sigma^2 \\sum_{i=1}^n D_{ii}\\tilde{z}_i^2 \\\\\n\u0026amp; = \\sigma^2 \\sum_{i=1}^{n-k} \\tilde{z}_i^2 \\\\\n\u0026amp; \\sim \\sigma^2 \\chi^2_{n-k}. \\end{align*}\n","date":1585782000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585867009,"objectID":"aec31d922122695838799adb871e7698","permalink":"/courses/qsci483/linear-regression/standard-error/","publishdate":"2020-04-02T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/standard-error/","section":"courses","summary":"A standard error is the estimated standard deviation $\\hat{\\sigma}$ for some variable. The residual standard error for linear regression is our estimate of the standard deviation of the noise $\\epsilon$. Recall that we assume the noise is independent and heteroskedastic. This is important since it means we only need to estimate one single parameter for all of the noise variables.\nWe estimate the residual variance using the equation $$\\hat{\\sigma}^2 = \\frac{1}{n-k}\\sum \\hat{\\epsilon}_i^2.","tags":null,"title":"The Residual Standard Error","type":"docs"},{"authors":null,"categories":null,"content":"The simplest version of the finite element method is said to have \u0026ldquo;natural\u0026rdquo; Neumann boundary conditions, meaning that Neumann boundary conditions are naturally satisfied without imposing any additional structure. Dirichlet boundary conditions then are said to be \u0026ldquo;essential,\u0026rdquo; meaning they must be explicitly imposed after the fact. Certain FEM formulations change up this Neumann=natural, Dirichlet=essential pradigm but for our purposes we will treat these terms as interchangeable.\nNow, suppose that we are confronted with the FEM equation $$\\begin{bmatrix} K_{11} \u0026amp; K_{12} \\\\ K_{21} \u0026amp; K_{22} \\end{bmatrix}\\begin{bmatrix} u_1 \\\\ u_2\\end{bmatrix} = \\begin{bmatrix} L_{11} \u0026amp; L_{12} \\\\ L_{21} \u0026amp; L_{22} \\end{bmatrix} \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix},$$ with the Dirichlet BC $u_2 = u^*$. We can modify our FEM equation to enforce this, to $$\\begin{bmatrix} K_{11} \u0026amp; K_{12} \\\\ 0 \u0026amp; I \\end{bmatrix}\\begin{bmatrix} u_1 \\\\ u_2\\end{bmatrix} = \\begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\\\ u^* \\end{bmatrix}.$$ We can simplify this to an equation for $u$ by inverting the matrix on the left: $$\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} K_{11}^{-1} \u0026amp; -K_{11}^{-1}K_{12} \\\\ 0 \u0026amp; I \\end{bmatrix} \\begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\\\ u^* \\end{bmatrix}$$ Now, assuming that $f\\sim N(\\mu_f,Q_f)$ and $u^*$ is given (often $u^* = 0$) we obtain the following distribution for $u_1$: \\begin{align*} u_1 \u0026amp; = K_{11}^{-1}L_1f - K_{11}^{-1}K_{12}u^* \\\\\nu_1 \u0026amp; \\sim N\\left(K_{11}^{-1}(L_1\\mu_f - K_{12}u^*), K_{11}^{-1}L_1\\Sigma_fL_1^TK_{11}^{-T}\\right). \\end{align*}\nNow often we are interested in the posterior distribution for $u$ and/or $f$. Since we have assumed that $u^*$ is given the two are deterministically related. However, $u$ is of lower dimension than $f$. So, we will try to calculate the posterior distribution of $f$ since this can be used to calculate the posterior of $u$ but not vice versa. We will assume that we have made some observations $y = y_1 + y_2 = A_1u_1 + A_2u_2 + \\epsilon$ of $u$ from which we wish to make inference. Since $u^*$ is known we essentially have observations: \\begin{align*} y \u0026amp; = A_1K_{11}^{-1}(L_1f - K_{12}u^*) + A_2u^* + \\epsilon \\\\\n\u0026amp; = A_1K_{11}^{-1}L_1f + (A_2 - A_1K_{11}^{-1}K_{12})u^* + \\epsilon \\\\\n\u0026amp; = y_f + y^* + \\epsilon \\end{align*} Using the results for a posterior from a linearly-observed MVN distribution we see that: \\begin{align*} [f|y=a] \u0026amp; = N\\left(\\mu_f + Q_{f|y}^{-1}B^TQ_{\\epsilon}(y - y^* - B^T\\mu_f), Q_{f|y}^{-1}\\right) \\\\\nQ_{f|y} \u0026amp; = Q_f + B^TQ_{\\epsilon}B \\\\\nB \u0026amp; = A_1K_{11}^{-1}L_1 \\end{align*}\nSo the challenges that we face in implementing these two different ideas are as follows:\n $u_1$ is lower dimensional than $f$ and so $f$ is not uniquely defined by $u_1$ $L_1\\Sigma_fL_1^T$ is not easily invertible since $L_1$ is not square OR: $K_{11}^{-1}$ is dense  My solution to this is to take the approach of modeling at the $u$ level. We can deal with $u_1$ being lower dimensional by calculating the degenerate MVN distribution for $f|u_1$. We can deal with $L_1$ being rectangular by considering our old approximation $\\tilde{L}$. Using this approximation, which we have always needed to do to keep $L^{-1}$ sparse, we have that $(\\tilde{L}_1\\Sigma_f\\tilde{L}_1^T)_{ij} = \\tilde{L}_{ii}\\Sigma_{ij(f)}\\tilde{L}_{jj}$ for $1\\leq i,j\\leq n_1$. This is exactly the same as if we truncated the additional rows and columns of $\\tilde{L}$ and $\\Sigma_f$, since $\\tilde{L}$ is zero in all of those columns anyways. Thus we obtain \\begin{align*} (L_1\\Sigma_fL_1^T)^{-1} \u0026amp; \\approx (\\tilde{L}_1\\Sigma_f\\tilde{L}_1^T)^{-1} \\\\\n\u0026amp; = (\\tilde{L}_{11}\\Sigma_{11(f)}\\tilde{L}_{11})^{-1} \\\\\n\u0026amp; = \\tilde{L}^{-1}_{11}\\Sigma_{11(f)}^{-1}\\tilde{L}_{11}^{-1} \\\\\n\u0026amp; = \\tilde{L}^{-1}_{11}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\\tilde{L}^{-1}_{11}. \\end{align*} Since $u_2$ is low dimensional, we can easily invert the dense $Q_{22}$ and we still end up with a sparse matrix $Q_{11(u)}$. So we have solved problem (1).\nNow we hit up against the following problem: we may conduct efficient inference on $u_1$, but even though $u_1$ and $f$ are deterministically linked, $f$ is not uniquely identified by $u_1$. So we need to find the degenerate MVN distribution defining $f|u_1$. Alternatively we can assume that there is some small noise that enters between $f$ and $u_1$ so that the two are not deterministically linked and so that we have a non-degenerate distribution for $f$. This becomes more important if we have measurements for $f$ as well as $u$. For now we will assume this is not the case.\nThe primary obstacle we need to overcome here is how to obtain inference for a degenerate distribution while maintaining our sparsity paradigm, which depends upon the inverse of the covariance matrix. To obtain the distribution for $f|u_1$ we will consider the following algebra: \\begin{align*} [f|u_1] \u0026amp; = [f_1,f_2|u] \\\\\n\u0026amp; = [f_1|u,f_2][f_2|u]. \\end{align*} So what we are able to do here is specify each of these distributions separately. Since all we really need to be able to do is calculate the posterior mean of $f$ and simulate draws of $f|u$, if we can do this we are finished. So what are these distributions? Once $f_2$ is specified, $f_1$ is uniquely determined by $u_1$ so we find that: $$[f_1|u_1,f_2] = L_{11}^{-1}(K_{11}u_1 + K_{12}u^* - L_{12}f_2).$$ So then all we really need to sample is $[f_2|u]$. This can be calculated using Bayes formula: \\begin{align*} [f_2|u] \u0026amp; = [u|f_2][f_2] \\\\\n\u0026amp; = [u_1|f_2,u^*][f_2] \\end{align*} Then we can use the formula $$K_{11}u_1 + K_{12}u^* = L_{11}f_1 + L_{12}f_{2}$$ to obtain $$u_1 = K_{11}^{-1}(L_{11}f_1 + L_{12}f_2 - K_{12}u^*)$$ and the (non-degenerate MVN) distribution \\begin{align*} u_1 \u0026amp; = N\\left(K_{11}^{-1}(L_{11}\\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}L_{11}\\Sigma_{11(f)}L_{11}K_{11}^{-T}\\right) \\\\\n\u0026amp; \\approx N\\left(K_{11}^{-1}(L_{11}\\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\\bar{L}_{11}^{-1}\\Sigma_{11(f)}^{-1}\\bar{L}_{11}^{-1}K_{11}\\right) \\\\\n\u0026amp; = N\\left(K_{11}^{-1}(L_{11}\\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\\bar{L}_{11}^{-1}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\\bar{L}_{11}^{-1}K_{11}\\right) \\end{align*} Now this distribution bears a remarkable resemblance to what we have previously written for $[u_1]$ with no conditioning at all, since our approximation $\\tilde{L}$ removes the direct dependence of $u_1$ on $f_2$. However, it is important to note that in this formulation we _actually_ have the matrix $L_{11}$, rather than a truncation of $\\tilde{L}$. Thus we need to approximate $L_{11}$ directly rather than approximation $\\tilde{L}\\approx L$. Thus we get what we have denote $\\bar{L}_{11}\\approx L_{11}$ which we obtain by grouping all the terms in $L_{11}$ onto the diagonal (the same operation which gives us $\\tilde{L}$ from $L$). Since a number of non-zero terms from $L$ are removed by this operation, and in particular these non-zero terms are along the edges of $u_1$ where it abuts $u_2$, we see that in fact this is a substantively different distribution. Our approximation for $[u_1|f_2]$ has less uncertainty along the boundary, just as we would expect. Now that we have this distribution all we need do is find the unconditional distribution $[f_2]$, which may easily be found by blockwise inversion of $Q_{f}$: $$[f_2] = N(\\mu_{2(f)},(Q_{22} - Q_{21}Q_{11}^{-1}Q_{12})^{-1}).$$ This involves a dense $n_2\\times n_2$ matrix but since the boundary is lower-dimensional this is okay.\nNow actually calculating this full posterior involves some heavy algebra and I\u0026rsquo;m sure there\u0026rsquo;s a better way to do this but here goes: \\begin{align*} [f_2|u_1] \u0026amp; \\propto [u_1|f_2][f_2] \\\\\n\u0026amp; = N\\left(u_1|K_{11}^{-1}(L_{11}\\mu_1 + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}\\bar{L}_{11}\\Sigma_{11}\\bar{L}_{11}K_{11}^{-T}\\right)N\\left(f_2|\\mu_2,\\Sigma_{22}\\right) \\\\\n\u0026amp; \\propto \\exp\\left(-\\frac{1}{2}(u_1 - \\mu_{u|f})^TK_{11}^T\\bar{L}_{11}^{-1}\\Sigma_{11}^{-1}\\bar{L}_{11}^{-1}K_{11}(u_1 - \\mu_{u|f})\\right)\\exp\\left(-\\frac{1}{2}(f_2-\\mu_2)^T\\Sigma_{22}^{-1}(f_2-\\mu_2)\\right) \\\\\n\u0026amp; \\propto \\exp\\left(-\\frac{1}{2}f^T(\\Sigma_{22}^{-1}+L_{12}^T\\bar{L}_{11}^{-1}\\Sigma_{11}^{-1}\\bar{L}_{11}^{-1}L_{12})f - \\frac{1}{2}(f^Tv + v^Tf)\\right) \\\\\nv \u0026amp; = L_{12}^T\\bar{L}_{11}^{-1}\\Sigma_{11}^{-1}\\bar{L}_{11}^{-1}(L_{11}\\mu_1 + L_{12}f_2 - K_{12}u^* - K_{11}u_1) - f_2^T\\Sigma_{22}^{-1}\\mu_2 \\end{align*} Thus we see that $[f_2|u_1]$ is normal with precision matrix: \\begin{align*} Q_{f_2|u_1} \u0026amp; = \\Sigma_{22}^{-1} + L_{21}\\bar{L}_{11}^{-1}\\Sigma_{11}^{-1}\\bar{L}_{11}^{-1}L_{12} \\\\\n\u0026amp; = Q_{22} - Q_{21}Q_{11}^{-1}Q_{12} + L_{21}\\bar{L}_{11}^{-1}(Q_{11} - Q_{12}Q_{22}^{-1}Q_{21})\\bar{L}_{11}^{-1}L_{12} \\\\\n\\mu_{f_2|u_1} \u0026amp; = -Q_{f_2|u_1}^{-1}v \\end{align*}\n","date":1585695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585936094,"objectID":"5ed86b10387eca4ce14207b481dad3d1","permalink":"/courses/gaussian-processes/dirichlet/","publishdate":"2020-04-01T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/dirichlet/","section":"courses","summary":"The simplest version of the finite element method is said to have \u0026ldquo;natural\u0026rdquo; Neumann boundary conditions, meaning that Neumann boundary conditions are naturally satisfied without imposing any additional structure. Dirichlet boundary conditions then are said to be \u0026ldquo;essential,\u0026rdquo; meaning they must be explicitly imposed after the fact. Certain FEM formulations change up this Neumann=natural, Dirichlet=essential pradigm but for our purposes we will treat these terms as interchangeable.\nNow, suppose that we are confronted with the FEM equation $$\\begin{bmatrix} K_{11} \u0026amp; K_{12} \\\\ K_{21} \u0026amp; K_{22} \\end{bmatrix}\\begin{bmatrix} u_1 \\\\ u_2\\end{bmatrix} = \\begin{bmatrix} L_{11} \u0026amp; L_{12} \\\\ L_{21} \u0026amp; L_{22} \\end{bmatrix} \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix},$$ with the Dirichlet BC $u_2 = u^*$.","tags":null,"title":"Dirichlet BCs","type":"docs"},{"authors":null,"categories":null,"content":"The multivariate normal distribution has nicely behaved posterior distributions. In particular, the posterior given an observation of the vector is also multivariate normal.\nBasic Posterior Let $x$ be an $n\\times 1$ vector partitioned into $x = (x_1,x_2)$, with $x_1$ having dimension $k$ and $x_2$ having dimension $n-k$. Partition the mean $\\mu = (\\mu_1,\\mu_2)$ and covariance matrix $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\\\ \\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix}.$$ Then the posterior distribution for $x_1$ given $x_2=a$ is given by \\begin{align*} [x_1|x_2=a] \u0026amp; = N\\left(\\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(a-\\mu_2), \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right). \\end{align*}\nPrecision Formulation Often in my research we are interested in analyzing a MVN distribution with known sparse precision matrix. It is expensive to invert matrices and cheap to work with sparse matrices so we wish to work directly with this precision matrix. Furthermore, we want to calculate the precision matrix for our posterior because it is likely to also be computational advantageous. Let the precision matrix be partitioned $$Q = \\begin{bmatrix} Q_{11} \u0026amp; Q_{12} \\\\ Q_{21} \u0026amp; Q_{22} \\end{bmatrix}.$$ Then the equivalent posterior distribution is \\begin{align*} [x_1|x_2=a] \u0026amp; = N\\left(\\mu_1 + Q_{11}^{-1}Q_{12}(a-\\mu_2),Q_{11}\\right). \\end{align*} Furthermore, let $RR^T = Q$ be the Cholesky decomposition of $Q$ also be partitioned into blocks. If $Q$ is sparse then under mild conditions, $R$ is also sparse and we can work with it to do quick computation. The posterior distribution now is $$[x_1|x_2=a] = N\\left(\\mu_1 - R_{11}^{-1}(R_{11}^{-T}(Q_{12}(a-\\mu_2))), Q_{11}\\right).$$\nLinear Observations Often we are also interested in analyzing a MVN distribution where we observe not the individual components, but a linear combination thereof $y = Ax + \\epsilon$ with some mean-zero MVN noise vector $\\epsilon$. Then the observations are distributed $y|x \\sim N( Ax , Q_\\epsilon^{-1} )$. The posterior $x|y$ is given by: \\begin{align*} [x|y=a] \u0026amp; = N\\left(\\mu_x + Q_{x|y}^{-1}A^TQ_{\\epsilon}(y-A\\mu_x), Q_{x|y}\\right) \\\\\nQ_{x|y} \u0026amp; = Q + A^TQ_{\\epsilon}A \\end{align*}\n","date":1585695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585936094,"objectID":"df7426ebc579ce44668cf90fd3f8086a","permalink":"/courses/gaussian-processes/posteriors/","publishdate":"2020-04-01T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/posteriors/","section":"courses","summary":"The multivariate normal distribution has nicely behaved posterior distributions. In particular, the posterior given an observation of the vector is also multivariate normal.\nBasic Posterior Let $x$ be an $n\\times 1$ vector partitioned into $x = (x_1,x_2)$, with $x_1$ having dimension $k$ and $x_2$ having dimension $n-k$. Partition the mean $\\mu = (\\mu_1,\\mu_2)$ and covariance matrix $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\\\ \\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix}.$$ Then the posterior distribution for $x_1$ given $x_2=a$ is given by \\begin{align*} [x_1|x_2=a] \u0026amp; = N\\left(\\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(a-\\mu_2), \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right).","tags":null,"title":"Posteriors","type":"docs"},{"authors":null,"categories":null,"content":"In this document I will outline the math used to analyze our previous results for linear regression analysis. Make sure to check out my previous posts on notation and simple linear regression before diving in.\nThe Model Recall that linear regression is based upon the equation: $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid. We have previously found that the estimate $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}Xy$ minimizes the sum of squares. In this document we will show: (1) that $\\hat{\\beta}$ is also the maximum likelihood estimator, (2) that $\\hat{\\beta}$ is unbiased, and (3) the covariance matrix for the estimator $\\hat{\\beta}$.\nMaximum Likelihood Our probability model has that $y_i$ are normally distributed, and are independent of each other given the predictors $X$ and the coefficients $\\beta$. The likelihood function is given by the probability (density) of the data given the parameters ($\\beta$), expressed as a function of the parameter estimate ($\\hat{\\beta}$). This function in our case can be written as a product of Gaussian (normal) probability density functions (pdfs): \\begin{align*} \\ell(\\hat{\\beta}) \u0026amp; = \\prod_{i=1}^n N(y_i|X\\hat{\\beta},\\sigma^2) \\\\\n\u0026amp; = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - X\\hat{\\beta})}{2\\sigma^2}\\right) \\\\\n\u0026amp; = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\sum_{i=1}^n \\frac{(y_i - X\\hat{\\beta})}{2\\sigma^2}\\right). \\end{align*} We will now make use of a common trick in statistics: we will calculate the log-likelihood function $\\lambda(\\hat{\\beta}) = \\log(\\ell(\\hat{\\beta}))$. Since the probability density function is always non-negative, and therefore the likelihood is always non-negative, the log-likelihood can be defined. Furthermore, the log is a convex function, and because of this it has the property that $\\ell$ and $\\lambda$ are minimized at the same value $\\hat{\\beta}$. Why this is is not important for this class.\nSo taking the log we obtain \\begin{align*} \\lambda(\\hat{\\beta}) \u0026amp; = \\log\\left(\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\sum_{i=1}^n \\frac{(y_i - X\\hat{\\beta})}{2\\sigma^2}\\right)\\right) \\\\\n\u0026amp; = \\log\\left(\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\right) + \\log\\left(\\exp\\left(-\\sum_{i=1}^n \\frac{(y_i - X\\hat{\\beta})}{2\\sigma^2}\\right)\\right) \\\\\n\u0026amp; = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-X\\hat{\\beta})^2. \\end{align*} Now remember that we are looking for the _maximum likelihood estimator_. So we want to find the value of $\\hat{\\beta}$ which maximizes the likelihood (i.e. maximizes the probability of that data, given the parameters). Viewing this as a function of $\\hat{\\beta}$ we see that maximizing $\\lambda$ is equivalent to minimizing $$ \\sum_{i=1}^n (y_i-X\\hat{\\beta})^2. $$ Therefore the maximum likelihood estimator (MLE) of $\\hat{\\beta}$ is exactly the least-squares estimator $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y$. Importantly, we are at this point omitting the parameter $\\sigma^2$. In fact, the MLE for $\\hat{\\beta}$ is unchanged if we estimate this parameter as well. The MLE for $\\hat{\\sigma^2}$ is given by \\begin{align*} \\hat{\\sigma^2} \u0026amp; = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n\u0026amp; = \\frac{1}{n}y\u0026rsquo;(I-X(X\u0026rsquo;X)^{-1}X\u0026rsquo;)y. \\end{align*}\nUnbiasedness It is important to remember that estimators (such as $\\hat{\\beta}) are themselves random. If we were to simulate from our model, holding $\\beta$ fixed, we would fit a different estimator $\\hat{\\beta}$ to every simulation. Thus an important quality that an estimator can have is unbiasedness. This means that, if the model is correct, the estimator will neither tend to overestimate nor underestimate the true parameter. In mathematical terms, its expectation (or mean) is correct: $E[\\hat{\\beta}] = \\beta$.\nUsing our matrix math this property can be derived fairly quickly. We know that $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y$. We also know that $y = X\\beta + \\epsilon$. Putting these together we see: \\begin{align*} \\hat{\\beta} \u0026amp; = (X\u0026rsquo;X)^{-1}X\u0026rsquo;(X\\beta + \\epsilon) \\\\\n\u0026amp; = (X\u0026rsquo;X)^{-1}X\u0026rsquo;X\\beta + (X\u0026rsquo;X)^{-1}X\u0026rsquo;\\epsilon \\\\\n\u0026amp; = \\beta + (X\u0026rsquo;X)^{-1}X\u0026rsquo;\\epsilon. \\end{align*} Therefore the mean is \\begin{align*} E[\\hat{\\beta}] \u0026amp; = E\\left[\\beta + (X\u0026rsquo;X)^{-1}X\u0026rsquo;\\epsilon\\right]. \\end{align*} The mean has a useful property which we will use here, which is that it is _linear_. This means that the expectation of a sum is the sum of the expectations. In math we write this as $E[A+B] = E[A] + E[B]$. Since matrix operations are essentially just a bunch of sums, we can also write $E[Mv] = ME[v]$, if $v$ is random and $M$ is constant. Using this property we get \\begin{align*} E[\\hat{\\beta}] \u0026amp; = E\\left[\\beta + (X\u0026rsquo;X)^{-1}X\u0026rsquo;\\epsilon\\right] \\\\\n\u0026amp; = \\beta + E\\left[(X\u0026rsquo;X)^{-1}X\u0026rsquo;\\epsilon\\right] \\\\\n\u0026amp; = \\beta + (X\u0026rsquo;X)^{-1}X\u0026rsquo;E[\\epsilon] \\\\\n\u0026amp; = \\beta \\end{align*} since we have assumed that $\\epsilon$ is zero-mean noise. So, in fact, our estimator is unbiased!\nCovariance of $\\hat{\\beta}$ Remember that $\\hat{\\beta}$ is fundamentally a random quantity. It is different in every realization (or simulation) of the statistical model we have written down. It is sensitive to the addition of random noise ($\\epsilon_{i}$) to the data. Luckily, since we have written down a statistical model for our data $y$ we are able to do a statistical analysis for the estimator $\\hat{\\beta}$ to determine its random properties.\nWe showed in the previous section that $\\hat{\\beta}$ was unbiased, that is, it has its mean $E[\\hat{\\beta}] = \\beta$ at the correct place. We will now calculate the variance-covariance (or just covariance) matrix of $\\hat{\\beta}$. This tells us how much we can expect $\\hat{\\beta}$ to vary for different datasets. The diagonal of this covariance matrix tells us the variances of $\\hat{\\beta}_{i}$, which are important quantities that get used, for example, in calculating Rs model summaries.\nThe covariance of two random variables is given by $E[(A-E[A])(B-E[B])]$. In our case $A = \\hat{\\beta}_{i}$ and $B = \\hat{\\beta}_{j}$. This will give us the entry $\\Sigma_{ij}$ in the covariance matrix. We already know that $E[\\hat{\\beta}_{i}] = \\beta_{i}$ since the estimators are unbiased. Therefore the covariance is $$ \\Sigma_{ij} = E\\left[(\\hat{\\beta}_{i} - \\beta_{i})(\\hat{\\beta}_{j} - \\beta_{j})\\right]. $$ Expanding the quadratic in the middle, and remembering that expectations are linear, we get that \\begin{align*} \\Sigma_{ij} \u0026amp; = E[\\hat{\\beta}_i\\hat{\\beta}_j] - E[\\beta_i\\hat{\\beta}_j] - E[\\hat{\\beta}_i\\beta_j] + E[\\beta_i\\beta_j] \\\\\n\u0026amp; = E[\\hat{\\beta}_i\\hat{\\beta}_j] - \\beta_iE[\\hat{\\beta}_j] - E[\\hat{\\beta}_i]\\beta_j + \\beta_i\\beta_j \\\\\n\u0026amp; = E[\\hat{\\beta}_i\\hat{\\beta}_j] - \\beta_i\\beta_j. \\end{align*}\nThis is a useful formula, but it would be far more effective to analyze this problem in terms of matrix notation. Let\u0026rsquo;s go ahead and make that switch. Using matrix notation, the single entry $$ \\Sigma_{ij} = E\\left[(\\hat{\\beta}_i - \\beta_i)(\\hat{\\beta}_j - \\beta_j)\\right]. $$ can be written as a whole matrix $$ \\Sigma = E\\left[(\\hat{\\beta} - E[\\hat{\\beta}])(\\hat{\\beta} - E[\\hat{\\beta}])'\\right]. $$ Since we know that the estimator is unbiased we can plug this into the matrix equation to get \\begin{align*} \\Sigma \u0026amp; = E\\left[(\\hat{\\beta} - \\beta)(\\hat{\\beta}-\\beta)'\\right] \\\\\n\u0026amp; = E\\left[\\hat{\\beta}\\hat{\\beta}'\\right] - \\beta\\beta\u0026rsquo;. \\end{align*} This is exactly the same formula we have above, just written in matrix notation. Now, we already have a formula for $\\hat{\\beta}$ in matrix notation, $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y$, so let\u0026rsquo;s plug this in here. We get \\begin{align*} \\Sigma \u0026amp; = E\\left[(X\u0026rsquo;X)^{-1}X\u0026rsquo;yy\u0026rsquo;X(X\u0026rsquo;X)^{-1}\\right] - \\beta\\beta\u0026rsquo; \\\\\n\u0026amp; = (X\u0026rsquo;X)^{-1}X\u0026rsquo; E[yy\u0026rsquo;] X(X\u0026rsquo;X)^{-1} - \\beta\\beta\u0026rsquo;. \\end{align*} Now we can plug in our formula for $y = X\\beta + \\epsilon$. Remembering that expectations are linear (i.e. can be split up over summations), we get \\begin{align*} E[yy\u0026rsquo;] \u0026amp; = E\\left[(X\\beta + \\epsilon)(X\\beta+\\epsilon)'\\right] \\\\\n\u0026amp; = E\\left[X\\beta\\beta\u0026rsquo;X\u0026rsquo; + \\epsilon X\\beta + \\beta\u0026rsquo;X\u0026rsquo;\\epsilon + \\epsilon\\epsilon\u0026rsquo;\\right] \\\\\n\u0026amp; = X\\beta\\beta\u0026rsquo;X\u0026rsquo; + E[\\epsilon]X\\beta + \\beta\u0026rsquo;X\u0026rsquo;E[\\epsilon] + E[\\epsilon\\epsilon\u0026rsquo;] \\end{align*} Now we already know that $E[\\epsilon] = 0$. What about $E[\\epsilon\\epsilon\u0026rsquo;]$? Well, $$ \\mbox{cov}(\\epsilon_i,\\epsilon_j) = E[\\epsilon_i\\epsilon_j] - E[\\epsilon_i]E[\\epsilon_j] = E[\\epsilon_i\\epsilon_j]. $$ Since independent variables are uncorrelated (and we know that the $\\epsilon_i$ are iid) we see that this matrix is diagonal with entries equal to $\\sigma^2$, the variance of $\\epsilon_i$. Thus: $$ E[yy\u0026rsquo;] = X\\beta\\beta\u0026rsquo;X\u0026rsquo; + \\sigma^2I. $$ Finally, plugging this in, we can find that \\begin{align*} \\Sigma \u0026amp; = (X\u0026rsquo;X)^{-1}X\u0026rsquo; E[yy\u0026rsquo;] X(X\u0026rsquo;X)^{-1} - \\beta\\beta\u0026rsquo; \\\\\n\u0026amp; = (X\u0026rsquo;X)^{-1}X\u0026rsquo;(X\\beta\\beta\u0026rsquo;X\u0026rsquo; + \\sigma^2I)X(X\u0026rsquo;X)^{-1} - \\beta\\beta\u0026rsquo; \\\\\n\u0026amp; = (X\u0026rsquo;X)^{-1}(X\u0026rsquo;X)\\beta\\beta\u0026rsquo;(X\u0026rsquo;X)(X\u0026rsquo;X)^{-1} + (X\u0026rsquo;X)^{-1}X\u0026rsquo;(\\sigma^2I)X(X\u0026rsquo;X)^{-1} - \\beta\\beta\u0026rsquo; \\\\\n\u0026amp; = \\beta\\beta\u0026rsquo; + \\sigma^2(X\u0026rsquo;X)^{-1} - \\beta\\beta\u0026rsquo; \\\\\n\u0026amp; = \\sigma^2(X\u0026rsquo;X)^{-1}. \\end{align*} This is the covariance matrix of $\\hat{\\beta}$! Notably, this has two important properties. One, it depends on $\\sigma^2$, so if we want to use this we had better estimate $\\sigma^2$ somehow. More on this in the next section. Two, it depends on the _inverse_ of $X\u0026rsquo;X$. There\u0026rsquo;s no guarantee that this matrix is invertible. For example, if we have more predictors than we have data points (i.e. $k \u0026gt; n$) this matrix will certainly _not_ be invertible. You may have been warned about this scenario in the past. However, even if you have many data points, other situations can crop up where $X\u0026rsquo;X$ is either numerically difficult to invert (i.e. difficult to calculate on a computer), or produces very large variances. One such case is where two of the predictor variables are very highly correlated. More on this later.\nOther Quantities Here we will show briefly that $\\hat{y}$ is also unbiased (assuming the model is correct). This can be seen by some matrix calculations: \\begin{align*} E[\\hat{y}] \u0026amp; = E[Hy] \\\\\n\u0026amp; = E[X(X\u0026rsquo;X)^{-1}X\u0026rsquo;(X\\beta+\\epsilon)] \\\\\n\u0026amp; = E[X\\beta + H\\epsilon] \\\\\n\u0026amp; = X\\beta + HE[\\epsilon] \\\\\n\u0026amp; = X\\beta. \\end{align*} Meanwhile, $\\hat{\\epsilon}$ is: \\begin{align*} \\hat{\\epsilon} \u0026amp; = My \\\\\n\u0026amp; = (I-H)y \\\\\n\u0026amp; = y - (X\\beta + H\\epsilon) \\\\\n\u0026amp; = (I-H)\\epsilon \\\\\n\u0026amp; = M\\epsilon \\end{align*} This is unbiased in the sense that it has the same mean as $\\epsilon$. Unfortunately, although it might seem we can simply calculate $\\epsilon = M^{-1}\\hat{\\epsilon}$, this matrix may not be invertible (TODO: I\u0026rsquo;m like 99% certain it is always uninvertible)\n","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585867009,"objectID":"e2ce29f32b27c8adf6af584b8db1930e","permalink":"/courses/qsci483/linear-regression/properties/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/properties/","section":"courses","summary":"In this document I will outline the math used to analyze our previous results for linear regression analysis. Make sure to check out my previous posts on notation and simple linear regression before diving in.\nThe Model Recall that linear regression is based upon the equation: $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid.","tags":null,"title":"Properties of Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"The Moore-Penrose Pseudoinverse is a generalization of the inverse. In particular it extends the inverse matrix to non-square matrices. The inverse of a matrix $A$ is defined by any matrix $A^+$ with the following four properties\n $AA^+A = A$ $A^+AA^+ = A^+$ $(AA^+)^* = AA^+$ $(A^+A)^* = A^+A$  If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = (A^A)^{-1}A^$ and the pseudoinverse is a left inverse since $A^+A = I$. If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = A^*(AA^*)^{-1}$ and the pseudoinverse is a _right inverse_ since $AA^+ = I$.\nFinally, consider $(AB)^+$. This is equal to $B^+A^+$ if:\n $A$ has orthonormal columns (i.e. $A^*A = I$), or $B$ has orthonormal rows (i.e. $BB^* = I$), or $A$ has full column rank and $B$ has full row rank, or $B = A^*$ In general, however, $(AB)^+ \\neq $B^+A^+$.  ","date":1585782000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585936094,"objectID":"60f5020b4023f47ec39e92b78512e149","permalink":"/courses/gaussian-processes/pseudoinverse/","publishdate":"2020-04-02T00:00:00+01:00","relpermalink":"/courses/gaussian-processes/pseudoinverse/","section":"courses","summary":"The Moore-Penrose Pseudoinverse is a generalization of the inverse. In particular it extends the inverse matrix to non-square matrices. The inverse of a matrix $A$ is defined by any matrix $A^+$ with the following four properties\n $AA^+A = A$ $A^+AA^+ = A^+$ $(AA^+)^* = AA^+$ $(A^+A)^* = A^+A$  If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = (A^A)^{-1}A^$ and the pseudoinverse is a left inverse since $A^+A = I$.","tags":null,"title":"Moore-Penrose Pseudoinverse","type":"docs"},{"authors":null,"categories":null,"content":"In this document I will outline the math used to derive and analyze the statistics (calculated quantities) for a linear regression model. Make sure to check out my previous posts before diving in. Recall that linear regression is based upon the equation: $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid. We have previously derived the estimators $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y$, as well as for $\\mbox{cov}(\\hat{\\beta}) = \\sigma^2(X\u0026rsquo;X)^{-1}$.\nMeasures of Fit The $t$ statistic The most popular statistic used for linear regression is $t_i = \\hat{\\beta}_i/\\hat{\\sigma}(\\hat{\\beta}_i)$. Since we have shown that $\\hat{\\beta}$ is distributed as a multivariate normal random vector, $\\hat{\\beta}_i$ is distributed as a normal random variable. Furthermore, $\\hat{\\sigma}(\\hat{\\beta}_i)$, the standard error for this estimate, is a product of the standard error $\\hat{\\sigma}(\\epsilon)$ and the analytical standard deviation of $\\beta_i$ (for $\\sigma = 1$) which is just the $\\sqrt{(X\u0026rsquo;X)^{-1}_{ii}}$. The important thing is we have a normal variable and its standard error, and therefore this statistic is distributed as a $t$ random variable with $(n-k)$ degrees of freedom (recall $n$ is the number of data points and $k$ is the number of predictors).\nThe $F$ statistic The $F$ test for the overall model is a formal test with hypothesis: \\begin{align*} H_0: \u0026amp; ~~~\\beta_i = 0 \\mbox{ for all } i \\\\\nH_1: \u0026amp; ~~~\\beta_i \\neq 0 \\mbox{ for some }i. \\end{align*} Notably we will be assuming that there _is_ still a non-zero constant mean term even under $H_0$. If we do not assume this then we can basically set all the $\\overline{y}$ terms equal to zero and remove the 1 degree of freedom from the null hypothesis terms. To test this we need to calculate a few important quantities, all of which are sums of squares:\n SSE (sum of squares: error)  This is basically just the sum of squares for all the residuals SSE~$ = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\epsilon_i^2$ We showed previously that SSE$\\sim \\chi^2_{n-k}$ If you look back at the derivation you will see that this is true regardless of $\\beta$   SSM (sum of squares: model)  This is basically how much extra variation from the mean the model explains SSM~$ = \\sum_{i=1}^n (\\hat{y}_i - \\overline{y})^2$ This turns out to also be a $\\chi^2$ random variable We can prove this using the same basic argument. Let $J = 11^T$ be a matrix of all 1s. Then: $$\\mbox{SSM} = y\u0026rsquo;(H-\\frac{1}{n} J)'(H-\\frac{1}{n} J)y.$$ However $H - \\frac{1}{n}J$ is symmetric and idempotent, with rank $k-1$. Thus SSM$\\sim \\chi^2_{k-1}$ following the same logic from before .   SST (sum of squares: total)  This is how much total variation this is around the mean SST~$ = \\sum_{i=1}^n (y_i-\\overline{y})^2$ This is also $\\chi^2_{n-1}$ (prove this yourself?) Remarkably SSE+SSM=SST, which can be used as an elementary proof that SST is $\\chi^2_{n-1}$ (though there are other ways). This is the famous variance decomposition for ANOVA models. Why is this true?    TODO: what is an ANOVA?\nVariance Decomposition We can think about the variance decomposition several ways. The simplest proof is geometric in nature but requires jumping through some linear-subspace hoops. We can think about three points in $\\mathbb{R}^n$, $n$-dimensional space where our datapoints $y$ live. One point is just the data vector $y$. Another point is $\\hat{y}_i = \\hat{y}_i$ a third point is $\\overline{y}_i = \\overline{y}$. We can also define $k$ vectors in this space, corresponding to the values of our predictors $x^{(j)}_i = X_{ij}$. These $k$ vectors define a $k$-dimensional linear subspace of $\\mathbb{R}^n$ (which you can think of as $X\\beta$ for all of the possible $k$-dimensional values of $\\beta$. The prediction vector $\\hat{y}$ is nothing but the projection of $y$ onto the closest point of this subspace. Since one of the predictors is the mean (i.e. $X_{i1} = 1$) the mean point $\\hat{y}$ falls inside this linear subspace. This gives us the following geometric intuition:\n The mean is in the subspace The projection is in the subspace The projection is the closest point in the subspace to the data. This is because the projection minimizes the sum of squares, or the squared distance between the subspace and the data. Therefore the vector leading from the data to the projection is orthogonal or perpendicular to the subspace In particular the vector leading from the data to the projection is orthogonal to the vector leading from the projection to the mean Therefore we can apply the Pythagorean theorem, to find that the squared distance between the data and the projection (prediction) plus the squared distance between the projection and the mean is equal to the squared distance between the data and the mean. If you sit down and think about what these \u0026ldquo;squared distance\u0026rdquo; mean in mathematical terms you will find that this is a geometric proof of SSE+SSM=SST.  We can also think about the variance decomposition in terms of raw linear algebra and matrices: TODO\nA common mistake made in deriving the variance decomposition is noting that $$(y_i - \\hat{y}_i) + (\\hat{y}_i - \\overline{y}) = (y_i-\\overline{y}).$$ The false argument then goes: square the terms and sum and the equality holds. However, as you likely know $A+B=C$ does not imply that $A^2+B^2=C^2$. We require the additional structure of linear algebra and/or geometry to obtain this result\nBack to the $F$ test We can now adjust SSE and SSM to get the \u0026ldquo;mean sum of squares,\u0026rdquo; by dividing by the degrees of freedom:\n MSE = SSE$/(n-k)$ MSM = SSM$/(n-1)$ Finally, our test statistic is $F = \\mbox{MSM}/\\mbox{MSE}$. In order for this to be a valid $F$ statistic, SSM and SSE must be independent. This turns out to be true but I will not prove it here (TODO).  Note that if the model explains a lot more variation than just the mean we expect for MSE to be small and MSM to be large: make sure this make sense to you. So if the model is \u0026ldquo;real\u0026rdquo; then we expect $F$ to be large. If the null hypothesis is true then we expect $F$ to be small. So we reject the null for large values of $F$.\nThe $R^2$ There are numerous ways to calculate different versions of $R^2$ but at the end of the day they all boil down to correlation coefficients: the correlation $R$ between the predictors and the response variable. We square it because this gives us a measure of the proportion of variance explained by the predictors.\nBasic $R^2$ The most basic version of this statistic is just the correlation coefficient between the the predictors and the response. It is also known as the coefficient of determination. It is calculated as $SSM/SST$. This gives it the nice interpretation of being the ratio of \u0026ldquo;variance explained by the model\u0026rdquo; to \u0026ldquo;total variance around the mean.\u0026rdquo; It can also be defined mathematically as the square of the Pearson correlation coefficient between $y$ and $\\hat{y}$: \\begin{align*} R^2 \u0026amp; = \\left(\\frac{\\sum_{i=1}^n (y_i-\\overline{y})(\\hat{y}_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (y_i-\\overline{y})^2}\\sqrt{\\sum_{i=1}^n (\\hat{y}_i - \\overline{y})^2}}\\right)^2 \\end{align*}\nHowever, the raw $R^2$ calculation has some problems as a measure of model fit. For one thing, $R^2$ always increases whenever you add a new predictor. This is because $\\hat{y}$ will always get closer to $y$ as our minimization procedure gains degrees of freedom to find the \u0026ldquo;best fit.\u0026rdquo; Basically this means that $R^2$ does not penalize overfitting. As we add more and more predictors to our model $R^2$ will just keep getting better and better and will fit coefficients that are highly dependent on the random noise. So we need something better.\nAdjusted $R^2$ The adjusted $R^2$ tries to account for overfitting by decreasing the original $R^2$ statistic. There are two formulas we can use for the adjusted $R^2$ (which we will denote $\\overline{R}^2$): \\begin{align*} \\overline{R}^2 \u0026amp; = 1-(1-R^2)\\frac{n-1}{n-k-1} \\\\\n\u0026amp; = 1-\\frac{\\mbox{SSE}/(n-k-1)}{\\mbox{SST}/(n-1)}. \\end{align*} There are two thiings we can note from this. One is we can see how this adjustment works by comparing it to the original formula for $R^2$: \\begin{align*} R^2 \u0026amp; = \\frac{\\mbox{SSM}}{\\mbox{SST}} \\\\\n\u0026amp; = 1 - \\frac{\\mbox{SSE}}{\\mbox{SST}} \\\\\n\u0026amp; = 1 - \\frac{\\mbox{SSE}/n}{\\mbox{SST}/n}. \\end{align*} One way of thinking about $\\overline{R}^2$ is that the original (un-adjusted) $R^2$ was using biased estimates $\\frac{\\mbox{SSE}}{n}$ and $\\frac{\\mbox{SST}}{n}$ which need to be adjusted for their degrees of freedom $n-k-1$ and $n-1$. TODO(what is SST/n an estimate of).\nResiduals Standardizing Frequently when evaluating a model we will plot residuals against other things. Often when we do this we will standardize the residuals by dividing $\\hat{\\epsilon}_i$ by the residual standard error $\\hat{\\sigma}$. However, this is not necessarily the best way to achieve our goal of truly standardizing the residuals. The issue is that the residuals are different from the errors. Although the errors have equal variance, the residuals actually do not. This can be seen by remembering that: $\\hat{\\epsilon} = M\\epsilon$. Thus, in reality the residuals are correlated and have different variances from one another.\nStudentizing Luckily we can account for this. The way we do this is by \u0026ldquo;studentizing\u0026rdquo; the residuals. There are several formulas we can use to represent this, but basically it boils down to the fact that $\\hat{\\epsilon} \\sim N(0,\\sigma^2M)$. Thus the true variance of $\\hat{\\epsilon}_i$ is not $\\sigma^2$ but $\\sigma^2M_{ii}$. Thus we can studentize by calculating $$t_i = \\frac{\\hat{\\epsilon}_i}{\\hat{\\sigma}(\\epsilon)\\sqrt{M_{ii}}}.$$ We can equivalently express this in terms of the leverage as $$t_i = \\frac{\\hat{\\epsilon}_i}{\\hat{\\sigma}(\\epsilon)\\sqrt{1-h_{ii}}}.$$ TODO: some more on this\nDeleting Since outliers may have outsized influence on the model fit (see the next section we may want a more robust statistic for estimating the residuals. We can generate this by considering the deleted residuals, or the residuals when comparing a data point to the model fitted without that data point. We denote the vector of these as residuals $\\hat{y}_{(i)}$ and a particular residual as $\\hat{y}_{j(i)}$. The deleted residuals are expressed as: $$d_i = y_i - \\hat{y}_{i(i)}.$$ How can we express these mathematically?\nWe will start by considering the whole vector $\\hat{y}_{(i)}$. This is the vector of all $n$ predictions, generated using all the data except the $i$-th point. We can re-express $\\hat{y}_{(i)}$ in terms of matrices, although it takes a little thought. Recall that $\\hat{y} = Hy = X(X\u0026rsquo;X)X\u0026rsquo;y = X\\hat{\\beta}$. To calculate $\\hat{y}_{(i)}$ we essentially need to first calculate $\\hat{\\beta}_{(i)}$, the coefficient estimates without data point $i$. To do this we remove one entry from $y$ and therefore also one row from $X$. However the dimension of $\\hat{\\beta}$ does not change. We can actually write this as: $$\\hat{\\beta}_{(i)} = (X\u0026rsquo;X - x_i\u0026rsquo;x_i)^{-1}(X\u0026rsquo;y - x_i\u0026rsquo;y_i).$$ Then to calculate the predictions $\\hat{y}_{(i)}$ we still just multiply by $X$, since we wish to predict all data points, including $y_i$. Thus: $$\\hat{y}_{(i)} = X(X\u0026rsquo;X - x_i\u0026rsquo;x_i)^{-1}(X\u0026rsquo;y - x_i\u0026rsquo;y_i).$$ Now we can do a little more algebra to make this nicer, using the very nice Sherman-Morrison formula to calculate $$(X\u0026rsquo;X - x_i\u0026rsquo;x_i)^{-1} = (X\u0026rsquo;X)^{-1} + \\frac{(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;x_i(X\u0026rsquo;X)^{-1}}{1 - x_i(X\u0026rsquo;X)^{-1}x_i}.$$ The denominator here involves $x_i(X\u0026rsquo;X)^{-1}x_i$. This is just the leverage $h_{ii}$. We will denote the $n\\times 1$ vector representing a column of $H$ as $h_i$. Using this notation, our adjusted predictions become: \\begin{align*} \\hat{y}_{(i)} \u0026amp; = X\\left((X\u0026rsquo;X)^{-1} + \\frac{(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;x_i(X\u0026rsquo;X)^{-1}}{1 - h_{ii}}\\right)(X\u0026rsquo;y - x_i\u0026rsquo;y_i) \\\\\n\u0026amp; = X(X\u0026rsquo;X)^{-1}X\u0026rsquo;y - X(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;y_i + \\frac{1}{1-h_{ii}}X(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;x_i(X\u0026rsquo;X)^{-1}X\u0026rsquo;y - \\frac{1}{1-h_{ii}}X(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;x_i(X\u0026rsquo;X)^{-1}x_i\u0026rsquo;y_i \\\\\n\u0026amp; = Hy - h_iy_i + \\frac{1}{1-h_{ii}}h_ih_i\u0026rsquo;y - \\frac{1}{1-h_{ii}}h_ih_{ii}y_i \\\\\n\u0026amp; = \\hat{y} - \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right)h_iy_i + \\frac{h_ih_i\u0026rsquo;y}{1-h_{ii}} \\\\\n\u0026amp; = \\hat{y} - \\frac{h_i(y_i-h_i\u0026rsquo;y)}{1-h_{ii}} \\\\\n\u0026amp; = \\hat{y} - \\frac{y_i-\\hat{y}_i}{1-h_{ii}}h_i \\\\\n\u0026amp; = \\hat{y} - \\frac{\\hat{\\epsilon}_i}{1-h_{ii}}h_i \\end{align*} So the $i$th entry of the vector, $\\hat{y}_{i(i)} = \\hat{y}_i - \\frac{\\hat{\\epsilon}_ih_{ii}}{1-h_{ii}}$. So the deleted residuals become \\begin{align*} d_i \u0026amp; = y_i - \\hat{y}_{i(i)} \\\\\n\u0026amp; = y_i - \\hat{y}_i - \\frac{\\hat{\\epsilon}_ih_{ii}}{1-h_{ii}} \\\n\u0026amp; = \\hat{\\epsilon_i}\\left(1 - \\frac{h_{ii}}{1-h_{ii}}\\right) \\\n\u0026amp; = \\frac{1-2h_{ii}}{1-h_{ii}}\\hat{\\epsilon}_i \\end{align*}\nMeasures of Influence One other way of evaluating a regression fit is by considering the influence of each point. An unfortunate side effect of the least squares criterion is that it punishes point that are very far away from the fit more than it punishes points that are kinda far away from the fit. Basically this means that outliers are very influential. We can assess precisely how influential in a variety of ways.\nLeverage One of the most popular ways of assessing this infuence is with the leverage. The leverage is defined as a measure of \u0026ldquo;observation self-sensitivity.\u0026rdquo; It is defined as the partial derivative: $h_{ii} = \\dfrac{\\partial \\hat{y}_i}{\\partial y_i}$. That is, how quickly does the predicted value change as the data point changes. This can be easily calculated from our equation for the predicted values: $\\hat{y} = Hy$ so that we can see that $h_{ii}$ is in fact just the $ii$-th entry of the hat matrix.\nMoreover we can show (TODO) that the leverage is bounded: $0\\leq h_{ii}\\leq 1$. So the predicted value will always increase when the datapoint increases, but will never increase quite as fast. That fits our intuition that predictions should be consistent with the data, but that no one data point should completely determine the fit.\nBecause least-squares is so sensitive to outliers, observations with leverages close to 1, or much larger than all the other leverages, should be suspect and might be considered outliers.\nCook\u0026rsquo;s Distance Another way of assessing influence is with the Cook\u0026rsquo;s distance. This estimates the effect of deleting a particular observation. It is defined to be $D_i$: the squared distance between the predictions $\\hat{y}$ and the predictions $\\hat{y}_{(i)}$ when observation $i$ is removed from the sample, divided by $k$ times the mean squared error. In math: $$D_i = \\frac{\\sum_{j= 1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{k\\hat{\\sigma^2}}.$$\nTherefore the Cook\u0026rsquo;s distance can also be expressed as \\begin{align*} D_i \u0026amp; = \\frac{\\sum_{j= 1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{k\\hat{\\sigma^2}} \\\\\n\u0026amp; = \\frac{\\left(\\frac{\\hat{\\epsilon}_i}{1-h_{ii}}\\right)^2h_i\u0026rsquo;h_i}{k\\hat{\\sigma}^2} \\\\\n\u0026amp; = \\frac{\\hat{\\epsilon}_i}{k\\hat{\\sigma}^2}\\frac{h_{ii}}{(1-h_{ii})^2} \\end{align*}\nDFFITS A third measure of influence is DFFITS (which may stand for Difference of Fits). It is sort of like a cross between leverage and Cook\u0026rsquo;s distance. Like leverage, it is a measure of self-sensitivity, but like Cook\u0026rsquo;s distance it uses the predictions when a point is left out. The actual mathematical definition is: $$\\mbox{DFFITS}_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{s_{(i)}\\sqrt{h_{ii}}},$$ where $\\hat{y}_{i(i)}$ is the predicted value for $y_i$ when $y_i$ is removed from the dataset and $s_{(i)}$ is the standard error estimated without $y_i$.\nAlthough the formulas for DFFITS and Cook\u0026rsquo;s distance are different, it turns out to be possible to convert from one to another. We will not show this here (TODO) but the general relationship is $$D_i = \\frac{\\mbox{DFFITS}^2_i\\hat{\\sigma^2}_{(i)}}{k\\hat{\\sigma^2}}.$$ In particular except for very small datasets the mean squared error should not change too much when data point $i$ is remove, so the approximate relationship $$D_i \\approx \\frac{\\mbox{DFFITS}_i^2}{k}$$ holds.\nNormality Moments A common way of quantifying a statistical distribution is with its moments. These are defined to be the expectations $E[X], E[X^2], E[X^3], $ and so on ($E[X^i]$ in general). The central moments are defined to be $E[(X-\\mu)^i]$ and the standardized moments are $E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^i\\right]$. The normal distribution is entirely defined by its first and second moments. In particular its third standardized moment (its skewness) is 0 and its fourth standardized moment (its kurtosis) is 3. By calculating the sample skewness and sample kurtosis we can heuristically evaluate whether the residuals from our model seem to follow a normal distribution.\nShapiro-Wilk We can apply a more formal test of normality using the Shapiro-Wilk test. This test statistic is defined by considering the order statistics $x_{(i)}$ from a sample and calculating: $$W = \\frac{\\left(\\sum_{i=1}^n a_ix_{(i)}\\right)^2}{\\sum_{i=1}^n (x_i-\\overline{x})^2}.$$ The coefficients $a_i$ are obtained as the vector $\\frac{V^{-1}m}{C}$ where $m$ is the expected values of the order statistics for iid standard normal random variables, and $V$ is the covariance matrix for those order statistics. $C$ is the length of the vector $V^{-1}m$. There is no classical distribution that fits $W$, so testing is usually done using simulation methods.\nHeteroskedasticity The most common way of testing for heteroskedaticity is the Breusch-Pagan test (implemented in R as ncv.test). This tests whether the variance of the residuals is dependent on the values of the predictors. This tests works by conducting regression as ordinary and calculating the residuals $\\hat{\\epsilon}$. Then we consider the possibility that the square of the residuals $\\hat{\\epsilon}^2$ depends upon the predictors. We test this by fitting a new linear model: $$\\hat{\\epsilon}^2 = X\\gamma + \\eta.$$ The test statistic is $nR^2$ where $R^2$ is the coefficient of determination for the second model. Under the null hypothesis that the predictors have no effect. this test statistic is distributed $\\chi^2_{k-1}$. (TODO: why?)\n","date":1585609200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586197075,"objectID":"94bd8cb2ecb4da4555f1b8683956183a","permalink":"/courses/qsci483/linear-regression/math-diagnostics/","publishdate":"2020-03-31T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/math-diagnostics/","section":"courses","summary":"In this document I will outline the math used to derive and analyze the statistics (calculated quantities) for a linear regression model. Make sure to check out my previous posts before diving in. Recall that linear regression is based upon the equation: $$ y_i \\sim N(X\\beta,\\sigma^2) $$ or equivalently \\begin{align*} y_i \u0026amp; \\sim X\\beta + \\epsilon_i \\\\\n\\epsilon_i \u0026amp; \\sim N(0,\\sigma^2) \\end{align*} where the $\\epsilon_i$ are iid. We have previously derived the estimators $\\hat{\\beta} = (X\u0026rsquo;X)^{-1}X\u0026rsquo;y$, as well as for $\\mbox{cov}(\\hat{\\beta}) = \\sigma^2(X\u0026rsquo;X)^{-1}$.","tags":null,"title":"Linear Regression Model Diagnostics","type":"docs"},{"authors":null,"categories":null,"content":"In this document I will outline the math used to derive and analyze the model diagnostics for a simple linear regression model. Make sure to check out my previous posts on notation and linear regression before diving in.\nPlotting fitted vs residuals\n should see no trend/pattern centered on zero  Plot histogram of residuals\n basically centered on zero? skewed/kurtosis? Shapiro-Wilks test (can be very sensitive to small deviations with large datasets)  QQ plot\nDFFITS Deleted residuals Cook\u0026rsquo;s distance and leverage\n","date":1585695600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585850384,"objectID":"bf0bf19af10b36cccb0a1c807d4da3d4","permalink":"/courses/qsci483/linear-regression/diagnostics/","publishdate":"2020-04-01T00:00:00+01:00","relpermalink":"/courses/qsci483/linear-regression/diagnostics/","section":"courses","summary":"In this document I will outline the math used to derive and analyze the model diagnostics for a simple linear regression model. Make sure to check out my previous posts on notation and linear regression before diving in.\nPlotting fitted vs residuals\n should see no trend/pattern centered on zero  Plot histogram of residuals\n basically centered on zero? skewed/kurtosis? Shapiro-Wilks test (can be very sensitive to small deviations with large datasets)  QQ plot","tags":null,"title":"Using Model Diagnostics","type":"docs"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Spent most of yesterday working on the air pollution data. Plan to work today on the GP code and the paper, and maybe spend a little time cleaning the air pollution data and getting all the spatial info sorted out.\nMorning: 10-12\n Code the optimization GPs  Afternoon: 1-3\n Work on the paper  Afternoon: 4-5\n Get spatial data sorted out  Reality Spent most of the day coding the GPs.\n","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596585600,"objectID":"fdb8f8609f5962f7e7e746b281cea910","permalink":"/post/notebook/2020/summer/week1/wedn/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/post/notebook/2020/summer/week1/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 8/5","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Afternoon: 12-2\n Code the optimization GPs  Afternoon: 2-4\n Download and import air pollution data, wind data  Afternoon: 4-5\n Read VL chapter  Reality Spent most of the day coding the GPs.\n","date":1596412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596412800,"objectID":"b7ee2c3a95ba73c3aa50e1adb46d8e89","permalink":"/post/notebook/2020/summer/week1/mon/","publishdate":"2020-08-03T00:00:00Z","relpermalink":"/post/notebook/2020/summer/week1/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 8/3","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Fell off the horse here with goal setting, what with everything going on but we\u0026rsquo;re starting up again now so here we go\n Finish response to reviewers document for Editor/R1 Code GP simulation for optimizing under a misspecified model Begin analyzing air pollution data  Reality ","date":1596412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596412800,"objectID":"f262fd3bc8703e7b6c8e77a74495477f","permalink":"/post/notebook/2020/summer/week1/post-1/","publishdate":"2020-08-03T00:00:00Z","relpermalink":"/post/notebook/2020/summer/week1/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 1(ish) of summer 2020","tags":["Notebook"],"title":"Lab Notebook: Su20 Week 1","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals  Set date for general exam, do all the paperwork things Resubmit paper Make progress on source inference  Sub goal: Set up model and begin data analysis for air pollution case study Sub goal: Find a new pollen dataset/third case study?   Make progress on optimal survey design  Sub goal: Figure out how to run my sims on hyak? Sub goal: Code up sims for non-Matern field Sub goal: Add sims to write-up Sub goal: Figure out where we\u0026rsquo;re targeting   Read  Do some reading from Pukelsheim Do some reading from Bootstrap Do some reading from VL    ","date":1596412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596412800,"objectID":"f6c05a459709aac87a8b13b95fa3ebcb","permalink":"/post/notebook/2020/summer/spring-post-1/","publishdate":"2020-08-03T00:00:00Z","relpermalink":"/post/notebook/2020/summer/spring-post-1/","section":"post","summary":"My quarterly lab notebook goals post for summer 2020","tags":["Notebook"],"title":"Lab Notebook: Summer 2020","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-10:30\n Respond to TAing emails  Morning: 11-12:30\n Plug away on either SQL or optimization or both.  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-4:30\n Meet with Tyler  Reality I did these things! My optimization finally ran without bugs, I managed to do a little tiny bit of SQL.\n","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"2235e2ea2ea995f902de97d6385c6c33","permalink":"/post/notebook/2020/spring/week7/thurs/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week7/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 5/7","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Finish prepping for committee meeting. Start to grade and read VL.\nReality Feel food about committee meeting. Graded a few homeworks and read a few chapters.\n","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"4a0e174579e55be6b69b4c7c9f339501","permalink":"/post/notebook/2020/spring/week7/wedn/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week7/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 5/6","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"After large amounts of time spent practicing and recalling information, that information becames automatic. It is quickly and subconsciously accessible and does not impose upon expensive higher-level thought processes. When information is not automatic, these thought processes require are extremely tiring.\nReading is a natural example: when we read our eyes saccade or quickly refixate about every quarter-second. The eye focuses on the next word in sequence and we process each word individually for meaning at that pace. Missing the meaning of even a single word can threaten the understanding of the whole sentence, and so the mind automatizes this process after months or years of practice. Moreover, in order to synthesize and comprehend longer sentences and arguments, words must be processes fast enough for the whole argument to be held in working memory. Slow reading due to lack of automaticity threatens this process.\nThe theory that certain students struggle with reading comprehension because they are slow to process individual words is known as the simple theory of reading. Even though a child may understand each word individually, they cannot comprehend the whole sentence unless they can process words fast enough to intake the whole sentence more or less at once. This automatic processing is not possible without many many hours of intensive self-directed practice.\nNumber facts follow a similar pattern. Although mathematics instruction often focuses on \u0026ldquo;surface learning\u0026rdquo; vs \u0026ldquo;deep understanding,\u0026rdquo; there is almost no meaningful distinction between these, particularly in early education. The importance of automaticity and reducing cogniftive load is such that basic drilling of sums and products is necessary to develop future math skills. Furthermore speed of access of these number facts (among others) is associated with higher confidence and positive feelings.\nUltimately these important automatic skills can only be developed through large amounts of invested time, over the course of years. No one teacher can be responsible for these processes, but all must contribute for successful education to occur.\n","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"5999ca55099d3fef61498f0d81d3d3f9","permalink":"/post/teaching/visible-learning/ch-07/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-07/","section":"post","summary":"My notes on Chapter 7 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"Teaching for automaticity in basic academic skill","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"Teachers have long lists of reasons why feedback is important. Students overwhelmingly look to feedback for only one thing: how to improve their work to succeed next time. Students tend to focus on the future over the past, and often interpret even constructive feedback on past work as personal attacks and can react defensively.\nThe impact of negative events is stronger than the impact of positive events, but despite this teachers consistently give more negative feedback than positive.\nTODO\n","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"2eb2803ba945c07b8ea1961e2cef1dcc","permalink":"/post/teaching/visible-learning/ch-08/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-08/","section":"post","summary":"My notes on Chapter 8 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"The role of feedback","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-11\n Finish prepping for STRG  Morning: 11-12\n Space-time reading group  Afternoon: 1:30-3:30\n Office hours  Afternoon: 3:30-5\n Prep for committee meeting  Reality STRG went really well! People seemed to like my talk and we had a lot of good discussion. Continuing to prep for committee meeting.\n","date":1588636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"7eb60cd19994ef00a2a4f6d85efd2919","permalink":"/post/notebook/2020/spring/week7/tues/","publishdate":"2020-05-05T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week7/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 5/05","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Finish prepping for STRG tomorrow. Continue work on committee meeting presentation.\nReality Did that!\n","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"f7cada00e1294d3d801a821056933bc1","permalink":"/post/notebook/2020/spring/week7/mon/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week7/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 5/4","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals  Give presentation to STRG Give presentation to committee meeting and get useful feedback Complete VL Ch 7-12? Continue to learn SQL Need a lot of bandwidth to tackle revisions, so we will do this next week Grade HW 2  Reality Presentation went well.\n","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"be5828524d3f892dce95018b1ed34220","permalink":"/post/notebook/2020/spring/week7/post-1/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week7/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 7 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 7","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Choose and prep paper for STRG next week. Prepare for committee meeting.\nReality Did these things. STRG is going to take longer than I had hoped. Long week ahead of me.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"748bd380c0d7f63aae435545d69d0102","permalink":"/post/notebook/2020/spring/week6/fri/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 5/1","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-10:30\n Respond to TAing emails  Morning: 11-12:30\n Plug away on either SQL or optimization or both.  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-4:30\n Meet with Tyler  Reality I did these things! My optimization finally ran without bugs, I managed to do a little tiny bit of SQL.\n","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"f7d2b5820c2375d8754c8f910c0fd0eb","permalink":"/post/notebook/2020/spring/week6/thurs/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/30","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Find a dataset on Neotoma and debug the optimization case study.\nReality Spent a lot of time on both tasks. Neotoma is remarkably difficult to use. I\u0026rsquo;m trying to learn a tiny bit of SQL to access it because I don\u0026rsquo;t think there is a better dataset out there for this project, but it has been frustrating. The optimization still is not solving. I think it is either an issue with the bounds or an issue with the size of the problem.\n","date":1588118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"2702b012a6e6dde5cd9eec7010378b4e","permalink":"/post/notebook/2020/spring/week6/wedn/","publishdate":"2020-04-29T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/29","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-11\n Read Vecchia paper for STRG  Morning: 11-12\n Space-time reading group  Afternoon: 12-12:30\n Meet with Andrew  Afternoon: 1:30-3:30\n Office hours  Afternoon: 3:30-5 Neotoma\nReality Got a decent amount of reading done, but did not find a satisfactory dataset on Neotoma. More on this tomorrow I suppose.\n","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"e208067579f8e5837bb92a735df89deb","permalink":"/post/notebook/2020/spring/week6/tues/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/28","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-9:30\n Set up for the week  Morning: 9:30-10:30\n Respond to emails  Morning 10:30-12:30\n Integrate pilot study and optimization  Afternoon: 1:30-3:30\n TAing Work on TAing notes when possible  Afternoon: 3:30-5\n Find Neotoma dataset  Reality Responded to all my emails. Got the code running for the pilot study/optimization integration, but it\u0026rsquo;s been a while since I looked at the optimization code. It runs but slowly and I don\u0026rsquo;t entirely remember how to fix it. Did not end up finding Neotoma dataset, instead I ended up working on SSN stuff that I didn\u0026rsquo;t finish yesterday, and in particular I learned how Cholesky sparse up-looking updates work.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"9a4f70b0e3713276dd3e871e156c4401","permalink":"/post/notebook/2020/spring/week6/mon/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/27","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals  Select a pollen dataset to use Find a trade winds dataset to use Run the end-to-end pilot study/optimization procedure (and compare to spatially balanced) Complete VL Ch 7-10 Look ahead to QSCI HW 3 Write up the SSN scratch paper before I forget it all  Reality  Got access to Neotoma although it is much more complex than I feared No progress on trade winds End-to-end pilot study procedure is working, but haven\u0026rsquo;t incorporated spatially balanced yet Keeping up to speed on QSCI  ","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598811457,"objectID":"ee93bed5c844197919bf2c9c0de67146","permalink":"/post/notebook/2020/spring/week6/post-1/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week6/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 6 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 6","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-10\n Debug sampling design code  Morning: 10-11\n Look into paleopollen and email Mevin  Morning: 11-12\n Write on SSN paper  Afternoon: 1-2\n Read and take notes on VL Ch 7/8  Afternoon: 2-3\n Write on Mevin paper  Afternoon: 3-4\n Debug sampling design code  Afternoon: 4-5\n Read fun papers  Reality Lots of scattered goals today so not all of them actually happened. I did look into the paleopollen stuff but there\u0026rsquo;s a lot there so still need to sift through what may be most applicable. I took some notes on Ch 7 but not Ch 8, and I did read some fun papers. Spent much more than 1 hour debugging the sampling design code but I did finally get it working!\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588005139,"objectID":"da5f6f80335ccfa45d8f357bc661b7d5","permalink":"/post/notebook/2020/spring/week5/fri/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/24","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-11\n Finish grading and respond to TAing emails  Morning: 11-12:30\n Debug covariance parameter estimation for optimal sampling design  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-4:30\n Meet with Tyler  Reality Meetings and lab went well! Implemented profile likelihood method for parameter estimation in the sampling design case study but it still doesn\u0026rsquo;t seem to be working. Not sure what could be going on.\n","date":1587600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588005139,"objectID":"151c1c29c91a24ebcf5b6b03176f3580","permalink":"/post/notebook/2020/spring/week5/thurs/","publishdate":"2020-04-23T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/23","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Just grade all day!\nReality I did that! I got almost everything finished just one or two more assignments to go.\n","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588005139,"objectID":"0127c383e0a3a591aa1713caf8d46011","permalink":"/post/notebook/2020/spring/week5/wedn/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/22","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-11\n Read Vecchia-Laplace paper  Morning: 11-12\n Space-time reading group  Afternoon: 1-1:30\n Look for applications of Vecchia-Laplace paper  Afternoon: 1:30-3:30\n Attend lecture, grading  Afternoon: 3:30-5\n Grading  Reality I did these things! I think the Vecchia-Laplace paper could potentially be useful for the SSN stuff if it turns out that calculating the covariance matrix isn\u0026rsquo;t so so hard, or it could at least be useful for the existing SSN framework. Only got a few assignments done.\n","date":1587427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588005139,"objectID":"3b8b461c27253e6161252c2172bb8eed","permalink":"/post/notebook/2020/spring/week5/tues/","publishdate":"2020-04-21T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/21","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-9:30\n Lab checkins and set up for the week  Afternoon: 9:30-12:30\n Grade  Afternoon: 1:30-3:30\n TAing Work on TAing notes when possible  Afternoon: 3:30-5\n Work on optimal design synthetic experiment  Reality ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"8c725f42db0ea714d4f533162db113e7","permalink":"/post/notebook/2020/spring/week5/mon/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/20","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals  Email Mevin about animal movement and pollen Keep working on write-up and looking for datasets Select and simulate a pilot study procedure for optimal sampling design Read and take notes on VL Ch 7 and 8 Grade for QSCI 483  Reality Did email Mevin and got pointed to a really cool paleo dataset to use for that. Selected a cluster sampling pilot study procedure for the optimal sampling design project. Read and took (some) notes for VL Ch 7. Did a lot of gradnig for QSCI.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588005139,"objectID":"be28af5499e3fdc77096f6fcaa74a648","permalink":"/post/notebook/2020/spring/week5/post-1/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week5/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 5 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 5","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-10\n Respond to emails  Morning: 10-11\n Lab meeting  Morning: 11-12\n Write up animal movement case study  Afternoon: 1-2\n Read and take notes on VL Ch 5  Afternoon: 2-3\n Look for wind speed data  Afternoon: 3-4\n Work on sampling design case study  Afternoon: 4-5\n Work on SSN modeling  Reality Did all these things although I did not finish writing up the animal movement case study \u0026ndash; there\u0026rsquo;s a lot more background to include. Read and took notes on both VL Ch 5 and 6. Turns out the pollen dataset that I thought was from AAAAI/NAB was actually not. It is more paleoecological than really pollen counts that we\u0026rsquo;re looking for. Only real pollen counts I\u0026rsquo;ve been able to find so far are AAAAI and EAN (Europe) both of which are closely guarded and cannot be made public. I\u0026rsquo;d really like to use a public dataset so I\u0026rsquo;m not sure what to do about this\u0026hellip;however I did find a wind speed dataset that seems to be pretty decent although right now it only seems to be marine measurements so I\u0026rsquo;m trying to see if there\u0026rsquo;s a land-based sister dataset.\n","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"89fdef1d885e6690fd53ba292d522322","permalink":"/post/notebook/2020/spring/week4/fri/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/17","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"The recitation method, also known as the IRE cycle (initiation-response-evaluation) or the CDR method (conventional-direct-recitation) refers to a particular pattern used commonly by teachers. This pattern has stayed in place despite considerable criticism for hundreds of years. The cycle progresses from a teacher initiating an interaction, inviting a response (e.g. asking a question), and evaluating the response before beginning the cycle anew.\nThis system has been criticized as follows:\n Questions asked are mainly low level, calling only for simplistic answers Only one student is active at a time Education is reduced to receiving pre-packaged knowledge and demonstrating its retention Classroom conversation is inherently predictable, task-oriented, but unstimulating. Learning becomes sterile, non-emotional, and rule-bound. This system is ubiquitous around the world. However, about 75% of classtime in an average classroom is spent on the instruction stage. Less than 1% of time was spent on open questions that might ask for complex responses.  The recitation method has some advantages associated with overcrowded classrooms and the need to teach a rigid set curriculum. It is appealing to teachers because they remain in control of the interaction at virtual all stages and may speed up or slow down to any pace. However students learn little from just hearing teachers talk.\nOne of the major principles of learning is that the learner needs to be actively responding to get anything out of it. They do not necessarily need to respond overtly, they just need to be actively engaged. This is difficult to maintain for the long periods of time that teachers generally speak for.\nThe redundancy effect identified by cognitive load researchers states that when teachers talk for long periods of time, students are unable to determine which information is relevant and which is not. Effective teachers will explain material extremely well, but very briefly. Students will not learn simply by listening for longer periods of time. Mental focus drops up after perhaps 10 minutes.\nTwo theories underly this drop in mental focus, or mind wandering. One theory, known as ego depletion is that one\u0026rsquo;s ability to focus intensely actually literally runs out through exhaustion, measured by brain glucose. So, your mind wanders in order to build up energy for the next upcoming demand that will be placed upon it. The second theory is known as cascading inattention. This refers to when the mind is unable to clearly process and organize incoming information and fit it into a simple framework and structure.\nAn alternative is formulated as the PDC (progressive-discovery-constructivist) approach although the chapter did not explain what this approach entails. Another alternative is tha Paideia model which states that learning takes place in three parts: didactic instruction, Socratic questioning, and coached product. Each of these hsould take up a third of time. However, Socratic questioning is the key and entails more than just students talking. Questions must promote higher order thinking; student talk is a means not an end.\n","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"383c15b895fe56f2a860c497ac0f5a78","permalink":"/post/teaching/visible-learning/ch-06/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-06/","section":"post","summary":"My notes on Chapter 6 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"The recitation method and the nature of classroom learning","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12:30\n Work on animal movement and synthetic data  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-4:30\n Meet with Tyler  Reality Found the bug in my animal movement code!!! Finally I can move on to other parts of this case study. Ran MCMC and it is giving me good and reasonable answers.\n","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"8e3a726bb8247ef570a1662fd139c8c1","permalink":"/post/notebook/2020/spring/week4/thurs/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/16","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"Your pedagogical philosophy has no bearing on student outcomes unless you spend time on the issues you claim to emphasize.\nStudies used four different measures of time (see VL pg. 37) each a subset of the last\n Allocated time  time as programmed proactively, on documents and curriculum plans problems: interruptions, visitors, announcements, transitions improved by: school mandated policy   Instructional time  actual time genuinely available for instruction problems: poor classroom management, allowing time to be hijacked by low-priority issues improved by: managerial skill and time prioritization   Engaged time  time student actually pays attention problems: students not knowing what to focus on, distractions, boredom, fatigue improved by: clear instructions and meaningful tasks, encouragement, feedback   Academic learning time  time when student is learning and responding successfully problems: student may be unsuccessful despite effort, gaps in prior knowledge, tasks too challenging improved by: individualized guidance, encouragement    Amount of time dedicated to a particular task varies wildly between different teachers. However the most critical measure of time is academic learning time. Higher tiers can be used to help maximize academic learning time but this is the level that is most consistently correlated with student success. However, just spending time with a topic does not necessarily imply improvement. Rather deliberate practice with a focus on improvement is necessary, with critical components including guidance, instruction, goal setting, and feedback.\nTime is particular important for forming deep understanding and connections between topics. Comparing curriculums streamlined to cover the same material in shorter or longer periods of time, there was no difference in test scores on surface-level multiple choice questions. However, free response questions meant to test deep understanding and connections between topics showed wild variation with much higher scores for the students that took the longer courses.\n","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"b4d38c689dd891af2a6a5096ebbe8bee","permalink":"/post/teaching/visible-learning/ch-05/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-05/","section":"post","summary":"My notes on Chapter 5 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"Time as a global indicator of classroom learning","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Respond to Sándor and find spatial dataset  Afternoon: 1-1:30\n Work on Dirichlet BC code  Afternoon: 1:30-3:30\n Attend class Take notes on VL Ch 4 Work on SNN modeling math  Afternoon: 3:30-5\n Work on Dirichlet BC code  Reality Did most of these things. Started working on the synthetic data case study for the optimization project.\n","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"9ec29f7ab09318cdfa99680f3e0e9fd3","permalink":"/post/notebook/2020/spring/week4/wedn/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/15","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9:30-12:30\n Continue work on animal movement  Afternoon: 1:30-3:30\n TAing  Afternoon: 3:30-5\n NSF Activity Report  Reality Finished the activity report, held office hours, worked on movement. Still stumped by my movement bugs. I\u0026rsquo;m getting not-PSD errors from a function that should not possibly be able to produce a non-PSD matrix.\n","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"d12f85463d2c7f2c8bc46eacdaa5fa5d","permalink":"/post/notebook/2020/spring/week4/tues/","publishdate":"2020-04-14T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/14","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-9:30\n Lab checkins and set up for the week  Afternoon: 9:30-12:30\n Debug animal movement code While animal movement code is running, work on VL, then SNN math  Afternoon: 1:30-3:30\n TAing Work on TAing notes when possible  Afternoon: 3:30-5\n Work on NSF progress report  Reality Worked on animal movement code. It seems as though my bug is the \u0026ldquo;wiggles\u0026rdquo; common to FEM advection-type equations so I am attempting to implement FD. It wasn\u0026rsquo;t too hard for this simple example and is slightly better practice anyways. Spent pretty much all day on that however.\n","date":1586736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"2e9b0b612d86d2fd3e0eb42e2f899bfe","permalink":"/post/notebook/2020/spring/week4/mon/","publishdate":"2020-04-13T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/13","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12:30\n Work on animal movement case study  Afternoon: 1:30-2\n Make notes on VL Ch 3 and 4  Afternoon: 2-3\n Email Mevin and Sándor  Afternoon: 3-5\n Work on pollen case study  Reality Mostly worked on the case study. Made some notes on VL Ch 3 but the movement stuff was very buggy.\n","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586795372,"objectID":"12029a29c5f1639ee9ac2e424036087d","permalink":"/post/notebook/2020/spring/week3/fri/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/10","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"An empathy gap occurs when people are relatively unable to put themselves in the place of another person. Deep levels of empathy are difficult if you have never had the same experience as another person, or if you are not currently experiencing anything similar. It is easiest to empathize with people whose experiences you are currently mirroring, harder if you have mirrored them in the past, and hardest if you have never experienced them at all. Children in particular do not develop the ability to deeply understand others\u0026rsquo; viewpoints until late adolescence or early adulthood.\nInability to empathize with each other results in negative escalations. When one person wrongs or hurts another, the other person retaliates. The first underestimates the harm they incurred and so views the retaliation as out of proportion, which justifies further retaliation, leading to a snowball effect or negative cascade.\nOn the other hand establishing positive relationships cascades and results in lasting benefits. Marked improvements have been observed to occur the year following relationship-based interventions. Longitudinal studies have found persistent and profound benefits to positive teacher student relationships on a decadal scale. These positive relationships may help because they build trust that allows students to make mistakes and ask for help, and builds the confidence in students to try again.\nDeveloping positive relationships can be difficult if the focus is on responding to uncooperative students. Social psychology shows that emotional leakage is common. Hiding emotions is difficult, and when a teacher has negative emotions towards a student the student may realize this. No major theory of learning recommends punitive action in response to difficult students. For example, high school students tend to attribute disciplinary action to the teacher (\u0026ldquo;teacher hats me\u0026rdquo;) rather than accept personal responsibility.\nWhen a child has an unsupportive home environment, school can become a major source of social and cultural learning. Positive teacher-student relationships can be very influential in personal development in this stage and can mitigate the risk of negastive outcomes for children. On the other hand, positive at-home relationships can buffer negative relationships at school.\nCan these positive relationships be manufactured? A study found that just spending a few minute devoted to non-directive non-coercive friendly and child-centered activities positively influences teacher\u0026rsquo;s views for at-risk students.\n","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586795372,"objectID":"93020b37bf7b9e6c36c018a4c87a2c56","permalink":"/post/teaching/visible-learning/ch-03/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-03/","section":"post","summary":"My notes on Chapter 3 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"The teacher-student relationship","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"Nearly a century of research has shown that there is NOT a \u0026ldquo;best\u0026rdquo; personality for a teacher to have. Students value being treated with fairness, dignity, and respect much more than they value a particular personality type.\nIt has been found that children lose respect for adults who violate basic social rules, exhibit cruelty, or otherwise break social conventions. Studies show that very young children possess a strong sense of what is right and wrong and have a deep sense of fairness. They may not behave in this manner, since social judgment and risk assessment develop much later. Almost universally though, basic misdeeds such as being caught lying incur significant degradation of trust and positive relationships.\nStudies show that students evaluate teachers after exposure for as little as 10 seconds. This is an example of the blink effect. Moreover, these blink assessments are at least somewhat accurate, being correlated with ratings from actual students (although possible this could be an example of the blink effect itself being cemented in place in those genuine students). Deliberate use of warm and inviting body language, direct eye contact, and friendly intonation increased scores on academic tests for 7-year old students.\nIt is very difficult to detect when children tell lies. Children develop skillful deceit at a very young age and from that point accuracy for telling lies is about 60% (barely above random chance). Rather, we are very good at detecting emotions, but emotions are poor indicators of deceit since truth-tellers often show signs of anxiety when placed under suspicion. Meanwhile liars may prepare for being grilled. Takeaway: do not expect to be able to tell if a student is lying from social cues.\nSeeking help in the classroom is a good thing, and should not be conflated with dependency (relying on a single source excessively and consistently). Help-seeking is associated with \u0026ldquo;mastery goal orientation\u0026rdquo; where students are motivated by factors involving understanding and acquiring knowledge. This contrasts with \u0026ldquo;ego orientation\u0026rdquo; or \u0026ldquo;performance orientation\u0026rdquo; where the goal is to look good or outperform one\u0026rsquo;s immediate peers. Both orientatios coexist, but mastery orientation is associated with higher performance and deeper understanding.\nStudents are more likely to seek help if they trust their teacher, and if their teacher encourages help-seeking behavour. As students age they begin to associate question-asking with low ability. However, with supportive teachers, students associate question asking more positively.\n","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587399620,"objectID":"770ec0bef6c17b886ad6530fc4efb741","permalink":"/post/teaching/visible-learning/ch-04/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-04/","section":"post","summary":"My notes on Chapter 4 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"Your personality as a teacher","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12:30\n Make edits to animal movement modeling section  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-4:30\n Meet with Tyler  Reality Did these things! Was hoping to finish the animal movement section but it didn\u0026rsquo;t quite happen. Gotta make some decisions about presenting IOU vs OUF results.\n","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586537545,"objectID":"d5d0b04aef639da601273e1b696e54cc","permalink":"/post/notebook/2020/spring/week3/thurs/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/9","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Write personal statement and finish all my portion of the concurrent MS application  Afternoon: 1-1:30\n Work on Dirichlet BC code  Afternoon: 1:30-3:30\n Attend class and read VL Ch 3  Afternoon: 3:30-5\n Read Mevin\u0026rsquo;s sources  Reality Finished application by 11:30. Got Dirichlet BC code up and running and am not seeing any obvious bugs right away. Looking good! Skimmed VL Ch 3 and Ch 4, will take notes tomorrow.\n","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586537545,"objectID":"80a142fb77b0da07ec481fe0e5e0484d","permalink":"/post/notebook/2020/spring/week3/wedn/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/8","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Work on Dirichlet BC code  Afternoon: 1-1:30\n Read Visible Learning Chapter 3  Afternoon: 1:30-3:30\n Hold office hours Take notes on VL Ch 3 and read VL Ch 4  Afternoon: 3:30-5\n Email Sándor  Reality Got side tracked with adminstrative stuff again. Evidently I have to formally apply to the concurrent MS program in the stats department so that will take up much of my week. I did get the Dirichlet code up and running although there are probably a few lingering bugs. MCMC run is going now. Did not get to VL or emailing Sándor, although I actually looked at his email and there is not too much to respond to directly. I do owe him an update soon though.\n","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586302293,"objectID":"8d9afa194b7f859bef39eecaa6c290b8","permalink":"/post/notebook/2020/spring/week3/tues/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/7","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-11\n Prep for the week and finish up my preliminary analysis for the spatial stream network model  Afternoon: 11-1:30\n Work on Dirichlet BC code Try to run it at least once  Afternoon: 1:30-3:30\n TAing  Afternoon: 3:30-5\n Look at bypass responses  Reality Morning stuff went well but I fell into administrative tasks and did not get the code done. I did email my committee, email with Erica about my TAing position, and email with the stats department about the concurrent master\u0026rsquo;s program. I took a brief look at Mevin\u0026rsquo;s response in the afternoon.\n","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586302293,"objectID":"5c4f6ab913fe39e18675d5eb55c3efab","permalink":"/post/notebook/2020/spring/week3/mon/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/6","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Still need to finish up my Dirichlet BC code from last week but I got a lot off my plate! I am done with the bypass and the dam passage paper and can get back to research more-or-less full time (less my new TAing position). My next few goals for spring quarter are all focused on my source project, but I would like to make some more progress on the optimal design front to keep some forward momentum on that front.\nI would like to spend this week doing a mix of: finishing up the Dirichlet code and hopefully the animal movement case study, TAing, and responding to feedback from my bypass proposal. So my goals are\n Finish up Dirichlet BC code by Tuesday Read Visible Learning Chapters 3 and 4 by Wednesday/Friday Debug animal movement code and get it run at least once by Thursday Respond to Sándor\u0026rsquo;s email by Tuesday Read two of Mevin\u0026rsquo;s suggested papers by Wednesday, the rest by Friday  Reality  Finished up the Dirichlet BC code Read but have not yet taken notes on VL Chs 3 and 4 Ran the animal movement code many times, but still debugging Still need to get back to Mevin and Sándor with updates.  ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586795372,"objectID":"c002a9c2a3eedcb0d89168626ccd4568","permalink":"/post/notebook/2020/spring/week3/post-1/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week3/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 3 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 3","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12:30\n Work on Dirichlet BC stuff  Afternoon: 1:30-3:30\n Work on QSCI483 stuff  Afternoon: 3:30-5\n Work on Dirichlet BC stuff  Reality Made some really good progress on Dirichlet BC stuff, but did not manage to get the code to a working point. I feel like I have a good handle on QSCI for this week, and I made some good progress on stream network analysis.\n","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586197075,"objectID":"e1ea2d6787e7dd4a4015de36a3c8290a","permalink":"/post/notebook/2020/spring/week2/fri/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/3","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12:30\n Look at QSCI stuff  Afternoon: 1:30-3:30\n Computer lab  Afternoon: 3:30-5\n Work on Dirichlet BC code  Reality Made some good progress on remembering how all the model diagnostics work. Taking this as an opportunity to go beyond what the course really calls for and cement my understanding of these things because they\u0026rsquo;re important to know.\nLab went well.\nDirichlet BC stuff is chugging along. I definitely did something wrong last time, I think, because I\u0026rsquo;m running into new challenges. Good that I\u0026rsquo;m reimplementing it. I think I have figured out how to deal with these challenges (i.e. get sparse posterior $Q_u$ and then sample from $f$ as a degenerate MVN conditional on $u$.\n","date":1585785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585936094,"objectID":"b9ad0267a7d734498c46d352872c4f5d","permalink":"/post/notebook/2020/spring/week2/thurs/","publishdate":"2020-04-02T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/2","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":" Experts in a subject matter area seem to have worse intuition for teaching that subject matter, compared to novices  In theory this is because they have learned to think about it in a structured and organized way that is not the most effective way to learn that material Experts consistently underestimated how difficult a task is. Novices, having just learned it, know how difficult it is to learn and teach a more basic level Although students rated novices higher and performed better on a post-teaching assessment, the expert-taught students performed better on transferring their learning to a related task   Students appreciate being taught by knowledgeable, motivated, passionate individuals  Students will rate their best teachers highly on competency, credibility, and fariness More closely related to student motivation than actual learning. However children learn less from adults they view as ignorant   Knowledge about subject matter may make it harder to teach in a group setting, but on an individual level enables you to provide helpful feedback, and contextualize a student\u0026rsquo;s ideas and progress against your own knowledge base.  ","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"6eb87badaedf80193ef3c0304fec970a","permalink":"/post/teaching/visible-learning/ch-02/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-02/","section":"post","summary":"My notes on Chapter 2 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"Is Knowledge an Obstacle to Teaching?","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Work on Dirichlet BC code  Afternoon: 1-1:30\n Take notes from Visible Learning Chapter 2  Afternoon: 1:30-3:30\n Attend class and continue looking at QSCI HW 1  Afternoon: 3:30-5\n Work on Dirichlet BC code  Reality Wrote up some resources so that I can stop rederiving/looking up common GP/MVN properties that I need to use. Also included on there the math for Dirichlet BCs. Now that the derivation is there I can more effectively write the code for the implementation, rather than writing stuff down on paper every time I try to do this.\nForgot to account in my scheduling today for time to edit and submit my dissertation proposal. Did that.\nWrote up notes for VL Ch 2, and continued to expand on my own notes for QSCI, reminding myself how all the model diagnostics work.\n","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"f6f3166af62e521631206308ae5408e5","permalink":"/post/notebook/2020/spring/week2/wedn/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 4/1","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Look over the rest of QSCI HW 1  Afternoon: 1-1:30\n Take notes from Visible Learning Chapter 1  Afternoon: 1:30-3:30\n Hold office hours When no one shows up finish up looking at QSCI HW 1 Then read Visible Learning Chapter 2  Afternoon: 3:30-5\n Work on Dirichlet BC code  Reality Didn\u0026rsquo;t make it through the whole thing, had to look up some of the model diagnostics Tim is using, but did get through VL Ch 1 and 2. Struggled with website formatting in the afternoon.\n","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"4f159119f6ccfe6537698062e91cadd9","permalink":"/post/notebook/2020/spring/week2/tues/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/31","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"f83696de8e7521e3d693f2566335d9d8","permalink":"/post/teaching/visible-learning/ch-09/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-09/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"541b760bcd7c5404153842f7f0102b7e","permalink":"/post/teaching/visible-learning/ch-10/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-10/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"1f406f8d598d90e2b3fef88a8201581e","permalink":"/post/teaching/visible-learning/ch-11/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-11/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"0928e4c837fc523ee20b27ed7ab551ab","permalink":"/post/teaching/visible-learning/ch-12/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-12/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"f4e000340828851365e3ac14fe34231f","permalink":"/post/teaching/visible-learning/ch-13/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-13/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"52bc882358ab734a3478cbda7bcbc9e0","permalink":"/post/teaching/visible-learning/ch-14/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-14/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"b53bf1ddbcb4b7edb03168028ad42591","permalink":"/post/teaching/visible-learning/ch-15/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-15/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"25f98c5054431f27dc6424b25b68c2db","permalink":"/post/teaching/visible-learning/ch-16/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-16/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"105f22a11d12bbe89e366dd119ee4879","permalink":"/post/teaching/visible-learning/ch-17/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-17/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"fd99c82d4bd7cf7139944ad7f63617b1","permalink":"/post/teaching/visible-learning/ch-18/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-18/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"32b15a0883e35a80aede68347922ed9a","permalink":"/post/teaching/visible-learning/ch-19/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-19/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"f50b35a91a403153daa6459a9d9f894a","permalink":"/post/teaching/visible-learning/ch-20/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-20/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"25db1538a7e45f65278bdc7760e600cb","permalink":"/post/teaching/visible-learning/ch-21/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-21/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"9206e062fbf34f2f284c1b35abf848d1","permalink":"/post/teaching/visible-learning/ch-22/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-22/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"07a962aa9f6d0fe44ecefceb641885bb","permalink":"/post/teaching/visible-learning/ch-23/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-23/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"20de64d955fa3391c484690c13ca5d81","permalink":"/post/teaching/visible-learning/ch-24/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-24/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"e318ca113f10f54bc8e5ffd3e305a9bf","permalink":"/post/teaching/visible-learning/ch-25/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-25/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"f1217f81d16b4f49d115c78c945908c8","permalink":"/post/teaching/visible-learning/ch-26/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-26/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"d5a1940de85c0ac95741b628d0ec9ac2","permalink":"/post/teaching/visible-learning/ch-27/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-27/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"89d707609a19e7b2c8cad4762a08bd2e","permalink":"/post/teaching/visible-learning/ch-28/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-28/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"02833bfda201868bbebce625b2341c25","permalink":"/post/teaching/visible-learning/ch-29/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-29/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"593c4f3e5068580b1d8aed8bec524b87","permalink":"/post/teaching/visible-learning/ch-30/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-30/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Books"],"content":"","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585590997,"objectID":"7f64f7858f53e14ce4d38f3e547b8f21","permalink":"/post/teaching/visible-learning/ch-31/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-31/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning","Books"],"title":"","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Notes"],"content":"Nine basic principles  Explanations of human learning in terms of innate talent are consistently undermined by research which states that substantial amounts of time, energy, instruction, and effort are required to develop mastery of any subject area. We can naturally learn from any information, but to learn effectively that information has to be organized in a way that matches how our minds are organized. Minds are organized differently for different people, and even change as we age. The brain has severe, inherent limitations and deep processing becomes impossible when those limits are reached (cognitive load principle) People learn particularly well from other people, through directed instruction and feedback (social learning theory) People put in a lot more effort when they are confident that worthwhile goals are achievable in the short term. Activating effort and motivation is difficult but not impossible. Short-term goals are highly motivating, but when they conflict with valuable long-term goals we must develop and use strategies to control impulses and delay gratificatoin (personal regulation through self-control). Learners are humans, and other parts of them \u0026ndash; self esteem, sociality, etc \u0026ndash; must be maintained and acknowledged during the learning process. Humans are fundamentally social, down to the neurological level. This sociality can be used as a tool (social brain hypothesis) Ideas about learning that are contradicted by scientific evidence abound. Many of these ideas can be harmful.  Section 1  The mind may not be \u0026ldquo;designed for thinking\u0026rdquo; and requires time and effort to think Teachers need to recognize how difficult tasks are for beginners. Focus should be not on material but on the actual process of moving from not knowing to knowing. To do this students need a safe environment to acknowledge they don't understand. We can only holld so much in our cognitive centers, so it is important to ``overlearn\u0026rsquo;\u0026rsquo; basic concepts until it is ingrained and automatic  Chapter 1  Students do not dislike school, but tolerate it. The brain may not be \u0026ldquo;designed\u0026rdquo; for abstract thinking and as a result school is a taxing process. Thinking requires a large investment of resources for an uncertain outcome (understanding) and risk-averse humans prefer to invest these resources in crossing seemingly achievable \u0026ldquo;knowledge gaps\u0026rdquo; rather than \u0026ldquo;knowledge chasms\u0026rdquo;  Chapter 2  Experts may be \u0026ldquo;worse\u0026rdquo; at teaching than novices. The \u0026ldquo;expert blind spot effect\u0026rdquo; explains this:  experts forget how difficult it was to learn something in the first place there is a gap between the advanced level on which experts conceptualize of a topic and the short-term learning needs of students however, this abstract thinking may help students transfer their learning to related areas.    Section 2  Major ideas about acquisition, memory retention, mental storage, and overload Learning need not be conscious, we can only think about so much at once We need to develop a vocabulary for learning, and need multiple strategies for learning Challenges: learning styles (i.e. spatial, verbal, kinaesthetic), Mozart effects, multitasking, digital natives, and whether the Internet is really changing how we think  Section 3  Self-esteem follows success more often than predicting it Building confidence is still important though: in order to maintain positive views of ourselves we build explanations (I cannot; rather than I did not work hard enough) that help our self esteem but hurt our learning Learning situations are often distracting and knowing how to pay attention to learning is important, but tiring. It is important to know when to stop thinking to save cognitive resources.  ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585788743,"objectID":"53b2d5a8ce549973dcb32a2c8f23a93d","permalink":"/post/teaching/visible-learning/core-takeaways/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/core-takeaways/","section":"post","summary":"My (hopefully) brief notes on core takeaways from Visible Learning","tags":["Teaching","Visible Learning"],"title":"Broad Strokes of Learning","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-11:30\n Do QSCI HW and prep for TAing  Afternoon: 12:30-1:30\n Read Visible Learning Chapter 1  Afternoon: 3:30-5\n Work on Dirichlet BC code  Reality Did the first half of the QSCI HW, read Ch 1, but did not end up having time to work on the BC code.\n","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585669796,"objectID":"9aef627701e0f148fd9db2096d1d59b0","permalink":"/post/notebook/2020/spring/week2/mon/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/mon/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/30","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals First real week of the quarter and I am surprise TAing! I also found some frustrating uncommitted code from my Colorado trip which I need to reimplement, but rewriting code is often a way to make it better anyways\u0026hellip;\n Make all suggested changes to bypass proposal by Wednesday and send to committee Read chapters 1 and 2 of Visible Learning by Wednesday/Friday Do the QSCI hw ahead of time to make sure I\u0026rsquo;ve got it down by Tuesday Rewrite Dirichlet BC code by Friday  Reality Spent a lot of time on TAing this week, getting back up to speed on the basics of linear regression and model diagnostics. Mostly for myself, I made up a bunch of notes on the math behind these topics, to cement my own understanding, although the students certainly won\u0026rsquo;t need to know much of that. I made it through the first two chapters of Visible Learning, although I still feel like I need to discuss it with someone to really digest it properly.\nI didn\u0026rsquo;t finish off the Dirichlet code but I\u0026rsquo;m close: I got the theory worked out again, and I think I actually had it wrong the first time. I also got my bypass submitted and approved which is awesome and a big relief!\n","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586197075,"objectID":"fa33077b88814ae7cb1b7b7736cdfb6b","permalink":"/post/notebook/2020/spring/week2/post-1/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week2/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 2 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 2","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Visible Learning","Notes"],"content":"  There is no serious evidence that students, on average, actively dislike school. Rather they are neutral, or mildly positive towards school.\n  A frequent challenge faced by teachers is the apathy of students on an individual level, after working so hard to provide an engaging learning experience.\n  VL puts forward the following hypothesis, following researcher Daniel Willingham:\n The human brain is not primarily designed for \u0026ldquo;thinking\u0026rdquo;: it is designed for sociality, for language learning, for spatial reasoning, etc. Higher level reasoning is not the primary purpose and is, on average, very difficult. This means that thinking uses up limited resources very quickly Asking someone to invest this level of effort/resources is a hard task. The outcome of a \u0026ldquo;thinking\u0026rdquo; task could be positive (better understanding) or negative (no progress). Humans are risk-averse and are unlikely to invest effort if they cannot see a likely short-term pay-off. This means the best way to get effort out of students is to show them short-term goals that seem achievable: \u0026ldquo;we are motivated by knowledge gaps, but put off by knowledge chasms.\u0026rdquo; We cannot be curious about everything, and are instead curious about the things we feel we can understand with little additional effort    The ease with which people can access information has a large effect on how they use that information, and how they feel about that information\n  We rely much more heavily on memory than thinking: it is generally easier to remember a previous result than to derive it from scratch\n  ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585684146,"objectID":"d1ba0d9f0d5198a28fa374df8e0d9e7a","permalink":"/post/teaching/visible-learning/ch-01/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/teaching/visible-learning/ch-01/","section":"post","summary":"My notes on Chapter 1 of Visible Learning","tags":["Teaching","Visible Learning"],"title":"Why Don't Students Like School?","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals I might be surprise-TAing next quarter, so I\u0026rsquo;m focusing all my energy on TA prep today. I\u0026rsquo;m going to try to get through the whole UW TA resources page and will be adding notes in posts .\nReality I did that! I also met with Tim the course instructor to outline my responsibilities as a TA and did some editing on the Dirichlet BC code.\n","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"5e82fcb13ee60a02a54a20c4f3d0288e","permalink":"/post/notebook/2020/spring/week1/fri/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week1/fri/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/27","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"Online Office Hours:\n Zoom Pro + Canvas Discussions   ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"b0d0bdad44c423b043c9e25abc34f922","permalink":"/post/teaching/remote-teaching/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/teaching/remote-teaching/","section":"post","summary":"My notes about trying to teach remotely","tags":["Teaching"],"title":"Remote Teaching","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"Expectations Understand early on what the professors expectations are, in detail\n Am I expected to attend class? Will we hold regular TA meetings? Am I grading, writing answer keys, giving guest lectures, holding office hours, holding review sessions, leading discussion sections, responding to online questions, communicating from the instructor to the class, etc? Know what the instructor is emphasizing in class  Demeanor ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"40e5a0e3129ffa899a5721ac6abf54f5","permalink":"/post/teaching/taing/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/teaching/taing/","section":"post","summary":"My notes about TAing","tags":["Teaching"],"title":"TAing","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"Notes from UWs TA page .\nFour recommendations from UW:\n Create clear grading criteria Communicate these criteria to students Give constructive feedback Employ time management strategies when grading large amounts of work  Expect students to:\n focus their time and effort on the things with the highest grading weights be sensitive about their grades, and ask about grading-related topics frequently   Concrete Actions:\n create, communicate, and remind students about clear grading criteria create clear expectations about edge cases like late papers, exam timing, grade changes, and typos on assignments keep thorough records of evaluations, and keep for a while after the quarter is over promptly document interactions with unhappy students  See further resources on:\n  writing tests   assigning grades   evaluating students   Grading  Grading is really hard. First thing to make clear is that I am using mastery-based (i.e. individual skills) rather than norm-referenced (i.e. relative to each other) grading.\n Be consistent Make sure students know what to expect  What is being measured, how is it being measured, what does this have to do with the course at large? Do students need to recall information, recognize patterns, draw inferences, make connections, construct an argument, or what?   Mastery-based grading requires very clear measurable goals and objectives  Likely want to make some re-adjustment of pre-set expectations based on actual performance. Perhaps the test really was too hard?    Cheating How to stop it:\n Explain clearly what is and is not allowed Encourage students to seek help:  Often students cheat because they are doing poorly Encourage them to schedule a meeting and/or come to office hours   Give alternating versions of exams (different orders if not different questions) Regularly walk around the room and observe students After the exam mark answer sheets in such a way that alterations cannot be made (possibly scan a copy of each exam if possible) Give an alternate exam version for make-ups Homework assignments are difficult to monitor  UW recommends lowering the amount of the grade HW is worth\u0026hellip;I don\u0026rsquo;t know about that Replace homework with in-class quizzes?   Final projects  Think about whether students may have done similar projects in other classes    What to do when it happens:\n There is a specific reporting tool but for TAs it probably goes through the instructor first  ","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"f33e4b88e93003549ef25dea8913cc02","permalink":"/post/teaching/assessment/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/teaching/assessment/","section":"post","summary":"My notes about assessments","tags":["Teaching"],"title":"Assessments","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"I want to:\n Learn everyone\u0026rsquo;s name by week 1   First day:\n Introduce myself:  Name/pronouns, background, formality, how/when to reach me   Notecards:  Preferred name/pronunciation/pronouns Accommodation requests   Expectations:  grading/deadline criteria classroom policies my commitments as a teacher how I intend to teach/assess add codes/course conflicts course schedule   Quick (ungraded) quiz to gauge background? Self-reported pre-assessment of background knowledge? Resources for filling in gaps? ``Common Sense Inventory:\u0026rsquo;\u0026rsquo; determine whether 15 statements related to course content are true or false? Homework 0: voluntary/mandatory office hour? If there is time left to teach, model what the rest of the course will be like. Think about whether registration has stabilized and whether to teach foundational material, or just interesting stuff  General Practices\n Tie things into what I\u0026rsquo;m excited about. Students can tell if you\u0026rsquo;re into it Give real world examples/applications, particularly in my own work Keep up a high degree of connectivity, explain how things relate to other things  Active learning is good. Give students a chance to practice that thing they just learned  Office Hours  Most important thing is getting people to come in the first place Give feedback on what\u0026rsquo;s been done so far and offer direction and general guidance Ask them to explain their work and thought process so far  If that leaves a blank, ask them to solve an easier problem   Try to get them to ask specific questions:  Is X true? Does this specific logic follow?   Ask leading questions Try to understand what their thought process is, they are likely coming from a different background than me and will think about things in a different way Work through the homework and exams ahead of time  Try to anticipate sticking points and wrong turns   Know and communicate the instructors expectations and emphasize this material  ","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"51f1ae72b8169b58a8c82375c01a15cf","permalink":"/post/teaching/classroom-practices/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/teaching/classroom-practices/","section":"post","summary":"My notes about classroom practices","tags":["Teaching"],"title":"Classroom Practices","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"Notes from UWs TA page .\n Backward Design :\n Design a course in reverse by considering  What are your learning goals? How will you assess those goals? How will you achieve those goals through teaching?   In this way everything done in the course is intentional and working towards an established goal  Who Are My Students Before I even get to learning goals, also consider: who are my students? Why are they taking my course? What can I expect that they already know? What range of backgrounds might the have? What do I expect them to struggle with?\nWhat Are My Learning Goals  Blooms Taxonomy :\n A way of classifying increasingly ambitious learning objectives Three taxonomies for:  Knowledge-based goals Skills-based foals Affective goals (related to values, attitudes, interests)    How Will I Assess Those Goals Both during the course and at the end of the course. Designing assessments is a whole separate can of worms.\nHow Will I Achieve Those Goals Before I get into the nitty-gritty of lesson planning, etc, now is also a good time to write a syllabus . Once I\u0026rsquo;ve done that I think I personally will benefit from calendaring. This will help me remember little things like: two weeks from today is the exam, I need to write it so I can tell students about format, etc.\n","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"b866db37057a56b1fd278daa38d71bd1","permalink":"/post/teaching/course-design/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/teaching/course-design/","section":"post","summary":"My notes about course design","tags":["Teaching"],"title":"Course Design","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Fix Anaconda errors and start animal movement MCMC running overnight  Afternoon: 1-3\n Start reading UW TA resources  Afternoon: 3-5\n Think more about second-order inclusion probabilities  Reality Fixed Anaconda error, then discovered issue where some code was not pushed to GitHub. Unable to find the original code (mostly my implementation of Dirichlet BCs), but re-writing it should be a useful (if annyoing) exercise.\nStarted reading UW resources and made several posts . Not all of it is relevant just yet, but I\u0026rsquo;d like to get through all of it and start thinking about everything together.\nFound this paper about random sampling with given second-order inclusion probabilities. It gives results about which inclusion probability matrices (IPMs) are valid (the answer is complicated) and constructed a maximum-entropy distribution for fixed-size sampling with fixed IPM.\nThoughts on IPMs:\n Want to incorporate logistics somehow Can we estimate IPMs after filtering for feasible designs? Can we calculate IPMs after filtering for feasible designs? Can we calculate probability of infeasibility for CP(2) designs? How do CP(2) designs work in space-time? I feel like there\u0026rsquo;s something in here somewhere but it\u0026rsquo;s buried deep  ","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"e3db4be194a09c2613dc2c2c2764c8bc","permalink":"/post/notebook/2020/spring/week1/thurs/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week1/thurs/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/26","type":"post"},{"authors":["Connie Okasaki"],"categories":["Teaching","Notes"],"content":"Notes from UWs TA page .\nConsider:\n Why am I testing:  to monitor progress and adjust the pace of the course? to motivate students to provide data for a grade? to challenge students to apply concepts? Use this information to design the exam   Is my test consistent with my teaching:   Backwards design will help with this If the test emphasizes analysis and synthesis, make sure class time does as well   Do the students know what to expect?  Does the test match my stated course goals Have I reviewed the material on the test/explained what will be tested?   Does my exam test a range of learning?  Do students who have not mastered everything have room to demonstrate growth?    Multiple choice exams:\n Easy to grade Good for testing recall and facts, difficult to test analysis Make sure question is clear without reading the answers  Can a prepared student or colleague answer the question as a free response?   Avoid making the right answer stand out for the wrong reasons (i.e. grammer, length) Assess the exam questions afterwards:  Which questions were most difficult? Were there questions which most students with high-grades missed?    Regardless of exam format, assessing it after the fact is important:\n Did I test what I thought I was testing? Did I test what I tught? Did I test what I emphasized? Did I test what I really wanted the students to learn? If answers to these questions come back no, possibly go back to the Backwards Design and rethink course/assessment structure.  ","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591749,"objectID":"be72af0b24d6e01ec04315ab76521ec1","permalink":"/post/teaching/writing-tests/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/teaching/writing-tests/","section":"post","summary":"My notes about writing tests","tags":["Teaching"],"title":"Tests","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Make schedule for reading about teaching, and begin browsing UW resources  Afternoon: 1-3\n Make edits from yesterdays read-through of bypass proposal and look for last couple references  Afternoon: 3-5\n Refamiliarize myself with animal movement case study  Reality Schedule: The UW resource page has 20 sections. The CMU page has 16 sections, and the Visual Learning book has 31 chapters. I plan on reading roughly 1 section, 1 section, and 2 chapters each week, starting next week.\nProposal: Completed and sent to Andrew for feedback.\nAnimal Movement: Looked over the code, tried to re-run it. Got an Anaconda version conflict that I don\u0026rsquo;t remember. Will plan on dealing with that tomorrow.\n","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"ba7e8b01962b4988c508281b0962545e","permalink":"/post/notebook/2020/spring/week1/wedn/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week1/wedn/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/25","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"In this post I will outline the structure of my virtual lab notebook, for use during this time of woe. This is necessitated by the current COVID-19 work-from-home crisis, but will be helpful in general as a tool for accountability and productivity.\nPost Frequency I will post to the lab notebook at the beginning and end of each year, quarter, and week. I will also post to the lab notebook once per day.\nPost Content In each multi-day post I will outline my goals for the current time period, along with a self-imposed due date, and (particularly for yearly and quarterly posts) a rough outline of intermediate steps. Then at the end of the period I will summarize my progress toward those goals as well as any major progress in addition to those goals that I undertook in that time.\nIn each daily post I will outline what my goals were for the day, along with what I actually spent my time on. I will post mathematical and scientific outputs such as documents and plots that I produced that day, and reflect on my productivity and time management.\n","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"3342f9c64fed4d1b8ad5c057d57ac455","permalink":"/post/notebook/outline/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/notebook/outline/","section":"post","summary":"Introducing a Virtual Lab Notebook","tags":["Notebook"],"title":"Lab Notebook Structure","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"I am just starting this lab notebook now in March 2020, so almost a third of the year is behind me already. That said, one of my big goals for the year, submitting my dam passage paper to Proc B, I have already accomplished! Hooray! Hopefully I can continue the momentum now that COVID-19 has well and truly set in.\nGoals I have a tendency to be overly ambitious with my goal timelines. I think I\u0026rsquo;m okay with that for now, but I\u0026rsquo;m trying to temper my expectations a little bit. I\u0026rsquo;m hopeful that with my dam passage project behind me I can focus my time and make more steady progress on my other projects.\n Complete my bypass to the PhD track by April 1  This is a pretty near-term goal but has been a long time coming At this point most of what I need is just to do a bunch of lit reviewing and write up a beefier dissertation proposal than I had the first time round   Submit my source inference paper by August 1 2020  Hopefully the timeline on this is not too ambitious. I have a lot of the theory and code done already, but case studies, editting, and background reading take forever. Sub goal: Write up animal movement case study by April 10 Sub goal: Set up model and begin data analysis for pollen case study by April 17 Sub goal: Write up pollen case study by May 1 Sub goal: Complete third case study by June 5 Sub goal: Complete all background reading by June 5 Sub goal: Complete final editing by July 3 This leaves one month of wiggle room for things to take longer than I hope   Prepare to TA by September 15  This fall will be my first time TAing in any capacity since I was an undergrad. I will have more responsibility now than I did then and I want to make sure I am prepared. To do this I plan on doing background reading on teaching and TAing ahead of time. I know the best experience comes from doing but this is all I can do for now. Sub goal: Browse UW TAing resources  Sub goal: Browse CMU CollectedWisdom TAing resource  Sub goal: Read some of the evidence-based Visible Learning resources I haven\u0026rsquo;t assigned sub-goal deadlines because these are resources I intend to digest slowly over the period. Instead my goal is to do a little of this every week.   Submit my spatial survey design paper by Jan 1  I feel pretty good about getting the other goals done, if not on time, at least this year. This one is the stretch-iest of all of them. But I certainly won\u0026rsquo;t do it if I give up on it now! Sub goal: Find good spatial dataset by May 1 Sub goal: Complete background reading by July 3 Sub goal: Complete simulated case study by September 18 Sub goal: Complete final edits by December 1    ","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"60f289e0f35139fa7acd4576c7fd6be5","permalink":"/post/notebook/2020/annualpost-1/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/notebook/2020/annualpost-1/","section":"post","summary":"My annual lab notebook goals post for 2020","tags":["Notebook"],"title":"Lab Notebook: 2020","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Morning: 9-12\n Set up virtual lab notebook and COVID-19 accountability structure  Afternoon: 1-3\n Add references for optimal design chapter  Afternoon: 3-5\n Refamiliarize myself with animal movement case study  Reality Morning: 9-12\n Set up virtual lab notebook and COVID-19 accountability structure  Afternoon: 1-4\n Add references for optimal design chapter  including major reference to Lee 1998, which actually does do logistically constrained optimal design but with a different objective function propose to compare performance of my method with his, as well as to speed up his method with sparse matrices    Afternoon: 4-5\n Work on new idea for logistically-constrained design  logistically-constrained random sampling? a number of possibilities for how to achieve it all rely upon being able to calculate or estimate $\\pi_i$ and $\\pi_{ij}$ the first- and second-order inclusion probabilities if we want to estimate them, can we define a Gaussian-like spatial process over second-order inclusion probabilities how can we do that? what is the domain of valid second-order inclusion probability matrices?    ","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"4a1a2f63f34e293d58eaa8cbc7d1bb2f","permalink":"/post/notebook/2020/spring/week1/tues/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week1/tues/","section":"post","summary":"My daily lab notebook goals post","tags":["Notebook"],"title":"Lab Notebook: 3/24","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Part of the week has already gone by, but I\u0026rsquo;ll go ahead and fill in some meta goals anyway\n Set up virtual lab notebook and COVID-19 accountability structure by Tuesday Make schedule for reading about teaching and browse UW resources by Wednesday Refamiliarize myself with animal movement case study By Thursday Complete all references for optimal design chapter in bypass proposal by Friday  Reality Did all these things! Hooray!\n","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"716f864bf893140efdfd8911d645cdaa","permalink":"/post/notebook/2020/spring/week1/post-1/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week1/post-1/","section":"post","summary":"My weekly lab notebook goals post for week 1 of spring 2020","tags":["Notebook"],"title":"Lab Notebook: Sp20 Week 1","type":"post"},{"authors":["Connie Okasaki"],"categories":["Notebook"],"content":"Goals Much of this is copied and pasted from my goals for the year:\n Complete my bypass to the PhD track by April 1 Make progress on source inference  Sub goal: Write up animal movement case study by April 10 Sub goal: Set up model and begin data analysis for pollen case study by April 17 Sub goal: Write up pollen case study by May 1 Sub goal: Complete third case study by June 5 Sub goal: Complete all background reading by June 5   Prepare to TA  My real goal here is to do just a little bit of reading, every single week Sub goal: Browse UW TAing resources  Sub goal: Browse CMU CollectedWisdom TAing resource  Sub goal: Read some of the evidence-based Visible Learning resources   Make progress on optimal survey design  Sub goal: Find good spatial dataset by May 1 Sub goal: Complete half of background reading by June 15    ","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585591621,"objectID":"502c22ef1151e1b3525a0ae248c07f90","permalink":"/post/notebook/2020/spring/spring-post-1/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/notebook/2020/spring/spring-post-1/","section":"post","summary":"My quarterly lab notebook goals post for spring 2020","tags":["Notebook"],"title":"Lab Notebook: Spring 2020","type":"post"},{"authors":["Connie Okasaki"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes .  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1581514200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585075117,"objectID":"ae9e6e284dcee91d9f44f14289513c47","permalink":"/talk/spacetime-feb-2020/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/talk/spacetime-feb-2020/","section":"talk","summary":"I will be speaking about my work on source reconstruction for linear PDEs at the UW Space-Time Reading Group weekly meeting.","tags":["spacetime"],"title":"Source Reconstruction for Linear PDEs","type":"talk"},{"authors":["Connie Okasaki"],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation  Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export : E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask  Documentation ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581450678,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Connie Okasaki"],"categories":["spacetime"],"content":"The SPDE Method for Source Inference  Motivation  Pollution  Say, PCBs in the Duwamish Where did it come from? Easy case: point source  Measure all pipe outlets Use regularization method for unknown source   Hard case: non-point source  ???       Possible Approaches  Basically this is the advection-diffusion equation  Possibly with linear decay   Existing methods  FFT/Kalman filter (Sigrist et. al 2014)  complex method non-local basis functions   finite-difference/element method (Stroud et. al 2010)  comparatively simple, very popular local basis functions possibly slower in some contexts   Gaussian functional regression (Nguyen and Peraire 2015)  also complex, relatively unknown basically a fancy \u0026ldquo;kernel trick\u0026rdquo; method       The Math  Our model is $\\mathcal{L}u = f$ Assume $\\mathcal{L}$ is a linear operator Then we can discretize this as $Lu = f$ We model $f\\sim X\\beta + Z\\gamma + \\epsilon$ If everything is normal $[f|d] \\sim \\mbox{MVN}(\\mu_{\\rm post},Q_{\\rm post}^{-1})$   Pros/Cons  Fast, lots of sparse matrices Flexible modeling on $f$  But:\n Requires known physics Requires linear PDE Rigid statistical modeling of everything else   Demo     Next Steps  Stationary distribution of a space-time SPDE process Stochastic perturbation analysis More case studies  Animal movement data Snowpack data Pollen data     Questions? ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581450678,"objectID":"dd1ce71518c13c2ca3d9787f86e24425","permalink":"/slides/spacetime-feb-2020/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/spacetime-feb-2020/","section":"slides","summary":"A brief introduction to my use of the SPDE method to reconstruct PDE sources.","tags":["spacetime","spatial statistics","pdes"],"title":"Slides","type":"slides"},{"authors":["Julie C. Blackwood","Connie Okasaki","Andre Archer","Eliza W. Matt","Elizabeth Sherman","Kathryn Montovan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1517961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585075117,"objectID":"8110f1159113e1fd58283eca19c62842","permalink":"/publication/coral-reefs/","publishdate":"2018-02-07T00:00:00Z","relpermalink":"/publication/coral-reefs/","section":"publication","summary":"Caribbean coral reefs have two alternative stable states: a desirable state with high levels of coral cover, and a coral-depleted counterpart characterized by large amounts of macroalgae. Here we review the existing literature of mathematical models designed to understand the processes that generate these alternative stable states.","tags":["Source Themes"],"title":"Modeling alternative stable state in Caribbean coral reefs","type":"publication"},{"authors":["Connie Okasaki"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math .\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585075117,"objectID":"3477b676c76c65ceaee64bfb46f15431","permalink":"/publication/antibiotic-resistance/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/antibiotic-resistance/","section":"publication","summary":"Here we use mathematical modeling techniques to analyze strategies hospitals can use to slow the evolution of antibiotic resistance, and estimate the length of the delay between evolution and outbreak of antibiotic resistance.","tags":["Source Themes"],"title":"Slowing the Evolution and Outbreak of Antibiotic Resistance","type":"publication"},{"authors":null,"categories":null,"content":"In this project I use a bootstrap method to test a radio-telemetry dataset of Chinook and sockeye salmon for evidence of collective navigation while using fish ladders to pass dams.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585339307,"objectID":"72a17159bc18ef27c7cb36bb6af91c35","permalink":"/project/dam-passage/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/dam-passage/","section":"project","summary":"We investigate the possibility for collective behavior of salmon to facilitate navigation of man-made dams along the Columbia river.","tags":["Ecology"],"title":"Collective Behavior During Dam Passage","type":"project"},{"authors":null,"categories":null,"content":"In this project I investigate the use of sparse precision matrices and mixed integer/linear programs to efficient conduct logistically-constrained optimal spatial sampling design.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585339307,"objectID":"5c6d0bb7ce53d9ea5f3489dcb938ece3","permalink":"/project/opt-design/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/opt-design/","section":"project","summary":"We investigate the effect of logistical constraints on optimal spatial sampling design.","tags":["Statistics","Stochastic PDE","Spatial Statistics","Optimization"],"title":"Logistically-Constrained Optimal Spatial Sampling Design","type":"project"},{"authors":null,"categories":null,"content":"In this project I investigate the use of stochastic PDEs to conduct inference on the source term of a linear PDE.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585339307,"objectID":"9292d90d9af0c6d5b7fce77c87378250","permalink":"/project/source/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/source/","section":"project","summary":"We investigate the use of stochastic PDEs to conduct inference on the source term of a linear PDE.","tags":["Statistics","Stochastic PDE","Spatial Statistics"],"title":"Source Inference","type":"project"},{"authors":null,"categories":null,"content":"\\1;5202;0c\u0026mdash; authors:\n admin categories: Notebook date: \u0026ldquo;2020-04-13T00:00:00Z\u0026rdquo; draft: false featured: false image: projects: [] subtitle: summary: \u0026lsquo;My weekly lab notebook goals post for week 4 of spring 2020\u0026rsquo; tags: Notebook title: \u0026lsquo;Lab Notebook: Sp20 Week 4\u0026rsquo;   Goals  Need to debug and write up the animal movement case study, as I have fallen behind on my goal for that Need to import pollen data and set up a preliminary model for it Keep working a little bit on the SNN modeling math to get it into shape Finish up notes on VL Ch 3 and 4 Read and take notes on VL Ch 5 and 6 Find large spatial dataset for optimal sampling design Need to write up NSF GRF progress report  Reality Did debug and partially write up the animal movement case study but there is still more writing/background to do there. Found a problem with the pollen data so that also will take longer than expected. Worked a little bit on the SSN modeling and found a matrix formulation for it (another DT lyapunov equation) although in the end it\u0026rsquo;ll probably all be finite difference anyways. Took notes on VL Ch 3-6, and decided with Sándor that we will just do a purely synthetic dataset to start for the sampling design. Wrote up NSF progress report, although still waiting on Andrew to sign off.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0128beb7e4a0d377b6e281e38d96d0d7","permalink":"/post/notebook/2020/spring/week4/post-1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/notebook/2020/spring/week4/post-1/","section":"post","summary":"\\1;5202;0c\u0026mdash; authors:\n admin categories: Notebook date: \u0026ldquo;2020-04-13T00:00:00Z\u0026rdquo; draft: false featured: false image: projects: [] subtitle: summary: \u0026lsquo;My weekly lab notebook goals post for week 4 of spring 2020\u0026rsquo; tags: Notebook title: \u0026lsquo;Lab Notebook: Sp20 Week 4\u0026rsquo;   Goals  Need to debug and write up the animal movement case study, as I have fallen behind on my goal for that Need to import pollen data and set up a preliminary model for it Keep working a little bit on the SNN modeling math to get it into shape Finish up notes on VL Ch 3 and 4 Read and take notes on VL Ch 5 and 6 Find large spatial dataset for optimal sampling design Need to write up NSF GRF progress report  Reality Did debug and partially write up the animal movement case study but there is still more writing/background to do there.","tags":null,"title":"","type":"post"}]