<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gaussian Processes | Colin Okasaki</title>
    <link>/courses/gaussian-processes/</link>
      <atom:link href="/courses/gaussian-processes/index.xml" rel="self" type="application/rss+xml" />
    <description>Gaussian Processes</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 31 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon.png</url>
      <title>Gaussian Processes</title>
      <link>/courses/gaussian-processes/</link>
    </image>
    
    <item>
      <title>Notation and Terminology</title>
      <link>/courses/gaussian-processes/notation/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/notation/</guid>
      <description>&lt;h2 id=&#34;matrixvector-notation-&#34;&gt;Matrix/Vector Notation&lt;/h2&gt;
&lt;p&gt;In general, I will use an upper case letter for a matrix and a lower case letter for a vector. Vectors such as $v$ will almost always be accompanied by a description of an individual component $v_i$, referring to the $i$-th entry in the vector. If a vector&amp;rsquo;s components are not specified I will bold it such as $\mathbf{v}$. I will write $X&#39;$ to mean the transpose of a matrix $X$ or $v&#39;$ to mean the transpose of a matrix $v$, as is common in regression analysis.&lt;/p&gt;
&lt;h2 id=&#34;matrix-calculus-&#34;&gt;Matrix Calculus&lt;/h2&gt;
&lt;p&gt;For an $n\times 1$ vector $v$ and a scalar $f$ we will write $\dfrac{\partial v}{\partial f}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial v}{\partial f}\right)_i = \dfrac{\partial v_i}{\partial f}$. We will write $\dfrac{\partial f}{\partial v}$ to mean the $n\times 1$ vector with entries $\left(\dfrac{\partial f}{\partial v}\right)_i = \dfrac{\partial f}{\partial v_i}$. A useful resource for matrix calculus can be found on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;
 or in the more extensive 
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-process-notation-&#34;&gt;Gaussian Process Notation&lt;/h2&gt;
&lt;p&gt;I will write $f \sim GP(\mu(x),k(x,x&amp;rsquo;))$ to denote a Gaussian process with mean function $\mu$ and covariance function (kernel) $k(x,x&amp;rsquo;)$. I will in general &lt;em&gt;not&lt;/em&gt; assume that $k$ is stationary (see 
&lt;a href=&#34;#terminology&#34;&gt;terminology&lt;/a&gt;
). Under this definition $f$ is a stochastic process with the defining property that for any set of points ${x_i}$ (in whatever space $\Omega$ we choose), the vector $f(x)_i = f(x_i)$ is distributed as a multivariate normal (MVN) random vector with mean $\mu_i = \mu(x_i)$ and covariance matrix $\Sigma_{ij} = k(x_i,x_j)$. Many of the properties I will discuss on this page are actually properties of the MVN distribution.&lt;/p&gt;
&lt;h2 id=&#34;terminology-&#34;&gt;Terminology&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process is called &lt;em&gt;stationary&lt;/em&gt; (or &lt;em&gt;homogeneous&lt;/em&gt;) if $k(x,x&amp;rsquo;) = k(x-x&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;A Gaussian Process is called &lt;em&gt;isotropic&lt;/em&gt; if $k(x,x&amp;rsquo;) = k(|x-x&#39;|)$&lt;/li&gt;
&lt;li&gt;A matrix $M$ is said to be &lt;em&gt;positive semidefinite&lt;/em&gt; if it has the property that $v&amp;rsquo;Mv \geq 0$ for any vector $v$. Covariance matrices are positive semidefinite.&lt;/li&gt;
&lt;li&gt;A kernel is said to be positive semidefinite (psd) if $$\int_\Omega k(x,x&amp;rsquo;)f(x)f(x&amp;rsquo;)dxdx&amp;rsquo; \geq 0$$ for all $L_2$ functions $f$. Gram matrices (i.e. covariance matrices) from psd kernels are psd matrices.&lt;/li&gt;
&lt;li&gt;The inverse of the covariance matrix in a MVN distribution is $Q = \Sigma^{-1}$ and is called the &lt;em&gt;precision matrix&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>General Linear Algebraic Properties</title>
      <link>/courses/gaussian-processes/linear-algebra/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/linear-algebra/</guid>
      <description>&lt;p&gt;The Woodbury matrix identity is $$(A+UCV)^{-1}=A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.$$ Simpler versions of this identity are
\begin{align*}
(I+UV)^{-1} &amp;amp; = I-U(I+VU)^{-1}V, \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-(I+P)^{-1}P \\&lt;br&gt;
(I+P)^{-1} &amp;amp; = I-P(I+P)^{-1}
\end{align*}
In the special case that $u$ and $v$ are vectors and $C = I$ we get the Sherman-Morrison formula: $$(A+uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.$$ A similar formula is the matrix determinant lemma $$\mbox{det}(A+uv^T) = (1+v^TA^{-1}u)\mbox{det}(A).$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Block Matrices</title>
      <link>/courses/gaussian-processes/block-matrices/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/block-matrices/</guid>
      <description>&lt;p&gt;Block matrices have some nice linear algebraic properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\&lt;br&gt;
-(D-CA^{-1}B)^{-1}CA^{-1} &amp;amp; (D-CA^{-1}B)^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\begin{bmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{bmatrix}
=
\begin{bmatrix}
(A-BD^{-1}C)^{-1} &amp;amp; -(A-BD^{-1}C)^{-1}BD^{-1} \\&lt;br&gt;
-D^{-1}C(A-BD^{-1}C)^{-1} &amp;amp; D^{-1} + D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
\end{bmatrix}$$&lt;/li&gt;
&lt;li&gt;$$\mbox{det}
\begin{pmatrix}
A &amp;amp; B \\&lt;br&gt;
C &amp;amp; D
\end{pmatrix}
=
\mbox{det}(A)\times \mbox{det}(D-CA^{-1}B) = \mbox{det}(D)\times\mbox{det}(A-BD^{-1}C)$$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dirichlet BCs</title>
      <link>/courses/gaussian-processes/dirichlet/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/dirichlet/</guid>
      <description>&lt;p&gt;The simplest version of the finite element method is said to have &amp;ldquo;natural&amp;rdquo; Neumann boundary conditions, meaning that Neumann boundary conditions are naturally satisfied without imposing any additional structure. Dirichlet boundary conditions then are said to be &amp;ldquo;essential,&amp;rdquo; meaning they must be explicitly imposed after the fact. Certain FEM formulations change up this Neumann=natural, Dirichlet=essential pradigm but for our purposes we will treat these terms as interchangeable.&lt;/p&gt;
&lt;p&gt;Now, suppose that we are confronted with the FEM equation $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ K_{21} &amp;amp; K_{22} \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11} &amp;amp; L_{12} \\ L_{21} &amp;amp; L_{22} \end{bmatrix} \begin{bmatrix} f_1 \\ f_2 \end{bmatrix},$$
with the Dirichlet BC $u_2 = u^*$. We can modify our FEM equation to enforce this, to $$\begin{bmatrix} K_{11} &amp;amp; K_{12} \\ 0 &amp;amp; I \end{bmatrix}\begin{bmatrix} u_1 \\ u_2\end{bmatrix} = \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}.$$ We can simplify this to an equation for $u$ by inverting the matrix on the left: $$\begin{bmatrix} u_1 \\ u_2 \end{bmatrix} = \begin{bmatrix} K_{11}^{-1} &amp;amp; -K_{11}^{-1}K_{12} \\ 0 &amp;amp; I \end{bmatrix} \begin{bmatrix} L_{11}f_1 + L_{12}f_2 \\ u^* \end{bmatrix}$$
Now, assuming that $f\sim N(\mu_f,Q_f)$ and $u^*$ is given (often $u^* = 0$) we obtain the following distribution for $u_1$:
\begin{align*}
u_1 &amp;amp; = K_{11}^{-1}L_1f - K_{11}^{-1}K_{12}u^* \\&lt;br&gt;
u_1 &amp;amp; \sim N\left(K_{11}^{-1}(L_1\mu_f - K_{12}u^*), K_{11}^{-1}L_1\Sigma_fL_1^TK_{11}^{-T}\right).
\end{align*}&lt;/p&gt;
&lt;p&gt;Now often we are interested in the posterior distribution for $u$ and/or $f$. Since we have assumed that $u^*$ is given the two are deterministically related. However, $u$ is of lower dimension than $f$. So, we will try to calculate the posterior distribution of $f$ since this can be used to calculate the posterior of $u$ but not vice versa. We will assume that we have made some observations $y = y_1 + y_2 = A_1u_1 + A_2u_2 + \epsilon$ of $u$ from which we wish to make inference. Since $u^*$ is known we essentially have observations:
\begin{align*}
y &amp;amp; = A_1K_{11}^{-1}(L_1f - K_{12}u^*) + A_2u^* + \epsilon \\&lt;br&gt;
&amp;amp; = A_1K_{11}^{-1}L_1f + (A_2 - A_1K_{11}^{-1}K_{12})u^* + \epsilon \\&lt;br&gt;
&amp;amp; = y_f + y^* + \epsilon
\end{align*}
Using the 
&lt;a href=&#34;../posteriors/#linear-observations&#34;&gt;results&lt;/a&gt;
 for a posterior from a linearly-observed MVN distribution we see that:
\begin{align*}
[f|y=a] &amp;amp; = N\left(\mu_f + Q_{f|y}^{-1}B^TQ_{\epsilon}(y - y^* - B^T\mu_f), Q_{f|y}^{-1}\right) \\&lt;br&gt;
Q_{f|y} &amp;amp; = Q_f + B^TQ_{\epsilon}B \\&lt;br&gt;
B &amp;amp; = A_1K_{11}^{-1}L_1
\end{align*}&lt;/p&gt;
&lt;p&gt;So the challenges that we face in implementing these two different ideas are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u_1$ is lower dimensional than $f$ and so $f$ is not uniquely defined by $u_1$&lt;/li&gt;
&lt;li&gt;$L_1\Sigma_fL_1^T$ is not easily invertible since $L_1$ is not square&lt;/li&gt;
&lt;li&gt;OR: $K_{11}^{-1}$ is dense&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My solution to this is to take the approach of modeling at the $u$ level. We can deal with $u_1$ being lower dimensional by calculating the degenerate MVN distribution for $f|u_1$. We can deal with $L_1$ being rectangular by considering our old approximation $\tilde{L}$. Using this approximation, which we have always needed to do to keep $L^{-1}$ sparse, we have that $(\tilde{L}_1\Sigma_f\tilde{L}_1^T)_{ij} = \tilde{L}_{ii}\Sigma_{ij(f)}\tilde{L}_{jj}$ for $1\leq i,j\leq n_1$. This is exactly the same as if we truncated the additional rows and columns of $\tilde{L}$ and $\Sigma_f$, since $\tilde{L}$ is zero in all of those columns anyways. Thus we obtain
\begin{align*}
(L_1\Sigma_fL_1^T)^{-1}
&amp;amp; \approx (\tilde{L}_1\Sigma_f\tilde{L}_1^T)^{-1} \\&lt;br&gt;
&amp;amp; = (\tilde{L}_{11}\Sigma_{11(f)}\tilde{L}_{11})^{-1} \\&lt;br&gt;
&amp;amp; = \tilde{L}^{-1}_{11}\Sigma_{11(f)}^{-1}\tilde{L}_{11}^{-1} \\&lt;br&gt;
&amp;amp; = \tilde{L}^{-1}_{11}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\tilde{L}^{-1}_{11}.
\end{align*}
Since $u_2$ is low dimensional, we can easily invert the dense $Q_{22}$ and we still end up with a sparse matrix $Q_{11(u)}$. So we have solved problem (1).&lt;/p&gt;
&lt;p&gt;Now we hit up against the following problem: we may conduct efficient inference on $u_1$, but even though $u_1$ and $f$ are deterministically linked, $f$ is not uniquely identified by $u_1$. So we need to find the degenerate MVN distribution defining $f|u_1$. Alternatively we can assume that there is some small noise that enters between $f$ and $u_1$ so that the two are &lt;em&gt;not&lt;/em&gt; deterministically linked and so that we have a non-degenerate distribution for $f$. This becomes more important if we have measurements for $f$ as well as $u$. For now we will assume this is not the case.&lt;/p&gt;
&lt;p&gt;The primary obstacle we need to overcome here is how to obtain inference for a degenerate distribution while maintaining our sparsity paradigm, which depends upon the inverse of the covariance matrix. To obtain the distribution for $f|u_1$ we will consider the following algebra:
\begin{align*}
[f|u_1]
&amp;amp; = [f_1,f_2|u] \\&lt;br&gt;
&amp;amp; = [f_1|u,f_2][f_2|u].
\end{align*}
So what we are able to do here is specify each of these distributions separately. Since all we really need to be able to do is calculate the posterior mean of $f$ and simulate draws of $f|u$, if we can do this we are finished. So what are these distributions? Once $f_2$ is specified, $f_1$ is uniquely determined by $u_1$ so we find that: $$[f_1|u_1,f_2] = L_{11}^{-1}(K_{11}u_1 + K_{12}u^* - L_{12}f_2).$$ So then all we really need to sample is $[f_2|u]$. This can be calculated using Bayes formula:
\begin{align*}
[f_2|u]
&amp;amp; = [u|f_2][f_2] \\&lt;br&gt;
&amp;amp; = [u_1|f_2,u^*][f_2]
\end{align*}
Then we can use the formula $$K_{11}u_1 + K_{12}u^* = L_{11}f_1 + L_{12}f_{2}$$
to obtain $$u_1 = K_{11}^{-1}(L_{11}f_1 + L_{12}f_2 - K_{12}u^*)$$
and the (non-degenerate MVN) distribution
\begin{align*}
u_1
&amp;amp; = N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}L_{11}\Sigma_{11(f)}L_{11}K_{11}^{-T}\right) \\&lt;br&gt;
&amp;amp; \approx N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\bar{L}_{11}^{-1}\Sigma_{11(f)}^{-1}\bar{L}_{11}^{-1}K_{11}\right) \\&lt;br&gt;
&amp;amp; = N\left(K_{11}^{-1}(L_{11}\mu_{1(f)} + L_{12}f_2 - K_{12}u^*),K_{11}^{T}\bar{L}_{11}^{-1}(Q_{11(f)} - Q_{12(f)}Q_{22(f)}^{-1}Q_{21(f)})\bar{L}_{11}^{-1}K_{11}\right)
\end{align*}
Now this distribution bears a remarkable resemblance to what we have previously written for $[u_1]$ with no conditioning at all, since our approximation $\tilde{L}$ removes the direct dependence of $u_1$ on $f_2$. However, it is important to note that in this formulation we _actually_ have the matrix $L_{11}$, rather than a truncation of $\tilde{L}$. Thus we need to approximate $L_{11}$ directly rather than approximation $\tilde{L}\approx L$. Thus we get what we have denote $\bar{L}_{11}\approx L_{11}$ which we obtain by grouping all the terms in $L_{11}$ onto the diagonal (the same operation which gives us $\tilde{L}$ from $L$). Since a number of non-zero terms from $L$ are removed by this operation, and in particular these non-zero terms are along the edges of $u_1$ where it abuts $u_2$, we see that in fact this is a substantively different distribution. Our approximation for $[u_1|f_2]$ has less uncertainty along the boundary, just as we would expect. Now that we have this distribution all we need do is find the unconditional distribution $[f_2]$, which may easily be found by blockwise inversion of $Q_{f}$: $$[f_2] = N(\mu_{2(f)},(Q_{22} - Q_{21}Q_{11}^{-1}Q_{12})^{-1}).$$ This involves a dense $n_2\times n_2$ matrix but since the boundary is lower-dimensional this is okay.&lt;/p&gt;
&lt;p&gt;Now actually calculating this full posterior involves some &lt;em&gt;heavy&lt;/em&gt; algebra and I&amp;rsquo;m sure there&amp;rsquo;s a better way to do this but here goes:
\begin{align*}
[f_2|u_1]
&amp;amp; \propto [u_1|f_2][f_2] \\&lt;br&gt;
&amp;amp; = N\left(u_1|K_{11}^{-1}(L_{11}\mu_1 + L_{12}f_2 - K_{12}u^*),K_{11}^{-1}\bar{L}_{11}\Sigma_{11}\bar{L}_{11}K_{11}^{-T}\right)N\left(f_2|\mu_2,\Sigma_{22}\right) \\&lt;br&gt;
&amp;amp; \propto \exp\left(-\frac{1}{2}(u_1 - \mu_{u|f})^TK_{11}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}K_{11}(u_1 - \mu_{u|f})\right)\exp\left(-\frac{1}{2}(f_2-\mu_2)^T\Sigma_{22}^{-1}(f_2-\mu_2)\right) \\&lt;br&gt;
&amp;amp; \propto \exp\left(-\frac{1}{2}f^T(\Sigma_{22}^{-1}+L_{12}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}L_{12})f - \frac{1}{2}(f^Tv + v^Tf)\right) \\&lt;br&gt;
v &amp;amp; = L_{12}^T\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}(L_{11}\mu_1 + L_{12}f_2 - K_{12}u^* - K_{11}u_1) - f_2^T\Sigma_{22}^{-1}\mu_2
\end{align*}
Thus we see that $[f_2|u_1]$ is normal with precision matrix:
\begin{align*}
Q_{f_2|u_1}
&amp;amp; = \Sigma_{22}^{-1} + L_{21}\bar{L}_{11}^{-1}\Sigma_{11}^{-1}\bar{L}_{11}^{-1}L_{12} \\&lt;br&gt;
&amp;amp; = Q_{22} - Q_{21}Q_{11}^{-1}Q_{12} + L_{21}\bar{L}_{11}^{-1}(Q_{11} - Q_{12}Q_{22}^{-1}Q_{21})\bar{L}_{11}^{-1}L_{12} \\&lt;br&gt;
\mu_{f_2|u_1}
&amp;amp; = -Q_{f_2|u_1}^{-1}v
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Posteriors</title>
      <link>/courses/gaussian-processes/posteriors/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/posteriors/</guid>
      <description>&lt;p&gt;The multivariate normal distribution has nicely behaved posterior distributions. In particular, the posterior given an observation of the vector is also multivariate normal.&lt;/p&gt;
&lt;h2 id=&#34;basic-posterior-&#34;&gt;Basic Posterior&lt;/h2&gt;
&lt;p&gt;Let $x$ be an $n\times 1$ vector partitioned into $x = (x_1,x_2)$, with $x_1$ having dimension $k$ and $x_2$ having dimension $n-k$. Partition the mean $\mu = (\mu_1,\mu_2)$ and covariance matrix $$\Sigma = \begin{bmatrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix}.$$ Then the posterior distribution for $x_1$ given $x_2=a$ is given by
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(a-\mu_2), \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;precision-formulation&#34;&gt;Precision Formulation&lt;/h2&gt;
&lt;p&gt;Often in my research we are interested in analyzing a MVN distribution with known sparse &lt;em&gt;precision&lt;/em&gt; matrix. It is expensive to invert matrices and cheap to work with sparse matrices so we wish to work directly with this precision matrix. Furthermore, we want to calculate the precision matrix for our posterior because it is likely to also be computational advantageous. Let the precision matrix be partitioned
$$Q = \begin{bmatrix} Q_{11} &amp;amp; Q_{12} \\ Q_{21} &amp;amp; Q_{22} \end{bmatrix}.$$
Then the equivalent posterior distribution is
\begin{align*}
[x_1|x_2=a]
&amp;amp; = N\left(\mu_1 + Q_{11}^{-1}Q_{12}(a-\mu_2),Q_{11}\right).
\end{align*}
Furthermore, let $RR^T = Q$ be the Cholesky decomposition of $Q$ also be partitioned into blocks. If $Q$ is sparse then under mild conditions, $R$ is also sparse and we can work with it to do quick computation. The posterior distribution now is $$[x_1|x_2=a] = N\left(\mu_1 - R_{11}^{-1}(R_{11}^{-T}(Q_{12}(a-\mu_2))), Q_{11}\right).$$&lt;/p&gt;
&lt;h2 id=&#34;linear-observations&#34;&gt;Linear Observations&lt;/h2&gt;
&lt;p&gt;Often we are also interested in analyzing a MVN distribution where we observe not the individual components, but a linear combination thereof $y = Ax + \epsilon$ with some mean-zero MVN noise vector $\epsilon$. Then the observations are distributed $y|x \sim N( Ax , Q_\epsilon^{-1} )$. The posterior $x|y$ is given by:
\begin{align*}
[x|y=a] &amp;amp; = N\left(\mu_x + Q_{x|y}^{-1}A^TQ_{\epsilon}(y-A\mu_x), Q_{x|y}\right) \\&lt;br&gt;
Q_{x|y} &amp;amp; = Q + A^TQ_{\epsilon}A
\end{align*}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Moore-Penrose Pseudoinverse</title>
      <link>/courses/gaussian-processes/pseudoinverse/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>/courses/gaussian-processes/pseudoinverse/</guid>
      <description>&lt;p&gt;The Moore-Penrose Pseudoinverse is a generalization of the inverse. In particular it extends the inverse matrix to non-square matrices. The inverse of a matrix $A$ is defined by any matrix $A^+$ with the following four properties&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$AA^+A = A$&lt;/li&gt;
&lt;li&gt;$A^+AA^+ = A^+$&lt;/li&gt;
&lt;li&gt;$(AA^+)^* = AA^+$&lt;/li&gt;
&lt;li&gt;$(A^+A)^* = A^+A$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = (A^&lt;em&gt;A)^{-1}A^&lt;/em&gt;$ and the pseudoinverse is a &lt;em&gt;left inverse&lt;/em&gt; since $A^+A = I$. If $A$ has linearly independent columns then the pseudoinverse can be calculated as $A^+ = A^*(AA^*)^{-1}$ and the pseudoinverse is a _right inverse_ since $AA^+ = I$.&lt;/p&gt;
&lt;p&gt;Finally, consider $(AB)^+$. This is equal to $B^+A^+$ if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A$ has orthonormal columns (i.e. $A^*A = I$), or&lt;/li&gt;
&lt;li&gt;$B$ has orthonormal rows (i.e. $BB^* = I$), or&lt;/li&gt;
&lt;li&gt;$A$ has full column rank and $B$ has full row rank, or&lt;/li&gt;
&lt;li&gt;$B = A^*$
In general, however, $(AB)^+ \neq $B^+A^+$.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
